{
    "docs": [
        {
            "location": "/", 
            "text": "Tutorials and protocols\n\n\nThese tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the \nAustralian-made Genomics Virtual Laboratory\n and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.", 
            "title": "Home"
        }, 
        {
            "location": "/#tutorials-and-protocols", 
            "text": "These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the  Australian-made Genomics Virtual Laboratory  and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.", 
            "title": "Tutorials and protocols"
        }, 
        {
            "location": "/guides/bioinfo/", 
            "text": "What is bioinformatics?\n\n\nBioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites. \n\n\nGetting started\n\n\n\n\n\n\nLook at our guides on the left for a general introduction to several key topics in bioinformatics. \n\n\n\n\n\n\nUnderneath the guides, there are groups of tutorials for common tasks in bioinformatics. \n\n\n\n\n\n\nTo analyse data, we recommend getting started with \nGalaxy\n - a free, web-based bioinformatics platform. \n\n\n\n\n\n\nResources\n\n\n\n\n\n\nMelbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. \n\n\n\n\n\n\nAustralian Bioinformatics and Computational Biology Society:\n\nwww.abacbs.org\n\n\n\n\n\n\nStudent group of ABACBS - COMBINE: \nwww.combine.org.au\n\n\n\n\n\n\nWinter School in Mathematical and Computational Biology: \n\nwww.bioinformatics.org.au\n\n\n\n\n\n\nBioInfo Summer: \nwww.bis.amsi.org.au\n\n\n\n\n\n\nSoftware Carpentry: \nwww.software-carpentry.org\n\n\n\n\n\n\nBioinformatics problem solving challenges: \nwww.rosalind.info\n \n\n\n\n\n\n\nA selelection of key journal papers published last year in \nBMC Bioinformatics\n.", 
            "title": "What is bioinformatics?"
        }, 
        {
            "location": "/guides/bioinfo/#what-is-bioinformatics", 
            "text": "Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites.", 
            "title": "What is bioinformatics?"
        }, 
        {
            "location": "/guides/bioinfo/#getting-started", 
            "text": "Look at our guides on the left for a general introduction to several key topics in bioinformatics.     Underneath the guides, there are groups of tutorials for common tasks in bioinformatics.     To analyse data, we recommend getting started with  Galaxy  - a free, web-based bioinformatics platform.", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/bioinfo/#resources", 
            "text": "Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics.     Australian Bioinformatics and Computational Biology Society: www.abacbs.org    Student group of ABACBS - COMBINE:  www.combine.org.au    Winter School in Mathematical and Computational Biology:  www.bioinformatics.org.au    BioInfo Summer:  www.bis.amsi.org.au    Software Carpentry:  www.software-carpentry.org    Bioinformatics problem solving challenges:  www.rosalind.info      A selelection of key journal papers published last year in  BMC Bioinformatics .", 
            "title": "Resources"
        }, 
        {
            "location": "/guides/computing/", 
            "text": "Computing platforms and infrastructure\n\n\nThere are several options for data analysis and data storage. \n\n\nYour local computer\n\n\nInstall and run tools locally. \n\n\nMelbourne Bioinformatics has written tutorials for a few key tools, and we also run workshops throughout the year. For additional workshops, see \nResearch Platforms\n.\n\n\nWe recommend these tutorials to get started: \n\n\n\n\nUnix\n\n\nPython\n\n\nGit\n\n\nDocker\n\n\n\n\nThe Galaxy Platform\n\n\nGalaxy is a free, web-based bioinformatics platform. See our full guide \nhere\n. \n\n\nLaunch a Linux machine in the cloud\n\n\nUse a virtual machine. \n\n\nFor example, you could use the Genomics Virtual Lab (the GVL) in the Nectar cloud. \n\n\n\n\nTutorial: launch your own Galaxy\n\n\nNectar: https://nectar.org.au\n\n\n\n\nA high performance computer (cluster)\n\n\nMelbourne Bioinformatics runs large clusters on which researchers can run analyses. \n\n\nFor detailed information see the documentation \nhere\n. \n\n\n\n\n\n\nTo apply for usage, click \nhere\n\n\n\n\n\n\nFor information about logging on, click \nhere\n.\n\n\n\n\n\n\nFor information about running jobs, click \nhere\n. \n\n\n\n\n\n\nData storage\n\n\nLarge datasets may be difficult to store (and share) on your local computer. \n\n\nAn alternative option is using storage on the Nectar cloud. \n\n\n\n\nNectar is an Australian organization that provides a cloud for data storage and analysis. \n\n\nNectar link: \nnectar.org.au\n\n\nClick \nhere\n for information about storage options and tools on Nectar. \n\n\nClick \nhere\n for an introduction to cloud storage on Nectar.", 
            "title": "Computing"
        }, 
        {
            "location": "/guides/computing/#computing-platforms-and-infrastructure", 
            "text": "There are several options for data analysis and data storage.", 
            "title": "Computing platforms and infrastructure"
        }, 
        {
            "location": "/guides/computing/#your-local-computer", 
            "text": "Install and run tools locally.   Melbourne Bioinformatics has written tutorials for a few key tools, and we also run workshops throughout the year. For additional workshops, see  Research Platforms .  We recommend these tutorials to get started:    Unix  Python  Git  Docker", 
            "title": "Your local computer"
        }, 
        {
            "location": "/guides/computing/#the-galaxy-platform", 
            "text": "Galaxy is a free, web-based bioinformatics platform. See our full guide  here .", 
            "title": "The Galaxy Platform"
        }, 
        {
            "location": "/guides/computing/#launch-a-linux-machine-in-the-cloud", 
            "text": "Use a virtual machine.   For example, you could use the Genomics Virtual Lab (the GVL) in the Nectar cloud.    Tutorial: launch your own Galaxy  Nectar: https://nectar.org.au", 
            "title": "Launch a Linux machine in the cloud"
        }, 
        {
            "location": "/guides/computing/#a-high-performance-computer-cluster", 
            "text": "Melbourne Bioinformatics runs large clusters on which researchers can run analyses.   For detailed information see the documentation  here .     To apply for usage, click  here    For information about logging on, click  here .    For information about running jobs, click  here .", 
            "title": "A high performance computer (cluster)"
        }, 
        {
            "location": "/guides/computing/#data-storage", 
            "text": "Large datasets may be difficult to store (and share) on your local computer.   An alternative option is using storage on the Nectar cloud.    Nectar is an Australian organization that provides a cloud for data storage and analysis.   Nectar link:  nectar.org.au  Click  here  for information about storage options and tools on Nectar.   Click  here  for an introduction to cloud storage on Nectar.", 
            "title": "Data storage"
        }, 
        {
            "location": "/guides/galaxy/", 
            "text": "The Galaxy Platform\n\n\n\n\n\nGalaxy is a web platform for bioinformatics analysis. \n\n\nWhich Galaxy should I use?\n\n\n\n\n\n\nThere are many different Galaxy servers - each one has a different web address.\n\n\n\n\n\n\nFor researchers based in Melbourne, Australia, we recommend you use \nGalaxy-Mel\n.\n\n\n\n\n\n\nThe main worldwide Galaxy server is \nhttps://usegalaxy.org/\n\n\n\n\n\n\nOther Galaxy servers are listed \nhere\n.\n\n\n\n\n\n\nCheck the policy for the server you are using so that you know how long your data will be kept. \n\n\n\n\n\n\nIf you use more than one Galaxy server (e.g. Galaxy-Mel and useGalaxy.org) you need to register and log in separately for each server. They don't talk to each other. \n\n\n\n\n\n\nTutorials\n\n\nGo to Galaxy-Mel and log in, then try these tutorials:\n\n\n\n\nIntroduction to Galaxy\n \n\n\nGalaxy Workflows\n \n\n\nOther MB tutorials, e.g. Assembly, RNA-seq, etc. \n\n\n\n\nGalaxy Training Network\n\n\nThe Galaxy Training Network hosts a large collection of useful training material \nhere\n.\n\n\nTo get started, go to \nhttps://usegalaxy.org/\n, log in, then try these tutorials: \n\n\n\n\nIntroduction to Galaxy\n\n\nGalaxy histories\n\n\n\n\n\n\nRunning your own Galaxy server in the cloud\n\n\nIt is possible to run your own Galaxy instance using the Nectar cloud. \n\n\n\n\n\n\nTutorial to launch your own Galaxy\n\n\n\n\n\n\nNectar: https://nectar.org.au\n\n\n\n\n\n\nRunning Galaxy locally using Docker\n\n\nIt is usually preferable to use a docker Galaxy image rather than installing Galaxy itself locally, to limit issues with tool dependencies. \n\n\n\n\n\n\nYou would need to have Docker installed: see the \nDocker page\n.\n\n\n\n\n\n\nTry the \nDocker tutorial\n.\n\n\n\n\n\n\nThe docker container for Galaxy is at \nhttps://hub.docker.com/r/bgruening/galaxy-stable/\n\n\n\n\n\n\nTo run:\n\n\n\n\nOpen a terminal window. \n\n\nType \ndocker run -p 80:80 bgruening/galaxy-stable\n\n\nIn a web browser, enter \"localhost\" in the address bar.\n\n\nYour Galaxy should now be running. \n\n\n\n\n\n\n\n\nTo close the container:\n\n\n\n\nOpen a new terminal window.\n\n\nType \ndocker ps\n to show running containers.\n\n\nFind the container ID.\n\n\nType \ndocker stop \nCONTAINER ID", 
            "title": "The Galaxy platform"
        }, 
        {
            "location": "/guides/galaxy/#the-galaxy-platform", 
            "text": "Galaxy is a web platform for bioinformatics analysis.", 
            "title": "The Galaxy Platform"
        }, 
        {
            "location": "/guides/galaxy/#which-galaxy-should-i-use", 
            "text": "There are many different Galaxy servers - each one has a different web address.    For researchers based in Melbourne, Australia, we recommend you use  Galaxy-Mel .    The main worldwide Galaxy server is  https://usegalaxy.org/    Other Galaxy servers are listed  here .    Check the policy for the server you are using so that you know how long your data will be kept.     If you use more than one Galaxy server (e.g. Galaxy-Mel and useGalaxy.org) you need to register and log in separately for each server. They don't talk to each other.", 
            "title": "Which Galaxy should I use?"
        }, 
        {
            "location": "/guides/galaxy/#tutorials", 
            "text": "Go to Galaxy-Mel and log in, then try these tutorials:   Introduction to Galaxy    Galaxy Workflows    Other MB tutorials, e.g. Assembly, RNA-seq, etc.", 
            "title": "Tutorials"
        }, 
        {
            "location": "/guides/galaxy/#galaxy-training-network", 
            "text": "The Galaxy Training Network hosts a large collection of useful training material  here .  To get started, go to  https://usegalaxy.org/ , log in, then try these tutorials:    Introduction to Galaxy  Galaxy histories", 
            "title": "Galaxy Training Network"
        }, 
        {
            "location": "/guides/galaxy/#running-your-own-galaxy-server-in-the-cloud", 
            "text": "It is possible to run your own Galaxy instance using the Nectar cloud.     Tutorial to launch your own Galaxy    Nectar: https://nectar.org.au", 
            "title": "Running your own Galaxy server in the cloud"
        }, 
        {
            "location": "/guides/galaxy/#running-galaxy-locally-using-docker", 
            "text": "It is usually preferable to use a docker Galaxy image rather than installing Galaxy itself locally, to limit issues with tool dependencies.     You would need to have Docker installed: see the  Docker page .    Try the  Docker tutorial .    The docker container for Galaxy is at  https://hub.docker.com/r/bgruening/galaxy-stable/    To run:   Open a terminal window.   Type  docker run -p 80:80 bgruening/galaxy-stable  In a web browser, enter \"localhost\" in the address bar.  Your Galaxy should now be running.      To close the container:   Open a new terminal window.  Type  docker ps  to show running containers.  Find the container ID.  Type  docker stop  CONTAINER ID", 
            "title": "Running Galaxy locally using Docker"
        }, 
        {
            "location": "/guides/genome-assembly/", 
            "text": "Genome assembly\n\n\nA common project in bioinformatics is to assemble a genome. This means joining together all the pieces of the genome that were sequenced. \n\n\nMicrobial genome assembly\n\n\nBacteria have much smaller genomes than eukaryotes. \n\n\nTutorials \n\n\nEukaryotic genome assembly\n\n\nTutorials\n\n\nOrganelle genome assembly\n\n\nOrganelles are chloroplasts and mitochondria. These have their own genome, independent of the nuclear genome. \n\n\nTutorials", 
            "title": "Genome assembly"
        }, 
        {
            "location": "/guides/genome-assembly/#genome-assembly", 
            "text": "A common project in bioinformatics is to assemble a genome. This means joining together all the pieces of the genome that were sequenced.", 
            "title": "Genome assembly"
        }, 
        {
            "location": "/guides/genome-assembly/#microbial-genome-assembly", 
            "text": "Bacteria have much smaller genomes than eukaryotes.   Tutorials", 
            "title": "Microbial genome assembly"
        }, 
        {
            "location": "/guides/genome-assembly/#eukaryotic-genome-assembly", 
            "text": "Tutorials", 
            "title": "Eukaryotic genome assembly"
        }, 
        {
            "location": "/guides/genome-assembly/#organelle-genome-assembly", 
            "text": "Organelles are chloroplasts and mitochondria. These have their own genome, independent of the nuclear genome.   Tutorials", 
            "title": "Organelle genome assembly"
        }, 
        {
            "location": "/guides/RNA-seq/", 
            "text": "RNA-seq\n\n\nHow can I analyse my RNA-seq data to investigate differential gene expression? \n\n\nRNA-seq is often used to examine the genes expressed under two experimental conditions. If a gene is important for tolerating a particular condition, that gene may be highly expressed. Thus, there will be more copies of the gene's RNA transcript in that condition. \n\n\nTutorials\n\n\nIntro to Galaxy\nRNA-Seq DGE: Basic (Human)\nRNA-Seq DGE: Advanced \nRNA-Seq DGE: Experimental Design\nRNA-Seq DGE: in Galaxy (Microbial)\nRNA-Seq DGE: command line (Microbial)", 
            "title": "RNA-seq"
        }, 
        {
            "location": "/guides/RNA-seq/#rna-seq", 
            "text": "How can I analyse my RNA-seq data to investigate differential gene expression?   RNA-seq is often used to examine the genes expressed under two experimental conditions. If a gene is important for tolerating a particular condition, that gene may be highly expressed. Thus, there will be more copies of the gene's RNA transcript in that condition.   Tutorials  Intro to Galaxy\nRNA-Seq DGE: Basic (Human)\nRNA-Seq DGE: Advanced \nRNA-Seq DGE: Experimental Design\nRNA-Seq DGE: in Galaxy (Microbial)\nRNA-Seq DGE: command line (Microbial)", 
            "title": "RNA-seq"
        }, 
        {
            "location": "/guides/variant-calling/", 
            "text": "", 
            "title": "Variant calling"
        }, 
        {
            "location": "/guides/cancer-genomics/", 
            "text": "Cancer Genomics\n\n\nTutorials\n\n\n\n\nvariant calling?\n\n\nRNA-seq?\n\n\n\n\nis there a more specific cancer genomcis link? \nhttps://www.melbournebioinformatics.org.au/project/human-genomics/", 
            "title": "Cancer genomics"
        }, 
        {
            "location": "/guides/microbial_genomics/", 
            "text": "Sepsis tutorials:\n\n\nAssembly\n\n\nAnnotation\n\n\nVariant Calling\n\n\nhttps://www.melbournebioinformatics.org.au/project/microbial-genomics/", 
            "title": "Microbial genomics"
        }, 
        {
            "location": "/guides/more-help/", 
            "text": "More help\n\n\n\n\n\n\nMelbourne Bioinformatics online tutorials - see the left hand panel, under the Guides. \n\n\n\n\n\n\nMelbourne Bioinformatics offers several workshops throughout the year covering common workflows in bioinformatics - \nregister to attend\n.\n\n\n\n\n\n\nFor troubleshooting tools and methods we recommend \nSeqAnswers.com\n or \nBiostars.org\n.\n\n\n\n\n\n\nFor troubleshooting programming issues we recommend \nStackOverflow.com\n\n\n\n\n\n\nMelbourne Bioinformatics offers expert advice - submit a consultation request \nhere\n.\n\n\n\n\n\n\nMelbourne Bioinformatics also offers a \nsubscription service\n providing expertise and collaboration.", 
            "title": "More help"
        }, 
        {
            "location": "/guides/more-help/#more-help", 
            "text": "Melbourne Bioinformatics online tutorials - see the left hand panel, under the Guides.     Melbourne Bioinformatics offers several workshops throughout the year covering common workflows in bioinformatics -  register to attend .    For troubleshooting tools and methods we recommend  SeqAnswers.com  or  Biostars.org .    For troubleshooting programming issues we recommend  StackOverflow.com    Melbourne Bioinformatics offers expert advice - submit a consultation request  here .    Melbourne Bioinformatics also offers a  subscription service  providing expertise and collaboration.", 
            "title": "More help"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/", 
            "text": "Authors:\n\n\n\n\n\n\nBernie Pope, Melbourne Bioinformatics (formerly VLSCI)\n\n\n\n\n\n\nCatherine de Burgh-Day, Dept. of Physics, The University of Melbourne\n\n\n\n\n\n\nGeneral information\n\n\n\n\n\n\nPython modules are stored in files containing a \".py\" suffix (e.g solver.py).\n\n\n\n\n\n\nThe main implementation of Python is called CPython (it is written in C). It is byte-code interpreted.\n\n\n\n\n\n\nPython can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates\n  it immediately and then prints the result before prompting for a\n  new input. The interactive prompt is usually rendered as the\n  \nchevron\n \n. In scripted mode your program is stored in one\n  or more files which are executed as one monolithic entity. Such\n  programs behave like ordinary applications.\n\n\n\n\n\n\nPython has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed\n  automatically when no longer used.\n\n\n\n\n\n\nPython 2 versus Python 3\n\n\nCurrently there are two distinct flavours of Python available:\n\n\n\n\n\n\nPython 2 (2.7.10 at the time of writing)\n\n\n\n\n\n\nPython 3 (3.4.3 at the time of writing)\n\n\n\n\n\n\nPython 3 is the new and improved version of the language. Python 3 is\nnot entirely backwards compatible, but the two versions share much in\ncommon. Version 2 is now in maintenance mode; new features will only be\nadded to version 3. The public transition from 2 to 3 has been slower\nthan some people would like. You are encouraged to use version 3 where\npossible. These notes are generally compatible with both versions, but\nwe will point out key differences where necessary.\n\n\nIndentation for grouping code blocks\n\n\n\n\n\n\nPython uses indentation to group code blocks. Most other languages use some kind of brackets for grouping.\n\n\n\n\n\n\nThe recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth).\n\n\n\n\n\n\nYou are encouraged \nnot\n to use tabs for indentation because there is no standard width for a tab.\n\n\n\n\n\n\nMost good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.\n\n\n\n\n\n\nStyle Guide\n\n\nA popular style guide for Python is known as \nPEP 0008\n, there is a\ncorresponding tool called \npep8\n which will check your code against\nthe guide and report any transgressions.\n\n\nExample, Python compared to C:\n\n\nPython program for computing factorial:\n\n\n# Compute factorial of n,\n# assuming n \n= 0\n\ndef factorial(n):\n  result = 1\n  while n \n 0:\n      result *= n\n      n -= 1\n  return result\n\nprint(factorial(10))\n\n\n\n\nC program for computing factorial:\n\n\n#include \nstdio.h\n\n\n/* Compute factorial of n,\n  assuming n \n= 0 */\n\nint factorial(int n) {\n   int result = 1;\n\n   while (n \n 0) {\n      result *= n;\n      n -= 1;\n   }\n   return result;\n}\n\nint main(void) {\n   printf(\n%d\\n\n, factorial(10));\n}\n\n\n\n\nThings to note:\n\n\n\n\n\n\nThe difference in commenting style.\n\n\n\n\n\n\nC programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed.\n\n\n\n\n\n\nCode blocks in C are grouped by braces { }; Python uses indentation for grouping.\n\n\n\n\n\n\nThe C program must have a main function. Python does not require a\n    main function, it just executes the top-level statements of the\n    module.\n\n\n\n\n\n\nThe result returned by the C function is limited to the size of a\n    machine integer (say 32 bits). However, the result returned by the\n    Python function is unlimited in its size - it can compute\n    arbitrarily large factorials (up to the limit of the available\n    memory in your computer).\n\n\n\n\n\n\nComments\n\n\nProgram comments start with a hash character \"#\" and continue until the\nend of the line. There are no multi-line comment markers, but that can\nsometimes be faked with multi-line string literals.\n\n\nExamples:\n\n\n# This is a comment.\n# This is another comment.\nx = 5 # This is a comment that follows some code.\n'''This is\na multi-line\nstring literal\nwhich can sometimes act like\na\ncomment.\n'''\n\n\n\n\nRunning a Python program\n\n\nThere are many ways to run Python code:\n\n\n\n\n\n\nYou can run the interpreter in interactive mode. On Unix (Linux, OS\n    X) you can run the python command at the command line.\n\n\n\n\n\n\nIf you have Python code stores in a file, say example.py, you can\n    run it from the command line like so: python example.py\n\n\n\n\n\n\nYou can use one of several integrated programming environments.\n    Python ships with a fairly minimal one called \nIDLE\n, though\n    many scientists prefer the more comprehensive \nIPython\n.\n\n\n\n\n\n\nIf your Python code was installed as a package (see below), then it\n    may be executed like an ordinary application without the user\n    being aware of how the program was implemented.\n\n\n\n\n\n\nObjects and types\n\n\n\n\n\n\nEvery value in Python is an \nobject\n (including functions!).\n\n\n\n\n\n\nObjects can have attributes and methods, which are accessed via the\n    dot \".\" operator.\n\n\n\n\n\n\nAll objects have a type.\n\n\n\n\n\n\nTypes are also objects!\n\n\n\n\n\n\nPython is dynamically typed: you may get type errors at runtime but\n    never at compile time.\n\n\n\n\n\n\ntype(x)\n returns the type of x.\n\n\n\n\n\n\nPython variables may be assigned to values of different types at\n    different points in the program.\n\n\n\n\n\n\nInteractive examples (Python 3):\n\n\n # Create a list, assign to the variable x\n\n x = [3, 1, 2, 3]\n\n # Ask for the type of the value assigned to x\n\n type(x)\n\nclass 'list'\n\n\n # Ask for the type of the first item in the list (an integer)\n\n type(x[0])\n\nclass 'int'\n\n\n # Count the number of times 3 appears in the list\n\n # by calling the count method\n\n x.count(3)\n2\n\n # Sort the contents of the list in-place.\n\n # Note that this mutates the list object!\n\n # Also note that Python does not print the result in this case.\n\n x.sort()\n\n # Ask Python to show the value of the list\n\n # assigned to the variable x (note it is now sorted)\n\n x\n[1, 2, 3, 3]\n\n # Assign x to an object of a different type (a float)\n\n x = 3.142\n\n type(x)\n\nclass 'float'\n\n\n\n\n\nBooleans\n\n\n\n\nRepresent truth values\n\n\nValues: \nTrue\n, \nFalse\n\n\nType: \nbool\n\n\nOperators: \nand\n, \nor\n, \nnot\n\n\nbool(x)\n will convert x to a boolean. The heuristic is that empty things and zero-ish things are \nFalse\n, everything else is \nTrue\n (but the user can override for their own types).\n\n\nFalse\n values:\n\n\nFalse\n\n\n0\n (zero integer)\n\n\n0.0\n (zero float)\n\n\n{}\n (empty dictionary)\n\n\n()\n (empty tuple)\n\n\n[]\n (empty list)\n\n\n''\n (empty string)\n\n\nNone\n\n\n\n\n\n\nTrue\n values:\n\n\neverything else\n\n\n\n\n\n\n\n\n\n\nIn numerical contexts \nTrue\n is considered equal to the integer \n1\n and\n    \nFalse\n is considered equal to the integer \n0\n. However, these\n    conversions are a common cause of bugs and should be avoided.\n\n\nPython will automatically test the \ntruthiness\n of a value if it\n    appears in a boolean context.\n\n\n\n\nInteractive examples:\n\n\n not True\nFalse\n\n not False\nTrue\n\n not ()\nTrue\n\n not [1,2,3]\nFalse\n\n True and False\nFalse\n\n True and ()\n()\n\n\n\n\nConditional Statements\n\n\n\n\nConditional statements use the keywords: \nif\n, \nelif\n, \nelse\n. The syntax\n    for a conditional statement is:\n\n\n\n\nif expression:\n    statement-block\nelif expression:\n    statement-block\n...\nelse:\n    statement-block\n\n\n\n\n\n\n\n\nA conditional statement must have exactly one \nif\n part. It may have\n    zero or more \nelif\n parts, and a single optional \nelse\n part at the\n    end.\n\n\n\n\n\n\nThe \nif\n and \nelif\n parts test the value of their boolean expressions.\n    If the expression evaluates to something which is \nTrue\n or can be\n    converted to \nTrue\n (see the rules for Booleans above) then the\n    statement block immediately beneath that part is executed.\n    Otherwise the following condition (if any) is tried. The \nelse\n\n    part, if it exists, is always and only executed if no preceding\n    condition was \nTrue\n.\n\n\n\n\n\n\nInteractive examples:\n\n\n if []:\n...     print(\nWas considered True\n)\n... else:\n...     print(\nWas considered False\n)\n...\nWas considered False\n\n\n\n\nNumbers and basic mathematics\n\n\nIntegers\n\n\n\n\n\n\nRepresent whole negative and positive numbers (and zero).\n\n\n\n\n\n\nThe range of integer values is unbounded (up to some limit defined\n    by how much memory you have on your computer).\n\n\n\n\n\n\nPython 2 distinguishes between two integer types \nint\n and \nlong\n, and\n    automatically promotes \nint\n to long where necessary, whereas Python\n    3 considers them all one type called \nint\n.\n\n\n\n\n\n\nBase ten is the default literal notation: \n42\n (means \n(4 * 10) + 2\n)\n\n\n\n\n\n\nHexadecimal literals start with \n0x\n, octal literals start with \n0o\n,\n    binary literals start with \n0b\n.\n\n\n\n\n\n\nint(x)\n will try to convert x to an integer, x can be another numeric\n    type (including booleans) or a string. You may specify an optional\n    base for the conversion.\n\n\n\n\n\n\nInteractive examples (in Python 3):\n\n\n 2 ** 200\n1606938044258990275541962092341162602522202993782792835301376\n\n 0x10\n16\n\n 0b10\n2\n\n -0 == 0\nTrue\n\n int(\n123\n)\n123\n\n int(\n3.142\n)\nTraceback (most recent call last):\nFile \nstdin\n, line 1, in \nmodule\n\n    ValueError: invalid literal for int() with base 10: '3.142'\n\n\n\n\nFloating Point Numbers\n\n\n\n\n\n\nRepresent a finite approximation to the real numbers.\n\n\n\n\n\n\nType: \nfloat\n.\n\n\n\n\n\n\n(On most platforms) Python uses IEEE-754 double precision floating\n    point numbers which provide 53 bits of precision.\n\n\n\n\n\n\nsys.float_info\n contains details about max, min, epsilon etcetera.\n\n\n\n\n\n\nLiterals can be in ordinary notation or in exponential notation:\n\n\n\n\nOrdinary: \n3.142\n\n\nExponential: \n314.2e-2\n\n\n\n\n\n\n\n\nOrdinary notation requires a point \n.\n, but digits following the\n    point are optional.\n\n\n\n\n\n\nExponential notation does not require a point unless you have a\n    fractional component.\n\n\n\n\n\n\nfloat(x)\n will try to convert x to a floating point number, x can be\n    another numeric type (including booleans) or a string.\n\n\n\n\n\n\nNumeric operators will automatically convert integer arguments to\n    floating point in mixed-type expressions.\n\n\n\n\n\n\nIn Python 3 the division operator \n/\n computes a floating point result\n    for integer arguments. However, in Python 2 it computes an integer\n    result for integer arguments.\n\n\n\n\n\n\nInteractive examples:\n\n\n type(3.142)\n\nclass 'float'\n\n\n type(12)\n\nclass 'int'\n\n\n 3.142 + 12\n15.142\n\n 3.142 == 314.2e-2\nTrue\n\n 3. == 3.0\nTrue\n\n 1/0\nTraceback (most recent call last):\nFile \nstdin\n, line 1, in \nmodule\n\nZeroDivisionError: division by zero\n\n # Integer divided by integer yields a float in Python 3\n\n 10 / 3\n3.3333333333333335\n\n float(\n123\n)\n123.0\n\n float(\n3.142\n)\n3.142\n\n\n\n\nComplex Numbers\n\n\n\n\n\n\nRepresent a finite approximation to the complex numbers.\n\n\n\n\n\n\nType: \ncomplex\n\n\n\n\n\n\nA pair of floating point numbers: real +/- imaginary.\n\n\n\n\n\n\nThe real part is optional (defaults to 0). The imaginary part is\n    followed immediately by the character \nj\n.\n\n\n\n\n\n\nInteractive Examples:\n\n\n 5j + 3j\n8j\n\n 2-5j\n(2-5j)\n\n 2-5j + 3j\n(2-2j)\n\n\n\n\nNumeric Operators\n\n\n\n\n\n\n\n\nName\n\n\nOperation\n\n\nPrecedence\n\n\nAssociativity\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n+\n\n\nadd\n\n\nlow\n\n\nleft\n\n\nCan also be used to concatenate strings together.\n\n\n\n\n\n\n*\n\n\nmultiply\n\n\nmedium\n\n\nleft\n\n\n\n\n\n\n\n\n-\n\n\nsubtract\n\n\nlow\n\n\nleft\n\n\n\n\n\n\n\n\n/\n\n\ndivide\n\n\nmedium\n\n\nleft\n\n\nIn Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers.\n\n\n\n\n\n\n//\n\n\nfloor-divide\n\n\nmedium\n\n\nleft\n\n\ndivide then floor, result is an integer\n\n\n\n\n\n\n**\n\n\nexponent\n\n\nhigh\n\n\nright\n\n\n\n\n\n\n\n\n%\n\n\nmodulus\n\n\nmedium\n\n\nleft\n\n\nremainder after division\n\n\n\n\n\n\n\n\nInteractive Examples (Python 3):\n\n\n 3 + 4 * 5\n23\n\n (3 + 4) * 5\n35\n\n 10 / 3\n3.3333333333333335\n\n 10 // 3\n3\n\n 10 % 3\n1\n\n 2 ** 3 ** 4\n2417851639229258349412352\n\n (2 ** 3) ** 4\n4096\n\n\n\n\nStrings\n\n\n\n\n\n\nRepresent text\n\n\n\n\n\n\nType: \nstr\n\n\n\n\n\n\nIn Python 3, the str type contains Unicode characters.\n\n\n\n\n\n\nIn Python 2, the str type contains ASCII characters (sometimes\n    called byte strings). Python 2 has a separate type for unicode\n    strings, the type is called unicode; literals of this type are\n    prefixed by the letter \nu\n.\n\n\n\n\n\n\nString literals must be quoted. There are 3 quoting styles:\n\n\n\n\n\n\nsingle quote characters: \n'hello'\n\n\n\n\n\n\ndouble quote characters: \n\"hello\"\n\n\n\n\n\n\ntriple quote characters: \n'''hello'''\n (three single quotes in a\n    row) or \n\"\"\"hello\"\"\"\n (three double quote characters in a row)\n\n\n\n\n\n\n\n\n\n\nThe single quote and double quote versions of strings have the same\n    value. The purpose of the different quotation styles is to make it\n    convenient to have literal quotation marks inside strings\n    (avoiding the need to escape the quote character). For example:\n\n\n\n\n\n\n \nThis inverted comma won't be a problem inside quotation marks\n\n\nThis inverted comma won't be a problem inside quotation marks\n\n\n 'this \nquote\n will work'\n'this \nquote\n will work'\n\n 'this isn't going to work though'\nFile \nstdin\n, line 1\n'this isn't going to work though'\n^ SyntaxError: invalid syntax\n\n\n\n\n\n\n\n\nTriple quoted strings can be written on multiple lines. The line\n    breaks will be preserved within the string. Useful for docstrings\n    (see section on functions).\n\n\n\n\n\n\nThe usual set of escape characters are supported:\n\n\n\n\n\n\n\\n\n newline\n\n\n\n\n\n\n\\t\n tab\n\n\n\n\n\n\n\\\\\n backslash\n\n\n\n\n\n\n\\'\n single quote\n\n\n\n\n\n\n\\\"\n double quote\n\n\n\n\n\n\nand many more\n\n\n\n\n\n\n\n\n\n\nPython does not have a separate type for representing individual\n    characters. Instead you use strings of length one.\n\n\n\n\n\n\nStrings are iterable. If you iterate over a string (using a for\n    loop) you process it one character at a time from left to right.\n\n\n\n\n\n\nStrings can be indexed to obtain individual characters, e.g. \ns[5]\n\n\n\n\n\n\nIndices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the string).\n\n\n\n\n\n\nStrings are immutable: you cannot modify a string once it has been\n    created.\n\n\n\n\n\n\nInteractive Examples (Python 3):\n\n\n type(\nhello\n)\n\nclass 'str'\n\n\n \nhello\n == 'hello'\nTrue\n\n '''This string\n... is on\n... multiple\n... lines'''\n'This string\\\\nis on\\\\nmultiple\\\\nlines'\n\n \nbonjour\n.upper()\n'BONJOUR'\n\n len(\nbonjour\n)\n7\n\n \nbonjour\n.startswith(\nb\n)\nTrue\n\n \ncat,sat,flat\n.split(\n,\n)\n['cat', 'sat', 'flat']\n\n # Print the first 5 Chinese unicode characters\n\n print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04')\n\u4e00\u4e01\u4e02\u4e03\u4e04\n\n x = \nfloyd\n\n\n x[0]\n'f'\n\n \nhello\n + \n \n + \nworld\n\n'hello world'\n\n\n\n\nExample program:\n\n\n# Prompt the user to input a string:\ninput = raw_input(\nEnter string: \n)\n\n# Count the number of vowels in the input string\nvowels = 'aeiou'\ncount = 0\n\nfor char in input:\n    if char in vowels:\n        count += 1\n\n# Print the count to the standard output\nprint(count)\n\n\n\n\nExample usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called \nvowels.py\n:\n\n\npython vowels.py\nEnter string: abracadabra\n5\n\n\n\n\nLists\n\n\n\n\n\n\nRepresent mutable ordered sequences of values.\n\n\n\n\n\n\nType: \nlist\n\n\n\n\n\n\nList literals are written in between square brackets, e.g. \n[1, 2, 3]\n\n\n\n\n\n\nList elements can be objects of any type (including other lists).\n\n\n\n\n\n\nLike strings, lists can be indexed like so: \nx[3]\n\n\n\n\n\n\nIndices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the list).\n\n\n\n\n\n\nLists are mutable. You can update items, delete items and add new\n    items.\n\n\n\n\n\n\nIndexing into a list is a constant time (amortised) operation.\n\n\n\n\n\n\nInteractive Examples:\n\n\n type([1, 2, 3])\n\nclass 'list'\n\n\n x = []\n\n len(x)\n0\n\n x.append(\nhello\n)\n\n x\n['hello']\n\n len(x)\n1\n\n x[0]\n'hello'\n\n x.insert(0, True)\n\n x\n[True, 'hello']\n\n del x[1]\n\n x\n[True]\n\n x += [42, \nNewton\n, 3.142]\n\n x\n[True, 42, 'Newton', 3.142]\n\n\n\n\nDictionaries\n\n\n\n\n\n\nRepresent finite mappings from keys to values.\n\n\n\n\n\n\nAre implemented as \nhash tables\n. The key objects must be hashable\n    (which rules out mutable objects, such as lists).\n\n\n\n\n\n\nType: \ndict\n\n\n\n\n\n\nDictionary literals are written inside curly brackets, with\n    key-value pairs separated by colons: e.g. \n{12: \"XII\", 6: \"VI\"}\n\n\n\n\n\n\nDictionaries can be indexed by keys. If the key exists in the\n    dictionary its corresponding value is returned, otherwise a\n    \nKeyError\n exception is raised.\n\n\n\n\n\n\nThe cost of indexing a dictionary is proportional to the time taken\n    to hash the key. For many keys this can be considered constant\n    time. For variable sized objects, such as strings, this can be\n    considered to be proportional to the size of the object.\n\n\n\n\n\n\nIterating over a dictionary yields one key at a time. All keys in\n    the dictionary are visited exactly once. The order in which the\n    keys are visited is arbitrary.\n\n\n\n\n\n\nYou may test if an object is a key of a dictionary using the in\n    operator.\n\n\n\n\n\n\nInteractive Examples:\n\n\n type({12: \nXII\n, 6: \nVI\n})\n\nclass 'dict'\n\n\n friends = {}\n\n friends['Fred'] = ['Barney', 'Dino']\n\n friends\n{'Fred': ['Barney', 'Dino']}\n\n friends['Fred']\n['Barney', 'Dino']\n\n friends['Barney']\nTraceback (most recent call last):\nFile \n\\\nstdin\\\n, line 1, in \\\nmodule\\\n\nKeyError: 'Barney'\n\n friends['Wilma'] = ['Betty']\n\n friends\n{'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']}\n\n friends.keys()\ndict_keys(['Fred', 'Wilma'])\n\n friends.values()\ndict_values([['Barney', 'Dino'], ['Betty']])\n\n 'Dino' in friends\nFalse\n\n\n\n\nExample program:\n\n\n# Compute and print a histogram of a sequence of integers entered\n# on standard input, one number per line\n\nimport sys\n\nhistogram = {}\n\n# Iterate over each line in the standard input\nfor line in sys.stdin:\n    # Parse the next input as an integer\n    next_integer = int(line)\n    # Update the histogram accordingly\n    if next_integer in histogram:\n        # We've seen this integer before\n        histogram[next_integer] += 1\n    else:\n        # First occurrence of this integer in the input\n        histogram[next_integer] = 1\n\n# Print each key: value pair in the histogram in ascending\n# sorted order of keys\nfor key in sorted(histogram):\n    print(\n{} {}\n.format(key, histogram[key]))s\n\n\n\n\nExample usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called \nhisto.py\n:\n\n\npython histo.py\n\n\n\n\nUser types in a sequence of integers to the program, one per line, and\npresses control-d to terminate the input:\n\n\n3\n43\n12\n19\n3\n12\n12\n43\n\n\n\n\nProgram prints its output:\n\n\n3 2\n12 3\n19 1\n43 2\n\n\n\n\nTuples\n\n\n\n\n\n\nRepresent \nimmutable\n ordered sequences of values.\n\n\n\n\n\n\nVery much like lists except they cannot be modified once created.\n\n\n\n\n\n\nType: \ntuple\n\n\n\n\n\n\nLiterals are written in between parentheses: \n(1, 2, 3)\n\n\n\n\n\n\nThe can be used as keys in dictionaries (unlike lists).\n\n\n\n\n\n\nLoops\n\n\nWhile loops\n\n\n\n\n\n\nIterate until condition is \nFalse\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nwhile expression:\n    statement_block\n\n\n\n\n\n\nThe value of the boolean expression is tested. If it evaluates to\n    \nTrue\n then the statement block is executed once, before repeating\n    the loop. If it evaluates to False then the program continues\n    execution immediately after the loop.\n\n\n\n\nExample:\n\n\ndef factorial(n):\n    result = 1\n    while n \n 0:\n        result *= n\n        n -= 1\n    return result\n\n\n\n\nFor loops\n\n\n\n\n\n\nIterate over each item in a collection (e.g. list, string, tuple,\n    dictionary, file).\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nfor variable in expression:\n    statement_block\n\n\n\n\n\n\n\n\nEach item from the iterator expression is selected and assigned to\n    the variable, then the statement block is executed. The loop ends\n    when every item in the iterator has been visited.\n\n\n\n\n\n\nThe order of items visited in the iterator depends on the type of\n    the iterator. Lists, strings and tuples proceed in a left-to-right\n    fashion. Files proceed one line at a time. Dictionaries proceed in\n    an arbitrary order.\n\n\n\n\n\n\nThe \nrange()\n function is useful for generating iterators of numbers\n    within a range. Note that the lower bound is inclusive and the\n    upper bound is exclusive.\n\n\n\n\n\n\nExample:\n\n\ndef factorial(n):\n    result = 1\n    for item in range(n + 1):\n        result *= item\n    return result\n\n\n\n\nBreak and continue\n\n\n\n\n\n\nBoth types of loops support the \nbreak\n and \ncontinue\n keywords.\n\n\n\n\n\n\nbreak\n terminates the loop immediately.\n\n\n\n\n\n\ncontinue\n jumps immediately back to the start of the loop.\n\n\n\n\n\n\nThey can sometimes simplify the conditional logic of a loop, but\n    should be used sparingly.\n\n\n\n\n\n\nFunctions\n\n\n\n\n\n\nAllow you to define reusable abstractions. Sometimes called\n    \nprocedures\n.\n\n\n\n\n\n\nAre generally defined at the top level of a module, and can also be\n    nested.\n\n\n\n\n\n\nType: \nfunction\n\n\n\n\n\n\nNamed functions are bound to a variable name and may have complex\n    bodies.\n\n\n\n\n\n\nAnonymous functions are used in-line, and may only have expression\n    bodies.\n\n\n\n\n\n\nNamed function syntax:\n\n\n\n\n\n\ndef variable(parameter_list):\n    statement_block\n\n\n\n\n\n\nAnonymous function syntax:\n\n\n\n\nlambda parameter_list: expression\n\n\n\n\nExample:\n\n\ndef is_leap_year(year):\n    if year % 4 == 0 and year % 100 != 0:\n        return True\n    else:\n        return year % 400 == 0\n\nfor year in range(2000, 2100 + 1):\n    result = is_leap_year(year)\n    print(\n{} {}\n.format(year, result))\n\n\n\n\nAnonymous function example:\n\n\n squared = lambda x: x ** 2\n\n squared(2)\n4\n\n list(map(lambda x: x + 1, [1, 2, 3]))\n[2, 3, 4]\n\n\n\n\nInput and output\n\n\n\n\n\n\nThe \nprint\n function is useful for displaying text (and other values\n    converted to text).\n\n\n\n\n\n\nIn Python 2 \nprint\n was a special keyword. In Python 3 it is a\n    function defined in the builtins.\n\n\n\n\n\n\nFancy string formatting can be done with the format method on\n    strings. Older Python code uses the string interpolation operator\n    for the same task \n%\n, but its use is now discouraged.\n\n\n\n\n\n\nFiles must be opened before than can be manipulated. A file can be\n    opened in different modes: read \n\"r\"\n, write \n\"w\"\n, read-write \n\"r+\"\n,\n    and append \n\"a\"\n. Opening a new file in write or append modes\n    creates a new file. Opening an existing file in write mode\n    overwrites its contents from the start. Opening an existing file\n    in append mode adds new content at the end of the old content.\n\n\n\n\n\n\nWhen you are finished processing a file you should close it as soon\n    as possible. Closing a file releases limited operating system\n    resources, and ensures that any pending buffered writes a flushed\n    to the storage system.\n\n\n\n\n\n\nCertain file types have libraries for convenient processing. One\n    example is the CSV (comma separated values) library for processing\n    tabular data. It is very handy for working with spreadsheets.\n\n\n\n\n\n\nThe command line arguments of a Python program are contained in a\n    list called \nsys.argv\n (it is a variable exported from the \nsys\n\n    module). For complex program you should consider using a command\n    line argument parsing library such as \nargparse\n.\n\n\n\n\n\n\nExample program:\n\n\n# Count the number of words and lines in a file\n\nimport sys\n\n# Get the input file name from the command line arguments\nfilename = sys.argv[1]\n\n# Open the file\nfile = open(filename)\n\n# Count the number of lines in the file\nnum_lines = 0\nnum_words = 0\n\nfor line in file:\n    num_lines += 1\n    num_words += len(line.split())\n\nfile.close()\n\nprint(\nNumber of lines and words in {}: {} {}\n \\\n        .format(filename, num_lines, num_words))\n\n\n\n\nAdvanced Topics\n\n\nClasses\n\n\n\n\n\n\nClasses allow you to define your own types.\n\n\n\n\n\n\nClass definitions may define methods for the type.\n\n\n\n\n\n\nA class may inherit, and possibly override, some functionality from\n    a superclass.\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nclass variable(superclass_list):\n    body\n\n\n\n\n\n\n\n\nThe name of the class is given by the variable in the definition.\n    The superclass list defines the superclasses of the new class\n    (very often the base type object is used). The body of the class\n    typically defines one or more methods.\n\n\n\n\n\n\nInstances of classes are created by calling the class name as if it\n    were a function.\n\n\n\n\n\n\nIf defined, the special method called \n__init__\n is used to\n    initialise a newly created instance of a class.\n\n\n\n\n\n\nThe first parameter to each method is the object upon which the\n    method was called. The convention is to use the variable called\n    self, however any variable name will do. Many object oriented\n    languages make this variable an implicit parameter called this.\n\n\n\n\n\n\nExample:\n\n\nclass Vector(object):\n    def __init__(self, x=0, y=0, z=0):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def magnitude(self):\n        return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2)\n\n    def normalise(self):\n        magnitude = self.magnitude()\n        if magnitude == 0:\n            # Somehow we have a degenerate vector.\n            return self\n        else:\n            return self / self.magnitude()\n\n    def angle(self, other):\n        dp = self.dot_product(other)\n        return acos(dp / self.magnitude() * other.magnitude())\n\n    def dot_product(self, other):\n        return self.x * other.x + self.y * other.y + self.z * other.z\n\n\n\n\nExceptions\n\n\n\n\n\n\nExceptions allow Python programs to handle erroneous program\n    conditions.\n\n\n\n\n\n\nAn exception is raised (or thrown) at the point of the error and\n    handled (or caught) at some other place in the program.\n\n\n\n\n\n\nException handlers have the syntax:\n\n\n\n\n\n\ntry:\n    statement_block\nexcept exception_type as variable:\n    statement_block\n...\n\n\n\n\n\n\n\n\nThe statement block after \ntry\n is executed. If no exceptions are\n    raised in that block the program continues immediately after the\n    exception handler. If an exception is raised in the block then\n    program control jumps to the innermost closing \nexcept\n clause.\n    Except clauses may optionally specify the set of exception types\n    that they can handle. If the raised exception is an instance of\n    the handled type then the body of the except clause is executed,\n    otherwise the next except clause (if any) is tried.\n\n\n\n\n\n\nIf no matching exception handler is found then the program will\n    terminate with an unhandled exception error. Python will normally\n    print a stack trace at this point for error diagnosis.\n\n\n\n\n\n\nYou may raise your own exceptions using the \nraise\n keyword.\n\n\n\n\n\n\nExample:\n\n\n# alternative version of the histogram code from the section on\n# dictionaries\nfor line in sys.stdin:\n    next_integer = int(line)\n    try:\n        histogram[next_integer] += 1\n    except KeyError:\n        histogram[next_integer] = 1\n\n\n\n\nModules\n\n\n\n\n\n\nA module is a file which contains Python code.\n\n\n\n\n\n\nAny Python file you create is automatically a module.\n\n\n\n\n\n\nIt is considered good programming style to decompose complex\n    programs into multiple modules. Each module should collect\n    together code with similar purpose.\n\n\n\n\n\n\nVariables defined at the top level of a module (such as global\n    variables, functions and classes) can be imported into other\n    modules.\n\n\n\n\n\n\nPython comes with many standard modules.\n\n\n\n\n\n\nThe \nimport\n keyword is used to import an entire module.\n\n\n\n\n\n\nYou may import a subset of things from a module using the \nfrom ... import ...\n syntax.\n\n\n\n\n\n\nYou may import a module with a new name using the \nfrom ... import ... as ...\n or \nimport ... as ...\n\n\n\n\n\n\nWhen a module is first imported in a program, all of its top-level\n    statements are executed from top to bottom. Subsequent imports use\n    a cached version of its definitions, its statements are not\n    re-executed.\n\n\n\n\n\n\nA special module called \nbuiltins\n is imported into every other module\n    by default, and it is automatically imported at the interactive\n    prompt in the interpreter.\n\n\n\n\n\n\nInteractive Example:\n\n\n import math\n\n math.sqrt(100)\n10.0\n\n sqrt(100)\nTraceback (most recent call last):\nFile \nstdin\n, line 1, in \nmodule\n\nNameError: name 'sqrt' is not defined\n\n from math import sqrt\n\n sqrt(100)\n10.0\n\n import math as m\n\n m.sqrt(100)\n10.0\n\n\n\n\nPackages\n\n\n\n\n\n\nA package is a collection of modules in a hierarchy.\n\n\n\n\n\n\nPackages are the common way to structure Python libraries.\n\n\n\n\n\n\nThe \nPython Package Index (PyPI)\n is a big collection of open\n    source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of\n    writing).\n\n\n\n\n\n\nPackage installation tools such as \npip\n, make it easy to install\n    packages onto your computer.\n\n\n\n\n\n\nIf you want to make your own Python code easy for others to install\n    and use then you should consider making it a package. You can even\n    upload it to PyPI.\n\n\n\n\n\n\nMany people use \nvirtualenv\n to install packages into a local\n    \"sandboxed\" Python environment. This avoids conflicts with the\n    central Python package database on your computer, and allows\n    multiple different versions of packages to be used.", 
            "title": "Python Overview"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#authors", 
            "text": "Bernie Pope, Melbourne Bioinformatics (formerly VLSCI)    Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne", 
            "title": "Authors:"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#general-information", 
            "text": "Python modules are stored in files containing a \".py\" suffix (e.g solver.py).    The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted.    Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates\n  it immediately and then prints the result before prompting for a\n  new input. The interactive prompt is usually rendered as the\n   chevron   . In scripted mode your program is stored in one\n  or more files which are executed as one monolithic entity. Such\n  programs behave like ordinary applications.    Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed\n  automatically when no longer used.", 
            "title": "General information"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#python-2-versus-python-3", 
            "text": "Currently there are two distinct flavours of Python available:    Python 2 (2.7.10 at the time of writing)    Python 3 (3.4.3 at the time of writing)    Python 3 is the new and improved version of the language. Python 3 is\nnot entirely backwards compatible, but the two versions share much in\ncommon. Version 2 is now in maintenance mode; new features will only be\nadded to version 3. The public transition from 2 to 3 has been slower\nthan some people would like. You are encouraged to use version 3 where\npossible. These notes are generally compatible with both versions, but\nwe will point out key differences where necessary.", 
            "title": "Python 2 versus Python 3"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#indentation-for-grouping-code-blocks", 
            "text": "Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping.    The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth).    You are encouraged  not  to use tabs for indentation because there is no standard width for a tab.    Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.", 
            "title": "Indentation for grouping code blocks"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#style-guide", 
            "text": "A popular style guide for Python is known as  PEP 0008 , there is a\ncorresponding tool called  pep8  which will check your code against\nthe guide and report any transgressions.  Example, Python compared to C:  Python program for computing factorial:  # Compute factorial of n,\n# assuming n  = 0\n\ndef factorial(n):\n  result = 1\n  while n   0:\n      result *= n\n      n -= 1\n  return result\n\nprint(factorial(10))  C program for computing factorial:  #include  stdio.h \n\n/* Compute factorial of n,\n  assuming n  = 0 */\n\nint factorial(int n) {\n   int result = 1;\n\n   while (n   0) {\n      result *= n;\n      n -= 1;\n   }\n   return result;\n}\n\nint main(void) {\n   printf( %d\\n , factorial(10));\n}  Things to note:    The difference in commenting style.    C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed.    Code blocks in C are grouped by braces { }; Python uses indentation for grouping.    The C program must have a main function. Python does not require a\n    main function, it just executes the top-level statements of the\n    module.    The result returned by the C function is limited to the size of a\n    machine integer (say 32 bits). However, the result returned by the\n    Python function is unlimited in its size - it can compute\n    arbitrarily large factorials (up to the limit of the available\n    memory in your computer).", 
            "title": "Style Guide"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#comments", 
            "text": "Program comments start with a hash character \"#\" and continue until the\nend of the line. There are no multi-line comment markers, but that can\nsometimes be faked with multi-line string literals.  Examples:  # This is a comment.\n# This is another comment.\nx = 5 # This is a comment that follows some code.\n'''This is\na multi-line\nstring literal\nwhich can sometimes act like\na\ncomment.\n'''", 
            "title": "Comments"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#running-a-python-program", 
            "text": "There are many ways to run Python code:    You can run the interpreter in interactive mode. On Unix (Linux, OS\n    X) you can run the python command at the command line.    If you have Python code stores in a file, say example.py, you can\n    run it from the command line like so: python example.py    You can use one of several integrated programming environments.\n    Python ships with a fairly minimal one called  IDLE , though\n    many scientists prefer the more comprehensive  IPython .    If your Python code was installed as a package (see below), then it\n    may be executed like an ordinary application without the user\n    being aware of how the program was implemented.", 
            "title": "Running a Python program"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#objects-and-types", 
            "text": "Every value in Python is an  object  (including functions!).    Objects can have attributes and methods, which are accessed via the\n    dot \".\" operator.    All objects have a type.    Types are also objects!    Python is dynamically typed: you may get type errors at runtime but\n    never at compile time.    type(x)  returns the type of x.    Python variables may be assigned to values of different types at\n    different points in the program.    Interactive examples (Python 3):   # Create a list, assign to the variable x  x = [3, 1, 2, 3]  # Ask for the type of the value assigned to x  type(x) class 'list'   # Ask for the type of the first item in the list (an integer)  type(x[0]) class 'int'   # Count the number of times 3 appears in the list  # by calling the count method  x.count(3)\n2  # Sort the contents of the list in-place.  # Note that this mutates the list object!  # Also note that Python does not print the result in this case.  x.sort()  # Ask Python to show the value of the list  # assigned to the variable x (note it is now sorted)  x\n[1, 2, 3, 3]  # Assign x to an object of a different type (a float)  x = 3.142  type(x) class 'float'", 
            "title": "Objects and types"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#booleans", 
            "text": "Represent truth values  Values:  True ,  False  Type:  bool  Operators:  and ,  or ,  not  bool(x)  will convert x to a boolean. The heuristic is that empty things and zero-ish things are  False , everything else is  True  (but the user can override for their own types).  False  values:  False  0  (zero integer)  0.0  (zero float)  {}  (empty dictionary)  ()  (empty tuple)  []  (empty list)  ''  (empty string)  None    True  values:  everything else      In numerical contexts  True  is considered equal to the integer  1  and\n     False  is considered equal to the integer  0 . However, these\n    conversions are a common cause of bugs and should be avoided.  Python will automatically test the  truthiness  of a value if it\n    appears in a boolean context.   Interactive examples:   not True\nFalse  not False\nTrue  not ()\nTrue  not [1,2,3]\nFalse  True and False\nFalse  True and ()\n()", 
            "title": "Booleans"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#conditional-statements", 
            "text": "Conditional statements use the keywords:  if ,  elif ,  else . The syntax\n    for a conditional statement is:   if expression:\n    statement-block\nelif expression:\n    statement-block\n...\nelse:\n    statement-block    A conditional statement must have exactly one  if  part. It may have\n    zero or more  elif  parts, and a single optional  else  part at the\n    end.    The  if  and  elif  parts test the value of their boolean expressions.\n    If the expression evaluates to something which is  True  or can be\n    converted to  True  (see the rules for Booleans above) then the\n    statement block immediately beneath that part is executed.\n    Otherwise the following condition (if any) is tried. The  else \n    part, if it exists, is always and only executed if no preceding\n    condition was  True .    Interactive examples:   if []:\n...     print( Was considered True )\n... else:\n...     print( Was considered False )\n...\nWas considered False", 
            "title": "Conditional Statements"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#numbers-and-basic-mathematics", 
            "text": "", 
            "title": "Numbers and basic mathematics"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#integers", 
            "text": "Represent whole negative and positive numbers (and zero).    The range of integer values is unbounded (up to some limit defined\n    by how much memory you have on your computer).    Python 2 distinguishes between two integer types  int  and  long , and\n    automatically promotes  int  to long where necessary, whereas Python\n    3 considers them all one type called  int .    Base ten is the default literal notation:  42  (means  (4 * 10) + 2 )    Hexadecimal literals start with  0x , octal literals start with  0o ,\n    binary literals start with  0b .    int(x)  will try to convert x to an integer, x can be another numeric\n    type (including booleans) or a string. You may specify an optional\n    base for the conversion.    Interactive examples (in Python 3):   2 ** 200\n1606938044258990275541962092341162602522202993782792835301376  0x10\n16  0b10\n2  -0 == 0\nTrue  int( 123 )\n123  int( 3.142 )\nTraceback (most recent call last):\nFile  stdin , line 1, in  module \n    ValueError: invalid literal for int() with base 10: '3.142'", 
            "title": "Integers"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#floating-point-numbers", 
            "text": "Represent a finite approximation to the real numbers.    Type:  float .    (On most platforms) Python uses IEEE-754 double precision floating\n    point numbers which provide 53 bits of precision.    sys.float_info  contains details about max, min, epsilon etcetera.    Literals can be in ordinary notation or in exponential notation:   Ordinary:  3.142  Exponential:  314.2e-2     Ordinary notation requires a point  . , but digits following the\n    point are optional.    Exponential notation does not require a point unless you have a\n    fractional component.    float(x)  will try to convert x to a floating point number, x can be\n    another numeric type (including booleans) or a string.    Numeric operators will automatically convert integer arguments to\n    floating point in mixed-type expressions.    In Python 3 the division operator  /  computes a floating point result\n    for integer arguments. However, in Python 2 it computes an integer\n    result for integer arguments.    Interactive examples:   type(3.142) class 'float'   type(12) class 'int'   3.142 + 12\n15.142  3.142 == 314.2e-2\nTrue  3. == 3.0\nTrue  1/0\nTraceback (most recent call last):\nFile  stdin , line 1, in  module \nZeroDivisionError: division by zero  # Integer divided by integer yields a float in Python 3  10 / 3\n3.3333333333333335  float( 123 )\n123.0  float( 3.142 )\n3.142", 
            "title": "Floating Point Numbers"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#complex-numbers", 
            "text": "Represent a finite approximation to the complex numbers.    Type:  complex    A pair of floating point numbers: real +/- imaginary.    The real part is optional (defaults to 0). The imaginary part is\n    followed immediately by the character  j .    Interactive Examples:   5j + 3j\n8j  2-5j\n(2-5j)  2-5j + 3j\n(2-2j)", 
            "title": "Complex Numbers"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#numeric-operators", 
            "text": "Name  Operation  Precedence  Associativity  Notes      +  add  low  left  Can also be used to concatenate strings together.    *  multiply  medium  left     -  subtract  low  left     /  divide  medium  left  In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers.    //  floor-divide  medium  left  divide then floor, result is an integer    **  exponent  high  right     %  modulus  medium  left  remainder after division     Interactive Examples (Python 3):   3 + 4 * 5\n23  (3 + 4) * 5\n35  10 / 3\n3.3333333333333335  10 // 3\n3  10 % 3\n1  2 ** 3 ** 4\n2417851639229258349412352  (2 ** 3) ** 4\n4096", 
            "title": "Numeric Operators"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#strings", 
            "text": "Represent text    Type:  str    In Python 3, the str type contains Unicode characters.    In Python 2, the str type contains ASCII characters (sometimes\n    called byte strings). Python 2 has a separate type for unicode\n    strings, the type is called unicode; literals of this type are\n    prefixed by the letter  u .    String literals must be quoted. There are 3 quoting styles:    single quote characters:  'hello'    double quote characters:  \"hello\"    triple quote characters:  '''hello'''  (three single quotes in a\n    row) or  \"\"\"hello\"\"\"  (three double quote characters in a row)      The single quote and double quote versions of strings have the same\n    value. The purpose of the different quotation styles is to make it\n    convenient to have literal quotation marks inside strings\n    (avoiding the need to escape the quote character). For example:      This inverted comma won't be a problem inside quotation marks  This inverted comma won't be a problem inside quotation marks   'this  quote  will work'\n'this  quote  will work'  'this isn't going to work though'\nFile  stdin , line 1\n'this isn't going to work though'\n^ SyntaxError: invalid syntax    Triple quoted strings can be written on multiple lines. The line\n    breaks will be preserved within the string. Useful for docstrings\n    (see section on functions).    The usual set of escape characters are supported:    \\n  newline    \\t  tab    \\\\  backslash    \\'  single quote    \\\"  double quote    and many more      Python does not have a separate type for representing individual\n    characters. Instead you use strings of length one.    Strings are iterable. If you iterate over a string (using a for\n    loop) you process it one character at a time from left to right.    Strings can be indexed to obtain individual characters, e.g.  s[5]    Indices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the string).    Strings are immutable: you cannot modify a string once it has been\n    created.    Interactive Examples (Python 3):   type( hello ) class 'str'    hello  == 'hello'\nTrue  '''This string\n... is on\n... multiple\n... lines'''\n'This string\\\\nis on\\\\nmultiple\\\\nlines'   bonjour .upper()\n'BONJOUR'  len( bonjour )\n7   bonjour .startswith( b )\nTrue   cat,sat,flat .split( , )\n['cat', 'sat', 'flat']  # Print the first 5 Chinese unicode characters  print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04')\n\u4e00\u4e01\u4e02\u4e03\u4e04  x =  floyd   x[0]\n'f'   hello  +     +  world \n'hello world'  Example program:  # Prompt the user to input a string:\ninput = raw_input( Enter string:  )\n\n# Count the number of vowels in the input string\nvowels = 'aeiou'\ncount = 0\n\nfor char in input:\n    if char in vowels:\n        count += 1\n\n# Print the count to the standard output\nprint(count)  Example usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called  vowels.py :  python vowels.py\nEnter string: abracadabra\n5", 
            "title": "Strings"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#lists", 
            "text": "Represent mutable ordered sequences of values.    Type:  list    List literals are written in between square brackets, e.g.  [1, 2, 3]    List elements can be objects of any type (including other lists).    Like strings, lists can be indexed like so:  x[3]    Indices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the list).    Lists are mutable. You can update items, delete items and add new\n    items.    Indexing into a list is a constant time (amortised) operation.    Interactive Examples:   type([1, 2, 3]) class 'list'   x = []  len(x)\n0  x.append( hello )  x\n['hello']  len(x)\n1  x[0]\n'hello'  x.insert(0, True)  x\n[True, 'hello']  del x[1]  x\n[True]  x += [42,  Newton , 3.142]  x\n[True, 42, 'Newton', 3.142]", 
            "title": "Lists"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#dictionaries", 
            "text": "Represent finite mappings from keys to values.    Are implemented as  hash tables . The key objects must be hashable\n    (which rules out mutable objects, such as lists).    Type:  dict    Dictionary literals are written inside curly brackets, with\n    key-value pairs separated by colons: e.g.  {12: \"XII\", 6: \"VI\"}    Dictionaries can be indexed by keys. If the key exists in the\n    dictionary its corresponding value is returned, otherwise a\n     KeyError  exception is raised.    The cost of indexing a dictionary is proportional to the time taken\n    to hash the key. For many keys this can be considered constant\n    time. For variable sized objects, such as strings, this can be\n    considered to be proportional to the size of the object.    Iterating over a dictionary yields one key at a time. All keys in\n    the dictionary are visited exactly once. The order in which the\n    keys are visited is arbitrary.    You may test if an object is a key of a dictionary using the in\n    operator.    Interactive Examples:   type({12:  XII , 6:  VI }) class 'dict'   friends = {}  friends['Fred'] = ['Barney', 'Dino']  friends\n{'Fred': ['Barney', 'Dino']}  friends['Fred']\n['Barney', 'Dino']  friends['Barney']\nTraceback (most recent call last):\nFile  \\ stdin\\ , line 1, in \\ module\\ \nKeyError: 'Barney'  friends['Wilma'] = ['Betty']  friends\n{'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']}  friends.keys()\ndict_keys(['Fred', 'Wilma'])  friends.values()\ndict_values([['Barney', 'Dino'], ['Betty']])  'Dino' in friends\nFalse  Example program:  # Compute and print a histogram of a sequence of integers entered\n# on standard input, one number per line\n\nimport sys\n\nhistogram = {}\n\n# Iterate over each line in the standard input\nfor line in sys.stdin:\n    # Parse the next input as an integer\n    next_integer = int(line)\n    # Update the histogram accordingly\n    if next_integer in histogram:\n        # We've seen this integer before\n        histogram[next_integer] += 1\n    else:\n        # First occurrence of this integer in the input\n        histogram[next_integer] = 1\n\n# Print each key: value pair in the histogram in ascending\n# sorted order of keys\nfor key in sorted(histogram):\n    print( {} {} .format(key, histogram[key]))s  Example usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called  histo.py :  python histo.py  User types in a sequence of integers to the program, one per line, and\npresses control-d to terminate the input:  3\n43\n12\n19\n3\n12\n12\n43  Program prints its output:  3 2\n12 3\n19 1\n43 2", 
            "title": "Dictionaries"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#tuples", 
            "text": "Represent  immutable  ordered sequences of values.    Very much like lists except they cannot be modified once created.    Type:  tuple    Literals are written in between parentheses:  (1, 2, 3)    The can be used as keys in dictionaries (unlike lists).", 
            "title": "Tuples"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#loops", 
            "text": "", 
            "title": "Loops"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#while-loops", 
            "text": "Iterate until condition is  False    Syntax:    while expression:\n    statement_block   The value of the boolean expression is tested. If it evaluates to\n     True  then the statement block is executed once, before repeating\n    the loop. If it evaluates to False then the program continues\n    execution immediately after the loop.   Example:  def factorial(n):\n    result = 1\n    while n   0:\n        result *= n\n        n -= 1\n    return result", 
            "title": "While loops"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#for-loops", 
            "text": "Iterate over each item in a collection (e.g. list, string, tuple,\n    dictionary, file).    Syntax:    for variable in expression:\n    statement_block    Each item from the iterator expression is selected and assigned to\n    the variable, then the statement block is executed. The loop ends\n    when every item in the iterator has been visited.    The order of items visited in the iterator depends on the type of\n    the iterator. Lists, strings and tuples proceed in a left-to-right\n    fashion. Files proceed one line at a time. Dictionaries proceed in\n    an arbitrary order.    The  range()  function is useful for generating iterators of numbers\n    within a range. Note that the lower bound is inclusive and the\n    upper bound is exclusive.    Example:  def factorial(n):\n    result = 1\n    for item in range(n + 1):\n        result *= item\n    return result", 
            "title": "For loops"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#break-and-continue", 
            "text": "Both types of loops support the  break  and  continue  keywords.    break  terminates the loop immediately.    continue  jumps immediately back to the start of the loop.    They can sometimes simplify the conditional logic of a loop, but\n    should be used sparingly.", 
            "title": "Break and continue"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#functions", 
            "text": "Allow you to define reusable abstractions. Sometimes called\n     procedures .    Are generally defined at the top level of a module, and can also be\n    nested.    Type:  function    Named functions are bound to a variable name and may have complex\n    bodies.    Anonymous functions are used in-line, and may only have expression\n    bodies.    Named function syntax:    def variable(parameter_list):\n    statement_block   Anonymous function syntax:   lambda parameter_list: expression  Example:  def is_leap_year(year):\n    if year % 4 == 0 and year % 100 != 0:\n        return True\n    else:\n        return year % 400 == 0\n\nfor year in range(2000, 2100 + 1):\n    result = is_leap_year(year)\n    print( {} {} .format(year, result))  Anonymous function example:   squared = lambda x: x ** 2  squared(2)\n4  list(map(lambda x: x + 1, [1, 2, 3]))\n[2, 3, 4]", 
            "title": "Functions"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#input-and-output", 
            "text": "The  print  function is useful for displaying text (and other values\n    converted to text).    In Python 2  print  was a special keyword. In Python 3 it is a\n    function defined in the builtins.    Fancy string formatting can be done with the format method on\n    strings. Older Python code uses the string interpolation operator\n    for the same task  % , but its use is now discouraged.    Files must be opened before than can be manipulated. A file can be\n    opened in different modes: read  \"r\" , write  \"w\" , read-write  \"r+\" ,\n    and append  \"a\" . Opening a new file in write or append modes\n    creates a new file. Opening an existing file in write mode\n    overwrites its contents from the start. Opening an existing file\n    in append mode adds new content at the end of the old content.    When you are finished processing a file you should close it as soon\n    as possible. Closing a file releases limited operating system\n    resources, and ensures that any pending buffered writes a flushed\n    to the storage system.    Certain file types have libraries for convenient processing. One\n    example is the CSV (comma separated values) library for processing\n    tabular data. It is very handy for working with spreadsheets.    The command line arguments of a Python program are contained in a\n    list called  sys.argv  (it is a variable exported from the  sys \n    module). For complex program you should consider using a command\n    line argument parsing library such as  argparse .    Example program:  # Count the number of words and lines in a file\n\nimport sys\n\n# Get the input file name from the command line arguments\nfilename = sys.argv[1]\n\n# Open the file\nfile = open(filename)\n\n# Count the number of lines in the file\nnum_lines = 0\nnum_words = 0\n\nfor line in file:\n    num_lines += 1\n    num_words += len(line.split())\n\nfile.close()\n\nprint( Number of lines and words in {}: {} {}  \\\n        .format(filename, num_lines, num_words))", 
            "title": "Input and output"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#advanced-topics", 
            "text": "", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#classes", 
            "text": "Classes allow you to define your own types.    Class definitions may define methods for the type.    A class may inherit, and possibly override, some functionality from\n    a superclass.    Syntax:    class variable(superclass_list):\n    body    The name of the class is given by the variable in the definition.\n    The superclass list defines the superclasses of the new class\n    (very often the base type object is used). The body of the class\n    typically defines one or more methods.    Instances of classes are created by calling the class name as if it\n    were a function.    If defined, the special method called  __init__  is used to\n    initialise a newly created instance of a class.    The first parameter to each method is the object upon which the\n    method was called. The convention is to use the variable called\n    self, however any variable name will do. Many object oriented\n    languages make this variable an implicit parameter called this.    Example:  class Vector(object):\n    def __init__(self, x=0, y=0, z=0):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def magnitude(self):\n        return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2)\n\n    def normalise(self):\n        magnitude = self.magnitude()\n        if magnitude == 0:\n            # Somehow we have a degenerate vector.\n            return self\n        else:\n            return self / self.magnitude()\n\n    def angle(self, other):\n        dp = self.dot_product(other)\n        return acos(dp / self.magnitude() * other.magnitude())\n\n    def dot_product(self, other):\n        return self.x * other.x + self.y * other.y + self.z * other.z", 
            "title": "Classes"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#exceptions", 
            "text": "Exceptions allow Python programs to handle erroneous program\n    conditions.    An exception is raised (or thrown) at the point of the error and\n    handled (or caught) at some other place in the program.    Exception handlers have the syntax:    try:\n    statement_block\nexcept exception_type as variable:\n    statement_block\n...    The statement block after  try  is executed. If no exceptions are\n    raised in that block the program continues immediately after the\n    exception handler. If an exception is raised in the block then\n    program control jumps to the innermost closing  except  clause.\n    Except clauses may optionally specify the set of exception types\n    that they can handle. If the raised exception is an instance of\n    the handled type then the body of the except clause is executed,\n    otherwise the next except clause (if any) is tried.    If no matching exception handler is found then the program will\n    terminate with an unhandled exception error. Python will normally\n    print a stack trace at this point for error diagnosis.    You may raise your own exceptions using the  raise  keyword.    Example:  # alternative version of the histogram code from the section on\n# dictionaries\nfor line in sys.stdin:\n    next_integer = int(line)\n    try:\n        histogram[next_integer] += 1\n    except KeyError:\n        histogram[next_integer] = 1", 
            "title": "Exceptions"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#modules", 
            "text": "A module is a file which contains Python code.    Any Python file you create is automatically a module.    It is considered good programming style to decompose complex\n    programs into multiple modules. Each module should collect\n    together code with similar purpose.    Variables defined at the top level of a module (such as global\n    variables, functions and classes) can be imported into other\n    modules.    Python comes with many standard modules.    The  import  keyword is used to import an entire module.    You may import a subset of things from a module using the  from ... import ...  syntax.    You may import a module with a new name using the  from ... import ... as ...  or  import ... as ...    When a module is first imported in a program, all of its top-level\n    statements are executed from top to bottom. Subsequent imports use\n    a cached version of its definitions, its statements are not\n    re-executed.    A special module called  builtins  is imported into every other module\n    by default, and it is automatically imported at the interactive\n    prompt in the interpreter.    Interactive Example:   import math  math.sqrt(100)\n10.0  sqrt(100)\nTraceback (most recent call last):\nFile  stdin , line 1, in  module \nNameError: name 'sqrt' is not defined  from math import sqrt  sqrt(100)\n10.0  import math as m  m.sqrt(100)\n10.0", 
            "title": "Modules"
        }, 
        {
            "location": "/tutorials/python_overview/python_overview/#packages", 
            "text": "A package is a collection of modules in a hierarchy.    Packages are the common way to structure Python libraries.    The  Python Package Index (PyPI)  is a big collection of open\n    source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of\n    writing).    Package installation tools such as  pip , make it easy to install\n    packages onto your computer.    If you want to make your own Python code easy for others to install\n    and use then you should consider making it a package. You can even\n    upload it to PyPI.    Many people use  virtualenv  to install packages into a local\n    \"sandboxed\" Python environment. This avoids conflicts with the\n    central Python package database on your computer, and allows\n    multiple different versions of packages to be used.", 
            "title": "Packages"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/", 
            "text": "Using Git and Github for revision control\n\n\nWhat is Git?\n\n\nGit is a revision control system. It is designed to help you keep track\nof collections of files which stem from a common source and undergo\nmodifications over time. The files tend to be human generated text. It\nis very good at managing source code repositories, but it can also be\nused to manage other things, such as configuration files and text\ndocuments. It is not, however, a file backup system.\n\n\nGit encourages a distributed style of project development. Each\ncontributor to a project has their own complete repository. Changes are\nshared between repositories by \npushing\n changes to, or \npulling\n\nchanges from other repositories. Collaboration between developers is\ngreatly enhanced by websites such as \ngithub\n,\n\nbitbucket\n\nand \ngitorious\n which\nprovide convenient interfaces to managing multiple repositories.\n\n\nThere are many alternatives to git which each have their pros and cons.\nTwo of the more popular alternatives are:\n\n\n\n\n\n\nSubversion\n is particularly suited to a centralised model of development.\n\n\n\n\n\n\nMercurial\n is very similar to Git, but is sometimes considered more user friendly.\n\n\n\n\n\n\nGetting help\n\n\nThere are lots of resources on the web for learning how to use Git. A\npopular reference is \nPro Git\n, which is freely available online\n(\nhttp://git-scm.com/book\n). Another good\nreference is the book \nVersion Control with Git\n, by Loeliger and\nMcCullough.\n\n\nA simple workflow\n\n\nStep 1, create a github account.\n\n\nCreate a github account (\nhttps://github.com/\n).\nDo this step once only (unless you need multiple accounts).\n\n\nYou get unlimited numbers of (world readable) public repositories for\nfree.\n\n\nPrivate repositories (that can be shared with selected users) cost money\n(see \nhttps://github.com/plans\n), but \ndiscounts are available for academics\n.\n\n\nStep 2, sign into github and create a repository.\n\n\nSign in to your github account and create a new repository. Do this once\nfor every new project you have.\n\n\n\n\nYou will need to provide some information:\n\n\n\n\n\n\nthe repository name\n\n\n\n\n\n\na description of the repository\n\n\n\n\n\n\nchoose whether it is public (free) or private (costs money)\n\n\n\n\n\n\nwhether to initialise with a dummy README file (it is useful)\n\n\n\n\n\n\nwhether to provide an initial .gitignore file (probably leave this\n    \n out in the beginning)\n\n\n\n\n\n\n\n\nStep 3, clone your repository to your local computer.\n\n\nClone your new repository from github to your local computer.\n\n\nEach repository on github is identified by a URL, which will look like\nthe one below:\n\n\n\n\nRun the command below on your development machine in the directory where\nyou want to keep the repository (of course you should use the actual URL\nof your own repository, not the one in the example).\n\n\n$ git clone https://github.com/bjpop/test.git\nCloning into 'test'...\nremote: Counting objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\n\n\n\n\nThis will create a directory with the same name as your repository (in this example it is called \ntest\n).\n\n\nIf you change into that directory and list its contents you will see a\n\n.git\n subdirectory, which is where Git keeps all the data for your\nrepository. You will also see working copies of the files in the\nproject. In this example the only such file is \nREADME.md\n which was\ncreated automatically by github when the repository was first created.\n(The .md extension on the file suggests that it uses the \nMarkdown\n\nsyntax, see \nhttps://help.github.com/articles/github-flavored-markdown\n).\n\n\n$ cd test\n$ ls -a\n. .. .git README.md\n$ ls .git\nbranches config description HEAD hooks index info logs objects packed-refs refs\n\n\n\n\nStep 4, commit a file to the repository.\n\n\nCreate a new file in the repository on your local computer and commit it\nto your local repository.\n\n\nHow you create the file is immaterial. You could copy it from somewhere\nelse, create it in a text editor. In this case we\u2019ll make a little\npython program:\n\n\n$ echo 'print(\nhello world\n)' \n hello.py\n\n\n\n\nTest that your new file is satisfactory, in this case we test our code:\n\n\n$ python hello.py\nhello world\n\n\n\n\nCheck the status of your repository:\n\n\n$ git status\n# On branch master\n# Untracked files:\n# (use \ngit add \nfile\n...\n to include in what will be committed)\n#\n# hello.py\nnothing added to commit but untracked files present (use \ngit add\n to track)\n\n\n\n\nNotice that git tells you that the new file \nhello.py\n is not tracked\n(not in the repository).\n\n\nWhen you are happy with your file, you can stage it (this is not a\ncommit), but it will cause the file to be tracked:\n\n\n$ git add hello.py\n\n\n\n\nNote that git uses a two-stage process for committing changes. The first\nstage is to \"stage\" your changes. Staged changes appear in the\nrepository index, but are not committed. You can stage many changes\ntogether, and even amend or undo previously staged (but not committed)\nchanges. The second stage is to commit the current staged changes to the\nrepository. Committing causes the changes to be reflected in the state\nof the repository.\n\n\nRe-check the status of your repository:\n\n\n$ git status\n# On branch master\n# Changes to be committed:\n# (use \ngit reset HEAD \nfile\n...\n to unstage)\n#\n# new file: hello.py\n#\n\n\n\n\nNow we can see that the changes to \nhello.py\n have been staged and are\nready to be committed. Notice that \nhello.py\n is no longer untracked.\n\n\nCommit your changes with a commit message:\n\n\n$ git commit -m \nA little greeting program\n\n[master b1cce11] A little greeting program\n1 files changed, 1 insertions(+), 0 deletions(-)\ncreate mode 100644 hello.py\n\n\n\n\nRe-check the status of your repository:\n\n\n$ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)\n\n\n\n\nNow we see that there a no uncommitted changes in the repository,\nhowever git tells us that our local repository is one commit ahead of\nthe github version (which it calls \norigin/master\n).\n\n\nStep 5, push your changes to github.\n\n\nPush the commit in your local repository to github (thus synchronising them).\n\n\n$ git push origin\nUsername for 'https://github.com': \ntype your github username\n\nPassword for 'https://\nyour github username\n@github.com': \nCounting objects: 4, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 305 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n71a771a..b1cce11 master -\n master\n\n\n\n\nNow if you look at your repository on github you should see the file\n\nhello.py\n has been uploaded, along with its commit time and commit\nmessage.\n\n\n\n\nYou can inspect the contents of the file on github by clicking on its\nname:\n\n\n\n\nStep 6, create a branch in your local repository.\n\n\nYou can ask git to tell you about the names of the current branches:\n\n\n$ git branch\n* master\n\n\n\n\nBy default your repository starts with a branch called master. The\nasterisk next to the branch name tells you which is the current branch\n(at the moment there is only one branch).\n\n\n$ git branch documentation\n$ git branch\ndocumentation\n* master\n\n\n\n\nThe first command above creates a new branch called \ndocumentation\n. The\nsecond command shows us that the new branch has been created, but the\ncurrent branch is still \nmaster\n.\n\n\nTo switch to another branch you must check it out:\n\n\n$ git checkout documentation\nSwitched to branch 'documentation'\n$ git branch\n* documentation\nmaster\n\n\n\n\nLet\u2019s add a change to our existing \nhello.py\n file:\n\n\n$ echo '#this is a comment' \n hello.py\n\n\n\n\nCheck the status of the repository (now in the documentation branch):\n\n\n$ git status\n# On branch documentation\n# Changes not staged for commit:\n# (use \ngit add \nfile\n...\n to update what will be committed)\n# (use \ngit checkout -- \nfile\n...\n to discard changes in working\n directory)\n#\n# modified: hello.py\n#\nno changes added to commit (use \ngit add\n and/or \ngit commit -a\n)\n\n\n\n\nStage the new changes and commit them, and check the status again:\n\n\n$ git add hello.py \n$ git commit -m \nAdded a comment\n\n[documentation 9bbe430] Added a comment\n1 files changed, 1 insertions(+), 0 deletions(-)\n$ git status\n# On branch documentation\nnothing to commit (working directory clean)\n\n\n\n\nNow we can push the new \u201cdocumentation\u201d branch to github:\n\n\n$ git push origin documentation\nUsername for 'https://github.com': \nyour github username\n\nPassword for 'https://\nyour github username\n@github.com': \nCounting objects: 5, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 314 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n* [new branch] documentation -\n documentation\n\n\n\n\nOn github you should be able to see the new branch:\n\n\n\n\nStep 7, merge the changes back into the master\n\n\nbranch.\n\n\nTo go back to the master branch you must check it out:\n\n\n$ git checkout master\nSwitched to branch 'master'\n\n\n\n\nYou can confirm that the master branch does not yet have the changes\nmade in the documentation branch:\n\n\n$ cat hello.py \nprint(\nhello world\n)\n\n\n\n\nNotice that the comment is missing.\n\n\nYou can pull the changes in the documentation branch back into the\nmaster branch with the merge command:\n\n\n$ git merge documentation\nUpdating b1cce11..9bbe430\nFast-forward\nhello.py | 1 +\n1 files changed, 1 insertions(+), 0 deletions(-)\n\n\n\n\nIn this case the merge was easy because there were no conflicts between\nmaster and documentation. In this case git automatically updates the\ntracked files in the current branch.\n\n\nWe can test that the changes have taken place by looking at the contents\nof hello.py:\n\n\n$ cat hello.py \nprint(\nhello world\n)\n#this is a comment\n\n\n\n\nCheck the status of the master branch:\n\n\n$ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)\n\n\n\n\nPush the changes in the master branch back to github:\n\n\n$ git push origin master\nUsername for 'https://github.com': bjpop\nPassword for 'https://bjpop@github.com': \nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\nb1cce11..9bbe430 master -\n master\n\n\n\n\nAgain you can verify on github that the changes have taken place.\n\n\nTo get an idea of the history of a project you can ask for a log of the\ncommit messages:\n\n\n$ git log\ncommit 9bbe430f6e8b70187927b4a70a8402f71b17b426\nAuthor: Bernie \nflorbitous@gmail.com\n\nDate: Fri Mar 15 12:30:39 2013 +1100\nAdded a comment\ncommit b1cce115fb40a9b11917db7eb73c8295e276bb09\nAuthor: Bernie \nflorbitous@gmail.com\n\nDate: Fri Mar 15 12:08:01 2013 +1100\nA little greeting program\ncommit 71a771a86b8116c3f93c99db5416bfa371a6f772\nAuthor: Bernie Pope \nflorbitous@gmail.com\n\nDate: Thu Mar 14 17:29:02 2013 -0700", 
            "title": "Using Git and Github for revision control"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#using-git-and-github-for-revision-control", 
            "text": "", 
            "title": "Using Git and Github for revision control"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#what-is-git", 
            "text": "Git is a revision control system. It is designed to help you keep track\nof collections of files which stem from a common source and undergo\nmodifications over time. The files tend to be human generated text. It\nis very good at managing source code repositories, but it can also be\nused to manage other things, such as configuration files and text\ndocuments. It is not, however, a file backup system.  Git encourages a distributed style of project development. Each\ncontributor to a project has their own complete repository. Changes are\nshared between repositories by  pushing  changes to, or  pulling \nchanges from other repositories. Collaboration between developers is\ngreatly enhanced by websites such as  github , bitbucket \nand  gitorious  which\nprovide convenient interfaces to managing multiple repositories.  There are many alternatives to git which each have their pros and cons.\nTwo of the more popular alternatives are:    Subversion  is particularly suited to a centralised model of development.    Mercurial  is very similar to Git, but is sometimes considered more user friendly.", 
            "title": "What is Git?"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#getting-help", 
            "text": "There are lots of resources on the web for learning how to use Git. A\npopular reference is  Pro Git , which is freely available online\n( http://git-scm.com/book ). Another good\nreference is the book  Version Control with Git , by Loeliger and\nMcCullough.", 
            "title": "Getting help"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#a-simple-workflow", 
            "text": "", 
            "title": "A simple workflow"
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-1-create-a-github-account", 
            "text": "Create a github account ( https://github.com/ ).\nDo this step once only (unless you need multiple accounts).  You get unlimited numbers of (world readable) public repositories for\nfree.  Private repositories (that can be shared with selected users) cost money\n(see  https://github.com/plans ), but  discounts are available for academics .", 
            "title": "Step 1, create a github account."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-2-sign-into-github-and-create-a-repository", 
            "text": "Sign in to your github account and create a new repository. Do this once\nfor every new project you have.   You will need to provide some information:    the repository name    a description of the repository    choose whether it is public (free) or private (costs money)    whether to initialise with a dummy README file (it is useful)    whether to provide an initial .gitignore file (probably leave this\n      out in the beginning)", 
            "title": "Step 2, sign into github and create a repository."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-3-clone-your-repository-to-your-local-computer", 
            "text": "Clone your new repository from github to your local computer.  Each repository on github is identified by a URL, which will look like\nthe one below:   Run the command below on your development machine in the directory where\nyou want to keep the repository (of course you should use the actual URL\nof your own repository, not the one in the example).  $ git clone https://github.com/bjpop/test.git\nCloning into 'test'...\nremote: Counting objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.  This will create a directory with the same name as your repository (in this example it is called  test ).  If you change into that directory and list its contents you will see a .git  subdirectory, which is where Git keeps all the data for your\nrepository. You will also see working copies of the files in the\nproject. In this example the only such file is  README.md  which was\ncreated automatically by github when the repository was first created.\n(The .md extension on the file suggests that it uses the  Markdown \nsyntax, see  https://help.github.com/articles/github-flavored-markdown ).  $ cd test\n$ ls -a\n. .. .git README.md\n$ ls .git\nbranches config description HEAD hooks index info logs objects packed-refs refs", 
            "title": "Step 3, clone your repository to your local computer."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-4-commit-a-file-to-the-repository", 
            "text": "Create a new file in the repository on your local computer and commit it\nto your local repository.  How you create the file is immaterial. You could copy it from somewhere\nelse, create it in a text editor. In this case we\u2019ll make a little\npython program:  $ echo 'print( hello world )'   hello.py  Test that your new file is satisfactory, in this case we test our code:  $ python hello.py\nhello world  Check the status of your repository:  $ git status\n# On branch master\n# Untracked files:\n# (use  git add  file ...  to include in what will be committed)\n#\n# hello.py\nnothing added to commit but untracked files present (use  git add  to track)  Notice that git tells you that the new file  hello.py  is not tracked\n(not in the repository).  When you are happy with your file, you can stage it (this is not a\ncommit), but it will cause the file to be tracked:  $ git add hello.py  Note that git uses a two-stage process for committing changes. The first\nstage is to \"stage\" your changes. Staged changes appear in the\nrepository index, but are not committed. You can stage many changes\ntogether, and even amend or undo previously staged (but not committed)\nchanges. The second stage is to commit the current staged changes to the\nrepository. Committing causes the changes to be reflected in the state\nof the repository.  Re-check the status of your repository:  $ git status\n# On branch master\n# Changes to be committed:\n# (use  git reset HEAD  file ...  to unstage)\n#\n# new file: hello.py\n#  Now we can see that the changes to  hello.py  have been staged and are\nready to be committed. Notice that  hello.py  is no longer untracked.  Commit your changes with a commit message:  $ git commit -m  A little greeting program \n[master b1cce11] A little greeting program\n1 files changed, 1 insertions(+), 0 deletions(-)\ncreate mode 100644 hello.py  Re-check the status of your repository:  $ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)  Now we see that there a no uncommitted changes in the repository,\nhowever git tells us that our local repository is one commit ahead of\nthe github version (which it calls  origin/master ).", 
            "title": "Step 4, commit a file to the repository."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-5-push-your-changes-to-github", 
            "text": "Push the commit in your local repository to github (thus synchronising them).  $ git push origin\nUsername for 'https://github.com':  type your github username \nPassword for 'https:// your github username @github.com': \nCounting objects: 4, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 305 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n71a771a..b1cce11 master -  master  Now if you look at your repository on github you should see the file hello.py  has been uploaded, along with its commit time and commit\nmessage.   You can inspect the contents of the file on github by clicking on its\nname:", 
            "title": "Step 5, push your changes to github."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-6-create-a-branch-in-your-local-repository", 
            "text": "You can ask git to tell you about the names of the current branches:  $ git branch\n* master  By default your repository starts with a branch called master. The\nasterisk next to the branch name tells you which is the current branch\n(at the moment there is only one branch).  $ git branch documentation\n$ git branch\ndocumentation\n* master  The first command above creates a new branch called  documentation . The\nsecond command shows us that the new branch has been created, but the\ncurrent branch is still  master .  To switch to another branch you must check it out:  $ git checkout documentation\nSwitched to branch 'documentation'\n$ git branch\n* documentation\nmaster  Let\u2019s add a change to our existing  hello.py  file:  $ echo '#this is a comment'   hello.py  Check the status of the repository (now in the documentation branch):  $ git status\n# On branch documentation\n# Changes not staged for commit:\n# (use  git add  file ...  to update what will be committed)\n# (use  git checkout --  file ...  to discard changes in working\n directory)\n#\n# modified: hello.py\n#\nno changes added to commit (use  git add  and/or  git commit -a )  Stage the new changes and commit them, and check the status again:  $ git add hello.py \n$ git commit -m  Added a comment \n[documentation 9bbe430] Added a comment\n1 files changed, 1 insertions(+), 0 deletions(-)\n$ git status\n# On branch documentation\nnothing to commit (working directory clean)  Now we can push the new \u201cdocumentation\u201d branch to github:  $ git push origin documentation\nUsername for 'https://github.com':  your github username \nPassword for 'https:// your github username @github.com': \nCounting objects: 5, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 314 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n* [new branch] documentation -  documentation  On github you should be able to see the new branch:", 
            "title": "Step 6, create a branch in your local repository."
        }, 
        {
            "location": "/tutorials/using_git/Using_Git/#step-7-merge-the-changes-back-into-the-master", 
            "text": "branch.  To go back to the master branch you must check it out:  $ git checkout master\nSwitched to branch 'master'  You can confirm that the master branch does not yet have the changes\nmade in the documentation branch:  $ cat hello.py \nprint( hello world )  Notice that the comment is missing.  You can pull the changes in the documentation branch back into the\nmaster branch with the merge command:  $ git merge documentation\nUpdating b1cce11..9bbe430\nFast-forward\nhello.py | 1 +\n1 files changed, 1 insertions(+), 0 deletions(-)  In this case the merge was easy because there were no conflicts between\nmaster and documentation. In this case git automatically updates the\ntracked files in the current branch.  We can test that the changes have taken place by looking at the contents\nof hello.py:  $ cat hello.py \nprint( hello world )\n#this is a comment  Check the status of the master branch:  $ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)  Push the changes in the master branch back to github:  $ git push origin master\nUsername for 'https://github.com': bjpop\nPassword for 'https://bjpop@github.com': \nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\nb1cce11..9bbe430 master -  master  Again you can verify on github that the changes have taken place.  To get an idea of the history of a project you can ask for a log of the\ncommit messages:  $ git log\ncommit 9bbe430f6e8b70187927b4a70a8402f71b17b426\nAuthor: Bernie  florbitous@gmail.com \nDate: Fri Mar 15 12:30:39 2013 +1100\nAdded a comment\ncommit b1cce115fb40a9b11917db7eb73c8295e276bb09\nAuthor: Bernie  florbitous@gmail.com \nDate: Fri Mar 15 12:08:01 2013 +1100\nA little greeting program\ncommit 71a771a86b8116c3f93c99db5416bfa371a6f772\nAuthor: Bernie Pope  florbitous@gmail.com \nDate: Thu Mar 14 17:29:02 2013 -0700", 
            "title": "Step 7, merge the changes back into the master"
        }, 
        {
            "location": "/tutorials/hpc/", 
            "text": "em {font-style: normal; font-family: courier new;}\n\n\n\nHigh-Performance Computing\n\n\nA hands-on-workshop covering High-Performance Computing (HPC)\n\n\nHow to use this workshop\n\n\nThe workshop is broken up into a number of \nTopics\n each focusing on a particular aspect of HPCs.  You should take a short break between \neach to refresh and relax before tackling the next.\n\n\nTopic\ns may start with some background followed by a number of \nexercises\n.  Each \nexercise\n begins with a \nquestion\n, then \nsometimes a \nhint\n (or two) and finishes with the suggested \nanswer\n.\n\n\nQuestion\n\n\nAn example question looks like:\n\n\n\n\n\n\nWhat is the Answer to Life?\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\nHint\n\n\nDepending on how much of a challenge you like, you may choose to use hints.  Even if you work out the answer without hints, its a good \nidea to read the hints afterwards because they contain extra information that is good to know.\n\n\nNote: \nhint\ns may be staged, that is, there may be a \nmore\n section within a hint for further hints\n\n\n\n\n\n\nHint\n \n- click here to reveal hint\n\n\n\n\nWhat is the answer to everything?\n\n\nAs featured in \n\"The Hitchhiker's Guide to the Galaxy\"\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nIt is probably a two digit number\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable2\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink2\").text(\"More\");\n            } else {\n                $(\"#showablelink2\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nAnswer\n\n\nOnce you have worked out the answer to the question expand the Answer section to check if you got it correct.\n\n\n\n\n\n\nAnswer\n \n- click here to reveal answer\n\n\n\n\nAnswer\n: 42\n\n\nRef: \nNumber 42 (Wikipedia)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nUsage Style\n\n\nThis workshop attempts to cater for two usage styles:\n\n\n\n\nProblem solver\n: for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional):\n\n\nAttempt to answer the question by yourself.\n\n\nUse hints when you get stuck.\n\n\nOnce solved, reveal the answer and read through our suggested solution.\n\n\nIts a good idea to read the hints and answer description as they often contain extra useful information.\n\n\n\n\n\n\nBy example\n:  for those who learn by following examples:  \nExpand\n all sections\n\n\nExpand the Answer section at the start of each question and follow along with the commands that are shown and check you get the\n  same (or similar) answers.\n\n\nIts a good idea to read the hints and answer description as they often contain extra useful information.\n\n\n\n\n\n\n\n\nConnecting to HPC\n\n\nTo begin this workshop you will need to connect to an HPC.  Today we will use the LIMS-HPC.  The computer called \nlims-hpc-m\n (m is for \nmaster which is another name for head node) is the one that coordinates all the HPCs tasks.\n\n\nServer details\n:\n\n\n\n\nhost\n: lims-hpc-m.latrobe.edu.au\n\n\nport\n: 6022 \n\n\nusername\n: trainingXX (where XX is a two digit number, provided at workshop)\n\n\npassword\n: PROVIDED at workshop \n\n\n\n\nConnection instructions\n:\n\n\n\n\n\n\nMac OS X / Linux\n\n\n\n\nBoth Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line.  To use OpenSSH you must \nfirst start a terminal program on your computer.  On OS X the standard terminal is called Terminal, and it is installed by default. \nOn Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good \ndefault).  When you've started the terminal you should see a command prompt.  To log into LIMS-HPC, for example, type this command at \nthe prompt and press return (where the word username is replaced with your LIMS-HPC username):\n\n\nssh -p 6022 username@lims-hpc-m.latrobe.edu.au\n\n\n\n\nThe same procedure works for any other machine where you have an account except most other HPCs will not need the \n-p 6022\n \n(which is telling ssh to connect on a non-standard port number).\n\n\nYou may be presented with a message along the lines of:\n\n\nThe authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be  established.\n...\nAre you sure you want to continue connecting (yes/no)?\n\n\n\n\nAlthough you should never ignore a warning, this particular one is nothing to be concerned about; type \nyes\n and then \npress enter\n. \nIf all goes well you will be asked to enter your password.  Assuming you type the correct username and password the system should \nthen display a welcome message, and then present you with a Unix prompt.  If you get this far then you are ready to start entering \nUnix commands and thus begin using the remote computer.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nWindows\n\n\n\n\nOn Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client.  PuTTY (putty.exe) can be downloaded \nfrom this web page:\n\n\nhttp://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\n\n\nDocumentation for using PuTTY is here:\n\n\nhttp://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html\n\n\nWhen you start PuTTY you should see a window which looks something like this:\n\n\n\n\nTo connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and \n6022\n in the port, \nthen click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the \nsame as they are in the picture above).\n\n\nIn some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines \nof \n\"The server's host key is not cached in the registry\"\n. This is nothing to worry about, and you should agree to continue (by \nclicking on Yes). You usually see this message the first time you try to connect to a particular remote computer.\n\n\nIf all goes well, a terminal window will open, showing a prompt with the text \n\"login as:\"\n. An example terminal window is shown \nbelow. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your \npassword. Assuming you type the correct username and password the system should then display a welcome message, and then \npresent you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using \nthe remote computer.\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 1: Exploring an HPC\n\n\nAn HPC (short for \u2018High Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems.\n\n\n\n\nFigure\n: Overview of the computers involved when using an HPC.  Computer systems are shown in rectangles and arrows represent interactions.\n\n\nExercises\n\n\n\n\n\n\n1.1) What is the contact email for your HPC's System Administrator?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWhen you login, you will be presented with a message; this is called the \nMessage Of The Day\n and usually includes lots of useful \ninformation.  On LIMS-HPC this includes a list of useful commands, the last login details for your account and the contact email\nof the system administrator\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink7\").click(function(e){\n            e.preventDefault();\n            $(\"#showable7\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLIMS-HPC: andrew.robinson@latrobe.edu.au\n\n\nSNOWY \n BARCOO: help@melbournebioinformatics.org.au\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n1.2) Run the \nsinfo\n command.  How many nodes are there in this hpc?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe \nlims-hpc-[2-4]\n is shorthand for \nlims-hpc-2 lims-hpc-3 and lims-hpc-4\n and \nlims-hpc-[1,5]\n is shorthand for\n\nlims-hpc-1 and lims-hpc-5\n\n\n\n\n\n\nmore\n\n\n\n\nHave a look at the NODELIST column.  Only count each node once.\n\n\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncompute*     up 200-00:00:      3    mix lims-hpc-[2-4]\ncompute*     up 200-00:00:      2   idle lims-hpc-[1,5]\nbigmem       up 200-00:00:      1   idle lims-hpc-1\n8hour        up   08:00:00      3    mix lims-hpc-[2-4]\n8hour        up   08:00:00      3   idle lims-hpc-[1,5],lims-hpc-m\n\n\n\n\nNOTE: the above list will vary depending on the HPC setup.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable11\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink11\").text(\"more\");\n            } else {\n                $(\"#showablelink11\").text(\"less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink10\").click(function(e){\n            e.preventDefault();\n            $(\"#showable10\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe \nsinfo\n command lists all available partitions and the status of each node within them.  If you count up the names of nodes \n(uniquely) you will get the total nodes in this cluster.  \n\n\n\n\nLIMS-HPC: \n6\n (\nlims-hpc-m\n and \nlims-hpc-1\n through \nlims-hpc-5\n)\n\n\nMERRI: \n84\n (\nturpin\n and \nmerri001\n through \nmerri083\n)\n\n\nBARCOO: \n70\n (\nbarcoo001\n through \nbarcoo070\n)\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAlternate Method\n\n\n\n\nAn automatic (though more complex) way would have been running the following command:\n\n\n$ scontrol show node | grep NodeName | wc -l\n\n\n\n\nWhere:\n\n\n\n\nscontrol show node\n: lists details of all nodes (over multiple lines)\n\n\ngrep NodeName\n: only shows the NodeName line\n\n\nwc -l\n: counts the number of lines\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 2: Software Modules\n\n\nUp to this point we have been using only standard Unix software packages which are included with Linux/Unix computers.\nLarge computing systems such as HPCs often use a system of modules to load specific software packages (and versions)\nwhen needed for the user.\n\n\nIn this topic we will discover what science software modules (tools) are available and load them ready for analysis.\n\n\nThis topic uses the \nman\n and \nmodule\n commands heavily\n\n\nExercises\n\n\n\n\n\n\n2.1) What happens if you run the \nmodule\n command without any options / arguments?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nLiterally type \nmodule\n and press \nENTER\n key.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAnswer\n: It prints an error followed by a list of available options / flags\n\n\n$ module\ncmdModule.c(166):ERROR:11: Usage is 'module command  [arguments ...] '\n\n  Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991):\n\n  Usage: module [ switches ] [ subcommand ] [subcommand-args ]\n\nSwitches:\n    -H|--help       this usage info\n    -V|--version        modules version \n configuration options\n    -f|--force      force active dependency resolution\n    -t|--terse      terse    format avail and list format\n    -l|--long       long     format avail and list format\n    -h|--human      readable format avail and list format\n    -v|--verbose        enable  verbose messages\n    -s|--silent     disable verbose messages\n    -c|--create     create caches for avail and apropos\n    -i|--icase      case insensitive\n    -u|--userlvl \nlvl\n  set user level to (nov[ice],exp[ert],adv[anced])\n  Available SubCommands and Args:\n    + add|load      modulefile [modulefile ...]\n    + rm|unload     modulefile [modulefile ...]\n    + switch|swap       [modulefile1] modulefile2\n    + display|show      modulefile [modulefile ...]\n    + avail         [modulefile [modulefile ...]]\n    + use [-a|--append] dir [dir ...]\n    + unuse         dir [dir ...]\n    + update\n    + refresh\n    + purge\n    + list\n    + clear\n    + help          [modulefile [modulefile ...]]\n    + whatis        [modulefile [modulefile ...]]\n    + apropos|keyword   string\n    + initadd       modulefile [modulefile ...]\n    + initprepend       modulefile [modulefile ...]\n    + initrm        modulefile [modulefile ...]\n    + initswitch        modulefile1 modulefile2\n    + initlist\n    + initclear\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.2) How do you find a list of \navailable\n software?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTry the \nmodule\n command.  Don't forget the \nman\n command to get help for a command\n\n\n\n\n\n\nMore\n\n\n\n\nRun the command \nman module\n\n\nUse a search to find out about the \navail\n subcommand (e.g. /avail\nenter\n)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink19\").click(function(e){\n            e.preventDefault();\n            $(\"#showable19\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable19\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink19\").text(\"More\");\n            } else {\n                $(\"#showablelink19\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe module command is used to show details of software modules (tools).\n\n\nAnswer\n:\n\n\n$ module avail\n\n------------------- /usr/share/Modules/modulefiles --------------------\ndot         module-git  module-info modules     null        use.own\n\n------------------- /usr/local/Modules/modulefiles --------------------\nacana/1.60                         mafft-gcc/7.215\naftrrad/4.1.20150201               malt/0.1.0\narlequin/3.5.1.3                   matplotlib-gcc/1.3.1\n...\n\n\n\n\nThe modules list has been shortened because it is very long.  The modules after the \n/usr/local/Modules/modulefiles\n line\nare the science software; before this are a few built-in ones that you can ignore.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.3) How many modules are there starting with \u2018\nf\n\u2019?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRun the command \nman module\n\n\nUse a search to find out about the \navail\n subcommand (e.g. /avail\nenter\n).  You may have to press 'n' a few times\nto reach the section where the it describes the \navail\n subcommand.\n\n\n\n\n\n\nMore\n\n\n\n\n\n\nIf an argument is given, then each directory in the MODULEPATH is searched for modulefiles\nwhose pathname match the argument\n\n\n\n\nThis is a quote from the manual page for the module command explaining the avail subcommand.  It uses rather technical \nlanguage but basically it's saying you can put search terms after the avail subcommand when entering the command.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink23\").click(function(e){\n            e.preventDefault();\n            $(\"#showable23\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable23\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink23\").text(\"More\");\n            } else {\n                $(\"#showablelink23\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe man page told us that we could put a search term after \nmodule avail\n.\n\n\n$ module avail f\n------------------- /usr/local/Modules/modulefiles -------------------\nfasta-gcc/35.4.12            flex-gcc/2.5.39\nfastqc/0.10.1                fontconfig-gcc/2.11.93\nfastStructure-gcc/2013.11.07 freebayes-gcc/20140603\nfastx_toolkit-gcc/0.0.14     freetype-gcc/2.5.3\n\n\n\n\nAnswer\n: 8 modules\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAlternate Method\n\n\n\n\nTo get a fully automated solution your could do the following command:\n\n\n$ module -l avail 2\n1 | grep \n^f\n | wc -l\n\n\n\n\nWhere:\n\n\n\n\nmodule -l avail\n: lists all modules (in long format, i.e. one per line)\n\n\n2\n1\n: merges output from \nstandard error\n to the \nstandard output\n so it can be feed into grep.  For some reason the\ndevelopers of the \nmodule\n command thought it was a good idea to output the module names on the \nerror\n stream rather than\nthe logical \noutput\n stream.\n\n\ngrep \"^f\"\n: only shows lines beginning with \nf\n\n\nwc -l\n: counts the number of lines\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.4) Run the \npear\n command (without loading it), does it work?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThis question is very literal\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink27\").click(function(e){\n            e.preventDefault();\n            $(\"#showable27\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ pear\n-bash: pear: command not found\n\n\n\n\nThe error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear'\n\n\nAnswer\n: No, command not found\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.5) How would we \nload\n the \npear\n module?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheck the man page for \nmodule\n again and look for a subcommand that might load modules; it is quite literal as well.\n\n\n\n\n\n\nMore\n\n\n\n\nRun the command \nman module\n\n\nUse a search to find out about the \nload\n subcommand (e.g. /load\nenter\n)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable31\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink31\").text(\"More\");\n            } else {\n                $(\"#showablelink31\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink30\").click(function(e){\n            e.preventDefault();\n            $(\"#showable30\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ module load pear-gcc/0.9.4\n\n\n\n\n-gcc | -intel\n: Lots of modules will have either \n-gcc\n or \n-intel\n after the software name.  This refers to the compiler that\nwas used to make the software.  If you have a choice then usually the \n-intel\n one will be faster.\n\n\n\nVERSIONS\n: \nmodule load pear-gcc\n would have been sufficient to load the module however it is best-practice (in science) to specify the \nversion number so that the answer you get today will be the answer you get in 1 year time.  Some software will produce different results with different versions\nof the software.\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.6) Now it's \nload\ned, run pear again, what does it do?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe paper citation gives a clue.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink34\").click(function(e){\n            e.preventDefault();\n            $(\"#showable34\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ module load pear-gcc/0.9.4\n[15:59:19] training21@lims-hpc-m ~ $ pear\n ____  _____    _    ____ \n|  _ \\| ____|  / \\  |  _ \\\n| |_) |  _|   / _ \\ | |_) |\n|  __/| |___ / ___ \\|  _ \n\n|_|   |_____/_/   \\_\\_| \\_\\\nPEAR v0.9.4 [August 8, 2014]  - [+bzlib]\n\nCitation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR\nZhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593\n\n... REST REMOVED ...\n\n\n\n\nAnswer\n: \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink35\").click(function(e){\n            e.preventDefault();\n            $(\"#showable35\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.7) \nList\n all the loaded modules. How many are there? Where did all the others come from?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse man to find a subcommand that will list currently loaded modules.\n\n\nWe are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink37\").click(function(e){\n            e.preventDefault();\n            $(\"#showable37\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n \nList\n all the loaded modules. How many are there?\n\n\n$ module list\nCurrently Loaded Modulefiles:\n  1) gmp/5.1.3         3) mpc/1.0.2         5) bzip2-gcc/1.0.6\n  2) mpfr/3.1.2        4) gcc/4.8.2         6) pear-gcc/0.9.4\n\n\n\n\nAnswer\n: 6\n\n\nWhere did all the others come from?\n\n\nYou may have noticed when we loaded \npear-gcc\n the module called \ngcc\n was also loaded; this gives a hint as to where the others come from.\n\n\nAnswer\n: They are \ndependencies\n; that is, they are supporting software that is used by the module we loaded.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink38\").click(function(e){\n            e.preventDefault();\n            $(\"#showable38\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.8) How do you undo the loading of the \npear\n module?  List the loaded modules again, did they all disappear?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nComputer Scientists are not always inventive with naming commands, try something starting with \nun\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink40\").click(function(e){\n            e.preventDefault();\n            $(\"#showable40\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nHow do you undo the loading of the \npear\n module?\n\n\n$ module unload pear-gcc\n\n\n\n\nAnswer\n: the \nunload\n sub-command removes the named module from our current SSH session.\n\n\nList the loaded modules again, did they all disapear?\n\n\nAnswer\n: Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still\nneeded or not so we will need to do it manually (or see next question) \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink41\").click(function(e){\n            e.preventDefault();\n            $(\"#showable41\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.9) How do you clear ALL loaded modules?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIt's easier than running \nunload\n for all modules\n\n\nThis one isn't that straight forward; try a \nsynonym\n of \nrid\n.\n\n\n\n\n\n\nMore\n\n\n\n\nWe will \npurge\n the list of loaded modules.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink44\").click(function(e){\n            e.preventDefault();\n            $(\"#showable44\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable44\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink44\").text(\"More\");\n            } else {\n                $(\"#showablelink44\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink43\").click(function(e){\n            e.preventDefault();\n            $(\"#showable43\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ module purge \n\n\n\n\nAnswer\n: running the \npurge\n sub-command will unload all modules you loaded (and all dependencies).\n\n\nAlternative\n: if you close your SSH connection and re-open it the new session will be blank as well.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink45\").click(function(e){\n            e.preventDefault();\n            $(\"#showable45\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nLIMS-HPC Specific\n: \n\n\nThe following questions use the \nmoduleinfo\n command; this is only available on LIMS-HPC so if you are using\nanother HPC then you will need to skip ahead to topic 3. \n\n\n\n\n\n\n2.10) What does the \nmoduleinfo\n command do?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTry running it (with \nno\n or only \n-h\n option)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink47\").click(function(e){\n            e.preventDefault();\n            $(\"#showable47\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ moduleinfo -h\nmoduleinfo: support application for environment modules to provide \n            licence and citation information about each module\n...\n\n\n\n\nAnswer\n: provides information about modules\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink48\").click(function(e){\n            e.preventDefault();\n            $(\"#showable48\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.11) Find a \ndescription\n of the \nbiostreamtools\n module\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nView the help information provided when you ran \nmoduleinfo -h\n.  Search for a function that displays a description.\n\n\nUse the \nmodule\n command to find the full name for the \nbiostreamtools\n module\n\n\n\n\n\n\nMore\n\n\n\n\nFunction\n: desc\n\n\nModule\n: biostreamtools-gcc/0.4.0\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink51\").click(function(e){\n            e.preventDefault();\n            $(\"#showable51\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable51\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink51\").text(\"More\");\n            } else {\n                $(\"#showablelink51\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink50\").click(function(e){\n            e.preventDefault();\n            $(\"#showable50\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ moduleinfo desc biostreamtools-gcc/0.4.0\nbiostreamtools-gcc/0.4.0: A collection of fast generic bioinformatics \n                          tools implemented in C++\n\n\n\n\nAnswer\n: A collection of fast generic bioinformatics tools implemented in C++.\n\n\nDisclaimer\n: you may find that the author of this software also created this workshop :-P\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink52\").click(function(e){\n            e.preventDefault();\n            $(\"#showable52\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.12) How would you \ncite\n all currently loaded modules?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nView the help information provided when you ran \nmoduleinfo -h\n.  Search for a function that displays a citation.\n\n\n\n\n\n\nMore\n\n\n\n\nFunction\n: cite\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink55\").click(function(e){\n            e.preventDefault();\n            $(\"#showable55\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable55\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink55\").text(\"More\");\n            } else {\n                $(\"#showablelink55\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink54\").click(function(e){\n            e.preventDefault();\n            $(\"#showable54\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAssuming we had the pear module loaded\n\n\n$ moduleinfo cite\ngmp/5.1.3:                No information recorded\nmpfr/3.1.2:               No information recorded\nmpc/1.0.2:                No information recorded\ngcc/4.8.2:                No information recorded\nbzip2-gcc/1.0.6:          No information recorded\npear-gcc/0.9.4:           J. Zhang, K. Kobert, T. Flouri, A. Stamatakis. \n                          PEAR: A fast and accurate Illimuna Paired-End \n                          reAd mergeR\n\n\n\n\nAnswer\n: using the moduleinfo cite function with no module specified will display info for currently loaded modules.\n\n\nNote\n: When you see \n\"No information recorded\"\n it means that there is no moduleinfo record for that module.\n\n\"nil\"\n it means none was requested (at time software was installed, you should check software's website for updates since).\n\n\n\"No record\"\n means nothing could be found for this record/module\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink56\").click(function(e){\n            e.preventDefault();\n            $(\"#showable56\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.13) The malt module requires a special licence, how can you find out details of this?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nView the help information provided when you ran \nmoduleinfo -h\n.  Search for a function that displays a description.\n\n\nUse the \nmodule\n command to find the full name for the \nmalt\n module.\n\n\n\n\n\n\nMore\n\n\n\n\nVerbose flag tells moduleinfo to give more information if it is available\n\n\nFunction\n: licence\n\n\nModule\n: malt/0.1.0\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink59\").click(function(e){\n            e.preventDefault();\n            $(\"#showable59\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable59\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink59\").text(\"More\");\n            } else {\n                $(\"#showablelink59\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink58\").click(function(e){\n            e.preventDefault();\n            $(\"#showable58\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ moduleinfo -v licence malt/0.1.0\n[[malt/0.1.0]]\n-----[licence]-----\n\nCustom Academic:\n\n1) You need to complete this form (which will send you an email):\n   http://www-ab2.informatik.uni-tuebingen.de/software/megan5/register/index.php\n\n2) Save the emailed licence details to a text file (suggested name:\n   '~/megan-license.txt') on the LIMS-HPC.  NOTE: you need to copy the \n   text from the email starting at line \nUser: ...\n and ending with line\n   \nSignature: ...\n\n\n3) When running the malt-* commands you need to specify this file. (Even \n   for the --help option!!!).  e.g. malt-build -L ~/megan-license.txt ...\n\n\n\n\nAnswer\n: issuing the command \nmoduleinfo -v licence malt/0.1.0\n will display details on how to obtain the special\nlicence for malt.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink60\").click(function(e){\n            e.preventDefault();\n            $(\"#showable60\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 3: Job Submission\n\n\nUp to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC.  While this is ok for small jobs on small\nHPCs like LIMS-HPC, it's unworkable for most jobs.  In this topic we will start to learn how to make use of the rest of the HPCs immense compute power\n\n\nBackground\n\n\nOn conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output\nin front of us.  On an HPC this type of computation will only make use of one node, namely, the \nHead Node\n.  To make use of the remaining (\ncompute\n) nodes\nwe need to use the SLURM software package (called an HPC Scheduler).  The purpose of SLURM is to manage all user jobs and distribute the available resources\n(i.e. time on the compute nodes) to each job in a fair manner.  You can think of the SLURM software as like an electronic \ncalendar\n and the user jobs like \n\nmeetings\n.  Users \nsay\n to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job.\n\n\nTerminology\n:\n\n\n\n\nNode\n: a server grade computer which is part of an HPC\n\n\nBatch Job\n: a group of one or more related Unix commands that need to be run (executed) for a user.  e.g. run fastqc on all my samples\n\n\nPartition (or Queue)\n: a list of jobs that need to be run.  There is often more than one partition on an HPC which usually have specific requirements \nfor the jobs that can run be added to them.  e.g. \n8hour\n will accept jobs less than or equal to 8hours long\n\n\nRuntime\n: the amount of time a job is expected (or actually) runs\n\n\nResources\n: computation resources that can be given to our jobs in order to run them.  e.g. CPU Cores, Memory, and Time.\n\n\nJob Script\n: a special BASH script that SLURM uses to run a job on our behalf once resources become available.  Job scripts contain details of the \nresources that our commands need to run.\n\n\nOutput (or Results) file\n: When SLURM runs our batch job it will save the results that would normally be output on the terminal to a file; this file \nis called the output file.\n\n\n\n\nExercises\n\n\nUseful Commands\n: \nman, sinfo, cat, sbatch, squeue, cp, module, prime\n\n\n\n\n\n\n3.1) Which nodes could a \u2018compute\u2019 job go on?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTry the \nsinfo\n command\n\n\n\n\n\n\nmore\n\n\n\n\nHave a look at the PARTITION and NODELIST columns.  The \nlims-hpc-[2-4]\n is shorthand for \nlims-hpc-2 lims-hpc-3 \nand lims-hpc-4\n\n\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncompute*     up 200-00:00:      3    mix lims-hpc-[2-4]\ncompute*     up 200-00:00:      2   idle lims-hpc-[1,5]\nbigmem       up 200-00:00:      1   idle lims-hpc-1\n8hour        up   08:00:00      3    mix lims-hpc-[2-4]\n8hour        up   08:00:00      3   idle lims-hpc-[1,5],lims-hpc-m\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink63\").click(function(e){\n            e.preventDefault();\n            $(\"#showable63\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable63\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink63\").text(\"more\");\n            } else {\n                $(\"#showablelink63\").text(\"less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink62\").click(function(e){\n            e.preventDefault();\n            $(\"#showable62\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThe \nsinfo\n command will list the \npartitions\n.  It summaries the nodes by their current status so there may be more \nthat one line with \ncompute\n in the partition column.  It lists the nodes in shorthand i.e. lims-hpc-[1,3-5] means lims-hpc-1, \nlims-hpc-3, lims-hpc-4, lims-hpc-5.\n\n\nAnswer\n: lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink64\").click(function(e){\n            e.preventDefault();\n            $(\"#showable64\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.2) What about an \u20188hour\u2019 job?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse \nsinfo\n again but look at the 8hour rows\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink66\").click(function(e){\n            e.preventDefault();\n            $(\"#showable66\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5, lims-hpc-m\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink67\").click(function(e){\n            e.preventDefault();\n            $(\"#showable67\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nUse the \ncat\n command to view the contents of \ntask01\n, \ntask02\n and \ntask03\n job script\n\n\n\n\n\n\n3.3) How many \ncpu cores\n will each ask for?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nLookup the man page for \nsbatch\n command.  \nsbatch\n's options match up with the \n#SBATCH\n comments at the top of each job \nscript.  Some will be affected by more than one option\n\n\n\n\n\n\nMore\n\n\n\n\nNon-exclusive (shared) jobs\n:\n\n\nIt is \n--cpus-per-task x --ntasks\n but if \n--ntasks\n is not present it defaults to 1 so its \n--cpus-per-task x 1\n\n\nExclusive jobs\n:\n\n\nThe \n--nodes\n options tells us how many nodes we ask for and the \n--exclusive\n option says give us all it has.  This\none is a bit tricky as we don't really know until it runs.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink70\").click(function(e){\n            e.preventDefault();\n            $(\"#showable70\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable70\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink70\").text(\"More\");\n            } else {\n                $(\"#showablelink70\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink69\").click(function(e){\n            e.preventDefault();\n            $(\"#showable69\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n:\n\n\n\n\ntask01: \n1 cpu core\n\n\ntask02: \n6 cpu cores\n\n\ntask03: \nat least 1\n as this has requested all cpu cores on the node its running on (\n--exclusive\n).\n\nHowever, since we know that all nodes on LIMS-HPC have 16, we know it will get 16.\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink71\").click(function(e){\n            e.preventDefault();\n            $(\"#showable71\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.4) What about total memory?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nLookup the man page for \nsbatch\n command.  \nsbatch\n's options match up with the \n#SBATCH\n comments at the top of each job \nscript.  Some will be affected by more than one option\n\n\n\n\n\n\nMore\n\n\n\n\nThe \n--mem-per-cpu\n OR \n--mem\n options are holding the answer to total memory.\n\n\nFor task01 and task02 the calculation is \n--mem-per-cpu x --cpus-per-task x --ntasks\n\n\nFor task03, like with the cpus cores question, we get all the memory available on the node we get allocated\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink74\").click(function(e){\n            e.preventDefault();\n            $(\"#showable74\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable74\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink74\").text(\"More\");\n            } else {\n                $(\"#showablelink74\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink73\").click(function(e){\n            e.preventDefault();\n            $(\"#showable73\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThe \n--mem-per-cpu\n OR \n--mem\n options are holding the answer to total memory.\n\n\nFor task01 and task02 the calculation is \n--mem-per-cpu x --ntasks x --cpus-per-task\n\n\nFor task03, like with the cpus cores question, we get all the memory available on the node we get allocated\n\n\nNOTE\n: it might be tempting to use the \n--mem\n option on non-exclusive (i.e. \n--shared\n) jobs \nhowever this will \nNOT\n work since the meaning of \n--mem\n is \n\"go on a node with at least X MB of memory\"\n; it does \nnot actually allocate any of it to you so your job will get terminated once it tries to use any memory.\n\n\n\nAnswer\n:\n\n\n\n\ntask01: \n1024MB\n (1GB) i.e. 1024 x 1 x 1\n\n\ntask02: \n12288MB\n (12GB) i.e. 2048 x 3 x 2\n\n\ntask03: \nat least 1024MB\n (1GB). The actual amount could be 128GB (nodes 2 to 5) or 256GB (node 1)\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink75\").click(function(e){\n            e.preventDefault();\n            $(\"#showable75\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.5) How long can each run for?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nman sbatch\n command to look up the time specification.  If you search for \n--time\n it will describe the formats it uses (i.e. type \n\n/--time\n and press enter)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink77\").click(function(e){\n            e.preventDefault();\n            $(\"#showable77\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThe \n--time\n option is what tells slurm how long your job will run for.\n\n\nAnswer\n:\n\n\n\n\ntask01: requests \n30:00 (30mins 0secs)\n, uses ~30secs\n\n\ntask02: requests \n5:00 (5mins 0secs)\n, uses ~5secs\n\n\ntask03: requests \n1:00 (1min 0secs)\n, uses ~30secs\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink78\").click(function(e){\n            e.preventDefault();\n            $(\"#showable78\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.6) Is this maximum, minimum or both runtime?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nman sbatch\n command to look up the time specification.  If you search for \n--time\n it will describe the formats it uses (i.e. type \n\n/--time\n and press enter)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink80\").click(function(e){\n            e.preventDefault();\n            $(\"#showable80\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThis is a maximum time.  Your job may finish early, at which point it hands back the resources for the next job.  However if it \ntries to run longer the HPC will terminate the job.\n\nHINT\n: when selecting a time for your job its best to estimate your job runtime to be close to \nwhat it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough \ntime.  If you think your job may not complete in time you can ask the system administrator of your HPC to add more time.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink81\").click(function(e){\n            e.preventDefault();\n            $(\"#showable81\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.7) Calculate the \n--time\n specification for the following runtimes:\n\n\n\n\n\n\n1h30m:\n--time=\n\n\n1m20s:\n--time=\n\n\n1.5days:\n--time=\n\n\n30m:\n--time=\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink82\").click(function(e){\n            e.preventDefault();\n            $(\"#showable82\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nman sbatch\n command to look up the time specification.  If you search for \n--time\n it will describe the formats it uses (i.e. type \n\n/--time\n and press enter)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink83\").click(function(e){\n            e.preventDefault();\n            $(\"#showable83\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\n\n\n1h30m:\n--time=01:30:00 (alternatively: 0-01:30)\n\n\n1m20s:\n--time=01:20\n\n\n1.5days:\n--time=1-12\n\n\n30m:\n--time=30\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink84\").click(function(e){\n            e.preventDefault();\n            $(\"#showable84\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.8) What do the following --time specifications mean?\n\n\n\n\n\n\n--time=12-00:20\n\n\n--time=45\n\n\n--time=00:30\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink85\").click(function(e){\n            e.preventDefault();\n            $(\"#showable85\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nman sbatch\n command to look up the time specification.  If you search for \n--time\n it will describe the formats it uses (i.e. type \n\n/--time\n and press enter)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink86\").click(function(e){\n            e.preventDefault();\n            $(\"#showable86\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\n\n\n--time=12-00:20\n12 days and 20 minutes\n\n\n--time=45\n45 minutes\n\n\n--time=00:30\n30 seconds\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink87\").click(function(e){\n            e.preventDefault();\n            $(\"#showable87\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nNow use sbatch to submit the \ntask01\n job:\n\n\n\n\n\n\n\n\n3.9) What job id was your job given?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the man page for the sbatch command.  The \nSynopsis\n at the top will give you an idea how to run it.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink89\").click(function(e){\n            e.preventDefault();\n            $(\"#showable89\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\n$ sbatch task01 \nSubmitted batch job 9998\n\n\n\n\nAnswer\n: it's unique for each job; in the above example mine was \n9998\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink90\").click(function(e){\n            e.preventDefault();\n            $(\"#showable90\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.10) Which node did your job go on?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe \nsqueue\n command shows you the currently running jobs.  If its been longer than 30 seconds since you submitted it you might have to resubmit it.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink92\").click(function(e){\n            e.preventDefault();\n            $(\"#showable92\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nUse the \nsqueue\n command to show all jobs.  Search for your \njobid\n and look in the \nNODELIST\n column.\n\n\nNOTE\n: if there are lots of jobs you can use \nsqueue -u YOUR_USERNAME\n to only show your jobs, where \nYOUR_USERNAME is replaced with your actual username.\n\n\n\n$ sbatch task01\nSubmitted batch job 9999\n$ squeue -u training01\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n  9999   compute   task01 training  R       0:05      1 lims-hpc-2\n\n\n\n\nAnswer\n: it's dependent on node availability at time; in the above example mine was \nlims-hpc-2\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink93\").click(function(e){\n            e.preventDefault();\n            $(\"#showable93\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nAdvanced\n\n\n\n\n\n\n3.11) Make a copy of \ntask01\n and call it \nprime_numbers\n.  Make it load the training module and use the \nprime\n command calculate prime \nnumbers for 20 seconds.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nYou can find the \nprime\n command in the \ntraining/1.0\n module\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink95\").click(function(e){\n            e.preventDefault();\n            $(\"#showable95\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThe key points to change in the task01 script are:\n\n\n\n\nadding the \nmodule load training/1.0\n\n\nreplacing the \nsleep\n (and \necho\n) statements with a call to \nprime 20\n.\n\n\n\n\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1024\n#SBATCH --partition=training\n#SBATCH --time=30:00\n\nmodule load training/1.0\n\necho \nStarting at: $(date)\n\nprime 20\necho \nFinished at: $(date)\n\n\n\n\n\nRepeatable Science\n: It's good scientific practice to include the version number of the module when loading it as this will \nensure that the same version is loaded next time you run this script which will mean you get the same results.\n\n\n\nDate your work\n: It's also good practice to include the date command in the output so you have a permanent record \nof when this job was run.  If you have one before and after your main program you will get a record of how long it ran for as well.\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink96\").click(function(e){\n            e.preventDefault();\n            $(\"#showable96\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.12) Submit the job.  What was the \nlargest\n prime number it found in 20 seconds?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe output from the program will provide the results that we are after.  For HPC jobs this will be placed in the \nSLURM output file\n; this is called\n\nslurm-JOBID.out\n where JOBID is replaced by the actual job id.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink98\").click(function(e){\n            e.preventDefault();\n            $(\"#showable98\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nYou should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by \nthe amount of other jobs running on the HPC\n\n\n$ sbatch prime_numbers\nSubmitted batch job 9304\n$ cat slurm-9304.out \nStarting at: Fri May  8 16:11:07 AEST 2015\n\nPrimes:        710119\nLast trial:    10733927\nLargest prime: 10733873\nRuntime:       20 seconds\nFinished at: Fri May  8 16:11:27 AEST 2015\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink99\").click(function(e){\n            e.preventDefault();\n            $(\"#showable99\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.13) Modify your prime_numbers script to notify you via email when it starts and ends.  Submit it again\n\n\n\n\n\n\nDid it start immediately or have some delay?\n\n\nHow long did it actually run for?\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink100\").click(function(e){\n            e.preventDefault();\n            $(\"#showable100\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nThere are two options that you will need to set.  See sbatch manpage for details.\n\n\n\n\n\n\nMore\n\n\n\n\nBoth start with \n--mail\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink102\").click(function(e){\n            e.preventDefault();\n            $(\"#showable102\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable102\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink102\").text(\"More\");\n            } else {\n                $(\"#showablelink102\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink101\").click(function(e){\n            e.preventDefault();\n            $(\"#showable101\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1024\n#SBATCH --partition=training\n#SBATCH --time=30:00\n#SBATCH --mail-user=name@email.address\n#SBATCH --mail-type=ALL\n\nmodule load training/1.0\n\necho \nStarting at: $(date)\n\nprime 20\necho \nFinished at: $(date)\n\n\n\n\n\nAnswers\n:\n\n\n\n\nDid it start immediately or have some delay?\n The \nQueued time\n value in the subject of start email will tell you how long it waited.\n\n\nHow long did it actually run for?\n The \nRun time\n value in the subject of the end email will tell you how long it ran for which should \nbe ~20 seconds.\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink103\").click(function(e){\n            e.preventDefault();\n            $(\"#showable103\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 4: Job Monitoring\n\n\nIt is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based \non a number of factors (data set, number of CPU's, processing step etc.).\n\n\nIn this topic we will cover some of the tools that are available to you to \nwatch\n what is happening so we can make better predictions\nin the future.\n\n\nExercises\n\n\n\n\n\n\n4.1) What does the \ntop\n command show?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWhen all else fails, try \nman\n; specifically, the description section\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink105\").click(function(e){\n            e.preventDefault();\n            $(\"#showable105\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\n$ man top\n...\nDESCRIPTION\n       The top program provides a dynamic real-time view of a running system.\n...\n\n\n\n\nAnswer\n: in lay-person terms \n\"Continually updating CPU and Memory usage\"\n \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink106\").click(function(e){\n            e.preventDefault();\n            $(\"#showable106\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nRun the \ntop\n command.  Above the black line it shows some \nsystem-wide statistics\n and below are statistics specific to a single \nprocess (a.k.a, tasks OR software applications).\n\n\n\n\n\n\n4.2) How much total memory does this HPC (head-node) have?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThis would be a system-wide statistic.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink108\").click(function(e){\n            e.preventDefault();\n            $(\"#showable108\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: If you look at the first value on the \nMem\n line (line 4) it will tell you the total memory on this computer (node).\n\n\n\n\nLIMS-HPC\n: 132085396k or ~128 GigaBytes\n\n\nMERRI\n: 49413840k or ~48 GigaBytes\n\n\nBARCOO\n: 65942760k or ~64 GigaBytes\n\n\n\n\nTo transfer from kB to MB you divide by 1024 and MB to GB by 1024 again.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink109\").click(function(e){\n            e.preventDefault();\n            $(\"#showable109\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.3) What is the current total CPU usage?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThis might be easier to work out what is not used and subtract it from 100%\n\n\n\n\n\n\nMore\n\n\n\n\nIdle\n is another term for not used (or \nid\n for short)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink112\").click(function(e){\n            e.preventDefault();\n            $(\"#showable112\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable112\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink112\").text(\"More\");\n            } else {\n                $(\"#showablelink112\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink111\").click(function(e){\n            e.preventDefault();\n            $(\"#showable111\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: If you subtract the \n%id\n value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink113\").click(function(e){\n            e.preventDefault();\n            $(\"#showable113\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.4) What column does it appear to be sorting the processes by? Is this \nlow-to-high\n OR \nhigh-to-low\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIts not PID but from time to time it might be ordered sequentially.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink115\").click(function(e){\n            e.preventDefault();\n            $(\"#showable115\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: \n%CPU\n which gives you an indication of how much CPU time each process uses\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink116\").click(function(e){\n            e.preventDefault();\n            $(\"#showable116\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nAdd up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time.  NOTE: you may need to quit \n\ntop\n (by pressing q) so you can compare before it updates.\n\n\n\n\n\n\n4.5) Why might the numbers disagree?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIt might have something to do with the total number of CPU Cores on the system.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink118\").click(function(e){\n            e.preventDefault();\n            $(\"#showable118\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: \n%CPU\n column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top \nare exactly that, how much the entire system is utilised.  i.e. if you could see all processes in \ntop\n (excluding round errors) \nthey would add up 100% x the number of cpu cores available; on LIMS-HPC this would be 0-1600% in the individual processes and 0-100% \non the system-wide section.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink119\").click(function(e){\n            e.preventDefault();\n            $(\"#showable119\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.6) What command-line flag instructs \ntop\n to sort results by \n%MEM\n?\n\n\n\n\nCan you think of a reason that this might be useful?\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink120\").click(function(e){\n            e.preventDefault();\n            $(\"#showable120\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nUse the \ntop\n manpage.\n\n\n\n\n\n\nMore\n\n\n\n\n\"m is for memory!\"\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink122\").click(function(e){\n            e.preventDefault();\n            $(\"#showable122\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable122\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink122\").text(\"More\");\n            } else {\n                $(\"#showablelink122\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink121\").click(function(e){\n            e.preventDefault();\n            $(\"#showable121\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: \ntop -m\n will cause \ntop\n to sort the processes by memory usage.\n\n\nCan you think of a reason that this might be useful?\n\n\nYour program might be using a lot of memory and you want to know how much, by sorting by memory will cause your program to stay at the top.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink123\").click(function(e){\n            e.preventDefault();\n            $(\"#showable123\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.7) Run \n\"top -c\"\n.  What does it do?  How might this be helpful?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \ntop\n manpage.\n\n\n\n\n\n\nMore\n\n\n\n\n\"c is for complete!\"\n\n\n\"c is also for command!\"\n which is another name for program\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink126\").click(function(e){\n            e.preventDefault();\n            $(\"#showable126\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable126\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink126\").text(\"More\");\n            } else {\n                $(\"#showablelink126\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink125\").click(function(e){\n            e.preventDefault();\n            $(\"#showable125\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nWhat does it do?\n\nIt changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options.\n\n\nHow might this be helpful?\n\nSometimes you might be running a lot of commands with the same name that only differ by the command-line options.  In this case it is hard \nto tell which ones are still running unless you use the \n-c\n flag to show the complete command.\n\n\nNOTE\n:\n\nIf \ntop\n is running you can press the \nc\n key to toggle show/hide complete command\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink127\").click(function(e){\n            e.preventDefault();\n            $(\"#showable127\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.8) How can you get \ntop\n to only show your processes?  Why might this be useful?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \ntop\n manpage.\n\n\n\n\n\n\nMore\n\n\n\n\n\"u is for user[name]!\"\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink130\").click(function(e){\n            e.preventDefault();\n            $(\"#showable130\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable130\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink130\").text(\"More\");\n            } else {\n                $(\"#showablelink130\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink129\").click(function(e){\n            e.preventDefault();\n            $(\"#showable129\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nHow can you get \ntop\n to only show your processes?\n\n\nAnswer 1\n: \ntop -u YOURUSERNAME\n\n\nAnswer 2\n: while running \ntop\n press the \nu\n key, type YOURUSERNAME and press \n key \n\n\nWhy might this be useful?\n\nWhen you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard\nto quickly identify yours.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink131\").click(function(e){\n            e.preventDefault();\n            $(\"#showable131\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nLIMS-HPC Specific\n\n\nLIMS-HPC has an extra monitoring and graphing tool called Munin.\n\n\nOpen the munin webpage and have a look at the graphs\n\n\nMunin\n: \nhttp://munin-lims.latrobe.edu.au/lims-hpc.html\n\n\n\n\n\n\n4.9) What are the graphs showing?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTake a look at the title on the graphs.  Then the style of graphs.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink133\").click(function(e){\n            e.preventDefault();\n            $(\"#showable133\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nAnswer\n: CPU usage (stacked by type of usage)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink134\").click(function(e){\n            e.preventDefault();\n            $(\"#showable134\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.10) How much is the Compute Node 5 being used currently?\n\n\n\n\n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nYou can either:\n\n\n\n\nestimate this off the right most position on the graph (everything except mid-yellow is the CPU doing something) or \n\n\nlook at the \ncur\n value for \nidle\n and subtract it from 1600 (the maximum value for a 16 core server)\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink136\").click(function(e){\n            e.preventDefault();\n            $(\"#showable136\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.11) And at midday yesterday?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIts easiest to think in reverse (i.e. What is not being used?)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink138\").click(function(e){\n            e.preventDefault();\n            $(\"#showable138\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nYou have to estimate system idle at the point on the graph indicating 12:00 (yesterday).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink139\").click(function(e){\n            e.preventDefault();\n            $(\"#showable139\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 5: All Together\n\n\nThis topic will allow you to put all the skills that you learnt in this workshop to the test.  You might need to refer back to\nthe earlier topics if you have forgotten how to do these tasks.\n\n\nOverview\n:\n\n\n\n\nWrite jobscript\n\n\nLoad/use software module\n\n\nSubmit job\n\n\nMonitor job\n\n\n\n\n\n\nNOTE (for later)\n: to complete this topic from your regular LIMS-HPC account you will need to:\n\n\n\nfirst setup node logins.  This has already been done for your *training* account so no need to do this today\n\nLIMS-HPC Node Login Setup\n\n\n\n\nuse the *compute* partition instead of *training*\n\n\n\n\n\n\n\n\nTask 1: Write a job script\n\n\nWrite a job script that requests the following resources:\n\n\n\n\nFilename\n: monINITIALS.slurm\n\n\nwhere INITIALS is replaced with your initials.  e.g. for me it would be monAR.slurm\n\n\n\n\n\n\nTasks\n: 1 \n\n\nCPUs\n: 1\n\n\nPartition\n: training \n\n\nTime\n: 5 mins \n\n\nMemory\n: 1 GB (remember to specify it in MB)\n\n\n\n\nTask 2: Load/use software module\n\n\nEdit your job script so that it: \n\n\n\n\nLoads the training module\n\n\nRuns the \nfakejob\n command with your name as the first parameter\n\n\n\n\n\n\nNOTE\n: remember good practice here and add the date commands to print the date/time in your output.  You can copy them from the *task01* script.\n\n\n\n\nTask 3: Submit job\n\n\n\n\nUse \nsbatch\n to submit the job to the HPC.\n\n\nNote down the job id it was given (for later).\n\n\nUse squeue (or qs) to check that is started ok.\n\n\nWhen it starts check which compute node it is running on (for the next task).\n\n\n\n\nTask 4: Monitor the job\n\n\nUse the \ntop\n command to check how much CPU and Memory the job is using.  Given that SLURM is running the job on your behalf on one of the compute \nnodes, \ntop\n wont be able to see the job.  To be able to use top, you will first need to login to the compute node the is running your job.\n\n\nTo login:\n\n\n$ ssh lims-hpc-X\n\n\n\n\nWhere X is the actual node number you were allocated (See task 3.4).\n\n\nYou are now connected from your computer to lims-hpc-m which is connected to lims-hpc-X.\n\n\n+---------------+            +------------+            +------------+\n| YOUR COMPUTER | -- SSH --\n | LIMS-HPC-M | -- SSH --\n | LIMS-HPC-X |\n+---------------+            +------------+            +------------+\n\n\n\n\nYou can tell which node you are on by the text in the prompt\n\n\n[10:00:06] USERNAME@lims-hpc-m ~ $ \n\nChanges to:\n\n[10:06:05] USERNAME@lims-hpc-1 ~ $\n\n\n\n\nOnce logged in to the relevent compute node you can run \ntop\n to view you job.  Remember the \nu\n and \nc\n options we learnt earlier; they will be helpful \nhere when everyone is running the same jobs.\n\n\n\n\n\n\n\n\nHow does the CPU and Memory usage change over time?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIt should vary (within the limits you set in the job script)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink141\").click(function(e){\n            e.preventDefault();\n            $(\"#showable141\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nShow \nAnswer\n\n\n\n\nThe \nfakejob\n program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory (on lims-hpc-[m,2-5] this will equate to 0.4 to 0.8%)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink142\").click(function(e){\n            e.preventDefault();\n            $(\"#showable142\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nFinished\n\n\nWell done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it \nwas a lot to take in.\n\n\nFrom here you should be confortable to begin submitting real jobs to the HPC (in your real account, \nnot the training one).\n\n\nYou will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this \nWorkshop for later reference.\n\n\nThank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it\nback to the Workshop facilitators.", 
            "title": "Introduction to HPC"
        }, 
        {
            "location": "/tutorials/hpc/#high-performance-computing", 
            "text": "A hands-on-workshop covering High-Performance Computing (HPC)", 
            "title": "High-Performance Computing"
        }, 
        {
            "location": "/tutorials/hpc/#how-to-use-this-workshop", 
            "text": "The workshop is broken up into a number of  Topics  each focusing on a particular aspect of HPCs.  You should take a short break between \neach to refresh and relax before tackling the next.  Topic s may start with some background followed by a number of  exercises .  Each  exercise  begins with a  question , then \nsometimes a  hint  (or two) and finishes with the suggested  answer .", 
            "title": "How to use this workshop"
        }, 
        {
            "location": "/tutorials/hpc/#question", 
            "text": "An example question looks like:    What is the Answer to Life?    (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)", 
            "title": "Question"
        }, 
        {
            "location": "/tutorials/hpc/#hint", 
            "text": "Depending on how much of a challenge you like, you may choose to use hints.  Even if you work out the answer without hints, its a good \nidea to read the hints afterwards because they contain extra information that is good to know.  Note:  hint s may be staged, that is, there may be a  more  section within a hint for further hints    Hint   - click here to reveal hint   What is the answer to everything?  As featured in  \"The Hitchhiker's Guide to the Galaxy\"    More   - and here to show more   It is probably a two digit number     \n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable2\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink2\").text(\"More\");\n            } else {\n                $(\"#showablelink2\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Hint"
        }, 
        {
            "location": "/tutorials/hpc/#answer", 
            "text": "Once you have worked out the answer to the question expand the Answer section to check if you got it correct.    Answer   - click here to reveal answer   Answer : 42  Ref:  Number 42 (Wikipedia)     \n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Answer"
        }, 
        {
            "location": "/tutorials/hpc/#usage-style", 
            "text": "This workshop attempts to cater for two usage styles:   Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional):  Attempt to answer the question by yourself.  Use hints when you get stuck.  Once solved, reveal the answer and read through our suggested solution.  Its a good idea to read the hints and answer description as they often contain extra useful information.    By example :  for those who learn by following examples:   Expand  all sections  Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the\n  same (or similar) answers.  Its a good idea to read the hints and answer description as they often contain extra useful information.", 
            "title": "Usage Style"
        }, 
        {
            "location": "/tutorials/hpc/#connecting-to-hpc", 
            "text": "To begin this workshop you will need to connect to an HPC.  Today we will use the LIMS-HPC.  The computer called  lims-hpc-m  (m is for \nmaster which is another name for head node) is the one that coordinates all the HPCs tasks.  Server details :   host : lims-hpc-m.latrobe.edu.au  port : 6022   username : trainingXX (where XX is a two digit number, provided at workshop)  password : PROVIDED at workshop    Connection instructions :    Mac OS X / Linux   Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line.  To use OpenSSH you must \nfirst start a terminal program on your computer.  On OS X the standard terminal is called Terminal, and it is installed by default. \nOn Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good \ndefault).  When you've started the terminal you should see a command prompt.  To log into LIMS-HPC, for example, type this command at \nthe prompt and press return (where the word username is replaced with your LIMS-HPC username):  ssh -p 6022 username@lims-hpc-m.latrobe.edu.au  The same procedure works for any other machine where you have an account except most other HPCs will not need the  -p 6022  \n(which is telling ssh to connect on a non-standard port number).  You may be presented with a message along the lines of:  The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be  established.\n...\nAre you sure you want to continue connecting (yes/no)?  Although you should never ignore a warning, this particular one is nothing to be concerned about; type  yes  and then  press enter . \nIf all goes well you will be asked to enter your password.  Assuming you type the correct username and password the system should \nthen display a welcome message, and then present you with a Unix prompt.  If you get this far then you are ready to start entering \nUnix commands and thus begin using the remote computer.     \n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Windows   On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client.  PuTTY (putty.exe) can be downloaded \nfrom this web page:  http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html  Documentation for using PuTTY is here:  http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html  When you start PuTTY you should see a window which looks something like this:   To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and  6022  in the port, \nthen click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the \nsame as they are in the picture above).  In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines \nof  \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by \nclicking on Yes). You usually see this message the first time you try to connect to a particular remote computer.  If all goes well, a terminal window will open, showing a prompt with the text  \"login as:\" . An example terminal window is shown \nbelow. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your \npassword. Assuming you type the correct username and password the system should then display a welcome message, and then \npresent you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using \nthe remote computer.      \n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Connecting to HPC"
        }, 
        {
            "location": "/tutorials/hpc/#topic-1-exploring-an-hpc", 
            "text": "An HPC (short for \u2018High Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems.   Figure : Overview of the computers involved when using an HPC.  Computer systems are shown in rectangles and arrows represent interactions.", 
            "title": "Topic 1: Exploring an HPC"
        }, 
        {
            "location": "/tutorials/hpc/#exercises", 
            "text": "1.1) What is the contact email for your HPC's System Administrator?      Hint   When you login, you will be presented with a message; this is called the  Message Of The Day  and usually includes lots of useful \ninformation.  On LIMS-HPC this includes a list of useful commands, the last login details for your account and the contact email\nof the system administrator     \n    $(document).ready(function(){\n        $(\"#showablelink7\").click(function(e){\n            e.preventDefault();\n            $(\"#showable7\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    LIMS-HPC: andrew.robinson@latrobe.edu.au  SNOWY   BARCOO: help@melbournebioinformatics.org.au      \n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    });\n         1.2) Run the  sinfo  command.  How many nodes are there in this hpc?      Hint   The  lims-hpc-[2-4]  is shorthand for  lims-hpc-2 lims-hpc-3 and lims-hpc-4  and  lims-hpc-[1,5]  is shorthand for lims-hpc-1 and lims-hpc-5    more   Have a look at the NODELIST column.  Only count each node once.  $ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncompute*     up 200-00:00:      3    mix lims-hpc-[2-4]\ncompute*     up 200-00:00:      2   idle lims-hpc-[1,5]\nbigmem       up 200-00:00:      1   idle lims-hpc-1\n8hour        up   08:00:00      3    mix lims-hpc-[2-4]\n8hour        up   08:00:00      3   idle lims-hpc-[1,5],lims-hpc-m  NOTE: the above list will vary depending on the HPC setup.     \n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable11\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink11\").text(\"more\");\n            } else {\n                $(\"#showablelink11\").text(\"less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink10\").click(function(e){\n            e.preventDefault();\n            $(\"#showable10\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The  sinfo  command lists all available partitions and the status of each node within them.  If you count up the names of nodes \n(uniquely) you will get the total nodes in this cluster.     LIMS-HPC:  6  ( lims-hpc-m  and  lims-hpc-1  through  lims-hpc-5 )  MERRI:  84  ( turpin  and  merri001  through  merri083 )  BARCOO:  70  ( barcoo001  through  barcoo070 )      \n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Alternate Method   An automatic (though more complex) way would have been running the following command:  $ scontrol show node | grep NodeName | wc -l  Where:   scontrol show node : lists details of all nodes (over multiple lines)  grep NodeName : only shows the NodeName line  wc -l : counts the number of lines      \n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Exercises"
        }, 
        {
            "location": "/tutorials/hpc/#topic-2-software-modules", 
            "text": "Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers.\nLarge computing systems such as HPCs often use a system of modules to load specific software packages (and versions)\nwhen needed for the user.  In this topic we will discover what science software modules (tools) are available and load them ready for analysis.  This topic uses the  man  and  module  commands heavily", 
            "title": "Topic 2: Software Modules"
        }, 
        {
            "location": "/tutorials/hpc/#exercises_1", 
            "text": "2.1) What happens if you run the  module  command without any options / arguments?      Hint   Literally type  module  and press  ENTER  key.     \n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Answer : It prints an error followed by a list of available options / flags  $ module\ncmdModule.c(166):ERROR:11: Usage is 'module command  [arguments ...] '\n\n  Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991):\n\n  Usage: module [ switches ] [ subcommand ] [subcommand-args ]\n\nSwitches:\n    -H|--help       this usage info\n    -V|--version        modules version   configuration options\n    -f|--force      force active dependency resolution\n    -t|--terse      terse    format avail and list format\n    -l|--long       long     format avail and list format\n    -h|--human      readable format avail and list format\n    -v|--verbose        enable  verbose messages\n    -s|--silent     disable verbose messages\n    -c|--create     create caches for avail and apropos\n    -i|--icase      case insensitive\n    -u|--userlvl  lvl   set user level to (nov[ice],exp[ert],adv[anced])\n  Available SubCommands and Args:\n    + add|load      modulefile [modulefile ...]\n    + rm|unload     modulefile [modulefile ...]\n    + switch|swap       [modulefile1] modulefile2\n    + display|show      modulefile [modulefile ...]\n    + avail         [modulefile [modulefile ...]]\n    + use [-a|--append] dir [dir ...]\n    + unuse         dir [dir ...]\n    + update\n    + refresh\n    + purge\n    + list\n    + clear\n    + help          [modulefile [modulefile ...]]\n    + whatis        [modulefile [modulefile ...]]\n    + apropos|keyword   string\n    + initadd       modulefile [modulefile ...]\n    + initprepend       modulefile [modulefile ...]\n    + initrm        modulefile [modulefile ...]\n    + initswitch        modulefile1 modulefile2\n    + initlist\n    + initclear     \n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.2) How do you find a list of  available  software?      Hint   Try the  module  command.  Don't forget the  man  command to get help for a command    More   Run the command  man module  Use a search to find out about the  avail  subcommand (e.g. /avail enter )     \n    $(document).ready(function(){\n        $(\"#showablelink19\").click(function(e){\n            e.preventDefault();\n            $(\"#showable19\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable19\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink19\").text(\"More\");\n            } else {\n                $(\"#showablelink19\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The module command is used to show details of software modules (tools).  Answer :  $ module avail\n\n------------------- /usr/share/Modules/modulefiles --------------------\ndot         module-git  module-info modules     null        use.own\n\n------------------- /usr/local/Modules/modulefiles --------------------\nacana/1.60                         mafft-gcc/7.215\naftrrad/4.1.20150201               malt/0.1.0\narlequin/3.5.1.3                   matplotlib-gcc/1.3.1\n...  The modules list has been shortened because it is very long.  The modules after the  /usr/local/Modules/modulefiles  line\nare the science software; before this are a few built-in ones that you can ignore.     \n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.3) How many modules are there starting with \u2018 f \u2019?      Hint   Run the command  man module  Use a search to find out about the  avail  subcommand (e.g. /avail enter ).  You may have to press 'n' a few times\nto reach the section where the it describes the  avail  subcommand.    More    If an argument is given, then each directory in the MODULEPATH is searched for modulefiles\nwhose pathname match the argument   This is a quote from the manual page for the module command explaining the avail subcommand.  It uses rather technical \nlanguage but basically it's saying you can put search terms after the avail subcommand when entering the command.     \n    $(document).ready(function(){\n        $(\"#showablelink23\").click(function(e){\n            e.preventDefault();\n            $(\"#showable23\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable23\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink23\").text(\"More\");\n            } else {\n                $(\"#showablelink23\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The man page told us that we could put a search term after  module avail .  $ module avail f\n------------------- /usr/local/Modules/modulefiles -------------------\nfasta-gcc/35.4.12            flex-gcc/2.5.39\nfastqc/0.10.1                fontconfig-gcc/2.11.93\nfastStructure-gcc/2013.11.07 freebayes-gcc/20140603\nfastx_toolkit-gcc/0.0.14     freetype-gcc/2.5.3  Answer : 8 modules     \n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Alternate Method   To get a fully automated solution your could do the following command:  $ module -l avail 2 1 | grep  ^f  | wc -l  Where:   module -l avail : lists all modules (in long format, i.e. one per line)  2 1 : merges output from  standard error  to the  standard output  so it can be feed into grep.  For some reason the\ndevelopers of the  module  command thought it was a good idea to output the module names on the  error  stream rather than\nthe logical  output  stream.  grep \"^f\" : only shows lines beginning with  f  wc -l : counts the number of lines      \n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.4) Run the  pear  command (without loading it), does it work?      Hint   This question is very literal     \n    $(document).ready(function(){\n        $(\"#showablelink27\").click(function(e){\n            e.preventDefault();\n            $(\"#showable27\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ pear\n-bash: pear: command not found  The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear'  Answer : No, command not found     \n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.5) How would we  load  the  pear  module?      Hint   Check the man page for  module  again and look for a subcommand that might load modules; it is quite literal as well.    More   Run the command  man module  Use a search to find out about the  load  subcommand (e.g. /load enter )     \n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable31\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink31\").text(\"More\");\n            } else {\n                $(\"#showablelink31\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink30\").click(function(e){\n            e.preventDefault();\n            $(\"#showable30\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ module load pear-gcc/0.9.4  -gcc | -intel : Lots of modules will have either  -gcc  or  -intel  after the software name.  This refers to the compiler that\nwas used to make the software.  If you have a choice then usually the  -intel  one will be faster.  VERSIONS :  module load pear-gcc  would have been sufficient to load the module however it is best-practice (in science) to specify the \nversion number so that the answer you get today will be the answer you get in 1 year time.  Some software will produce different results with different versions\nof the software.     \n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.6) Now it's  load ed, run pear again, what does it do?      Hint   The paper citation gives a clue.     \n    $(document).ready(function(){\n        $(\"#showablelink34\").click(function(e){\n            e.preventDefault();\n            $(\"#showable34\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ module load pear-gcc/0.9.4\n[15:59:19] training21@lims-hpc-m ~ $ pear\n ____  _____    _    ____ \n|  _ \\| ____|  / \\  |  _ \\\n| |_) |  _|   / _ \\ | |_) |\n|  __/| |___ / ___ \\|  _  \n|_|   |_____/_/   \\_\\_| \\_\\\nPEAR v0.9.4 [August 8, 2014]  - [+bzlib]\n\nCitation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR\nZhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593\n\n... REST REMOVED ...  Answer : \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap)     \n    $(document).ready(function(){\n        $(\"#showablelink35\").click(function(e){\n            e.preventDefault();\n            $(\"#showable35\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.7)  List  all the loaded modules. How many are there? Where did all the others come from?      Hint   Use man to find a subcommand that will list currently loaded modules.  We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough.     \n    $(document).ready(function(){\n        $(\"#showablelink37\").click(function(e){\n            e.preventDefault();\n            $(\"#showable37\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer     List  all the loaded modules. How many are there?  $ module list\nCurrently Loaded Modulefiles:\n  1) gmp/5.1.3         3) mpc/1.0.2         5) bzip2-gcc/1.0.6\n  2) mpfr/3.1.2        4) gcc/4.8.2         6) pear-gcc/0.9.4  Answer : 6  Where did all the others come from?  You may have noticed when we loaded  pear-gcc  the module called  gcc  was also loaded; this gives a hint as to where the others come from.  Answer : They are  dependencies ; that is, they are supporting software that is used by the module we loaded.     \n    $(document).ready(function(){\n        $(\"#showablelink38\").click(function(e){\n            e.preventDefault();\n            $(\"#showable38\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.8) How do you undo the loading of the  pear  module?  List the loaded modules again, did they all disappear?      Hint   Computer Scientists are not always inventive with naming commands, try something starting with  un     \n    $(document).ready(function(){\n        $(\"#showablelink40\").click(function(e){\n            e.preventDefault();\n            $(\"#showable40\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   How do you undo the loading of the  pear  module?  $ module unload pear-gcc  Answer : the  unload  sub-command removes the named module from our current SSH session.  List the loaded modules again, did they all disapear?  Answer : Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still\nneeded or not so we will need to do it manually (or see next question)      \n    $(document).ready(function(){\n        $(\"#showablelink41\").click(function(e){\n            e.preventDefault();\n            $(\"#showable41\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.9) How do you clear ALL loaded modules?      Hint   It's easier than running  unload  for all modules  This one isn't that straight forward; try a  synonym  of  rid .    More   We will  purge  the list of loaded modules.     \n    $(document).ready(function(){\n        $(\"#showablelink44\").click(function(e){\n            e.preventDefault();\n            $(\"#showable44\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable44\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink44\").text(\"More\");\n            } else {\n                $(\"#showablelink44\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink43\").click(function(e){\n            e.preventDefault();\n            $(\"#showable43\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ module purge   Answer : running the  purge  sub-command will unload all modules you loaded (and all dependencies).  Alternative : if you close your SSH connection and re-open it the new session will be blank as well.     \n    $(document).ready(function(){\n        $(\"#showablelink45\").click(function(e){\n            e.preventDefault();\n            $(\"#showable45\").toggleClass(\"showable-hidden\");\n        });\n    });\n       LIMS-HPC Specific :   The following questions use the  moduleinfo  command; this is only available on LIMS-HPC so if you are using\nanother HPC then you will need to skip ahead to topic 3.     2.10) What does the  moduleinfo  command do?      Hint   Try running it (with  no  or only  -h  option)     \n    $(document).ready(function(){\n        $(\"#showablelink47\").click(function(e){\n            e.preventDefault();\n            $(\"#showable47\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ moduleinfo -h\nmoduleinfo: support application for environment modules to provide \n            licence and citation information about each module\n...  Answer : provides information about modules     \n    $(document).ready(function(){\n        $(\"#showablelink48\").click(function(e){\n            e.preventDefault();\n            $(\"#showable48\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.11) Find a  description  of the  biostreamtools  module      Hint   View the help information provided when you ran  moduleinfo -h .  Search for a function that displays a description.  Use the  module  command to find the full name for the  biostreamtools  module    More   Function : desc  Module : biostreamtools-gcc/0.4.0     \n    $(document).ready(function(){\n        $(\"#showablelink51\").click(function(e){\n            e.preventDefault();\n            $(\"#showable51\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable51\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink51\").text(\"More\");\n            } else {\n                $(\"#showablelink51\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink50\").click(function(e){\n            e.preventDefault();\n            $(\"#showable50\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ moduleinfo desc biostreamtools-gcc/0.4.0\nbiostreamtools-gcc/0.4.0: A collection of fast generic bioinformatics \n                          tools implemented in C++  Answer : A collection of fast generic bioinformatics tools implemented in C++.  Disclaimer : you may find that the author of this software also created this workshop :-P     \n    $(document).ready(function(){\n        $(\"#showablelink52\").click(function(e){\n            e.preventDefault();\n            $(\"#showable52\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.12) How would you  cite  all currently loaded modules?      Hint   View the help information provided when you ran  moduleinfo -h .  Search for a function that displays a citation.    More   Function : cite     \n    $(document).ready(function(){\n        $(\"#showablelink55\").click(function(e){\n            e.preventDefault();\n            $(\"#showable55\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable55\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink55\").text(\"More\");\n            } else {\n                $(\"#showablelink55\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink54\").click(function(e){\n            e.preventDefault();\n            $(\"#showable54\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Assuming we had the pear module loaded  $ moduleinfo cite\ngmp/5.1.3:                No information recorded\nmpfr/3.1.2:               No information recorded\nmpc/1.0.2:                No information recorded\ngcc/4.8.2:                No information recorded\nbzip2-gcc/1.0.6:          No information recorded\npear-gcc/0.9.4:           J. Zhang, K. Kobert, T. Flouri, A. Stamatakis. \n                          PEAR: A fast and accurate Illimuna Paired-End \n                          reAd mergeR  Answer : using the moduleinfo cite function with no module specified will display info for currently loaded modules.  Note : When you see  \"No information recorded\"  it means that there is no moduleinfo record for that module. \"nil\"  it means none was requested (at time software was installed, you should check software's website for updates since).  \"No record\"  means nothing could be found for this record/module     \n    $(document).ready(function(){\n        $(\"#showablelink56\").click(function(e){\n            e.preventDefault();\n            $(\"#showable56\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.13) The malt module requires a special licence, how can you find out details of this?      Hint   View the help information provided when you ran  moduleinfo -h .  Search for a function that displays a description.  Use the  module  command to find the full name for the  malt  module.    More   Verbose flag tells moduleinfo to give more information if it is available  Function : licence  Module : malt/0.1.0     \n    $(document).ready(function(){\n        $(\"#showablelink59\").click(function(e){\n            e.preventDefault();\n            $(\"#showable59\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable59\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink59\").text(\"More\");\n            } else {\n                $(\"#showablelink59\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink58\").click(function(e){\n            e.preventDefault();\n            $(\"#showable58\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ moduleinfo -v licence malt/0.1.0\n[[malt/0.1.0]]\n-----[licence]-----\n\nCustom Academic:\n\n1) You need to complete this form (which will send you an email):\n   http://www-ab2.informatik.uni-tuebingen.de/software/megan5/register/index.php\n\n2) Save the emailed licence details to a text file (suggested name:\n   '~/megan-license.txt') on the LIMS-HPC.  NOTE: you need to copy the \n   text from the email starting at line  User: ...  and ending with line\n    Signature: ... \n\n3) When running the malt-* commands you need to specify this file. (Even \n   for the --help option!!!).  e.g. malt-build -L ~/megan-license.txt ...  Answer : issuing the command  moduleinfo -v licence malt/0.1.0  will display details on how to obtain the special\nlicence for malt.     \n    $(document).ready(function(){\n        $(\"#showablelink60\").click(function(e){\n            e.preventDefault();\n            $(\"#showable60\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Exercises"
        }, 
        {
            "location": "/tutorials/hpc/#topic-3-job-submission", 
            "text": "Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC.  While this is ok for small jobs on small\nHPCs like LIMS-HPC, it's unworkable for most jobs.  In this topic we will start to learn how to make use of the rest of the HPCs immense compute power", 
            "title": "Topic 3: Job Submission"
        }, 
        {
            "location": "/tutorials/hpc/#background", 
            "text": "On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output\nin front of us.  On an HPC this type of computation will only make use of one node, namely, the  Head Node .  To make use of the remaining ( compute ) nodes\nwe need to use the SLURM software package (called an HPC Scheduler).  The purpose of SLURM is to manage all user jobs and distribute the available resources\n(i.e. time on the compute nodes) to each job in a fair manner.  You can think of the SLURM software as like an electronic  calendar  and the user jobs like  meetings .  Users  say  to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job.  Terminology :   Node : a server grade computer which is part of an HPC  Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user.  e.g. run fastqc on all my samples  Partition (or Queue) : a list of jobs that need to be run.  There is often more than one partition on an HPC which usually have specific requirements \nfor the jobs that can run be added to them.  e.g.  8hour  will accept jobs less than or equal to 8hours long  Runtime : the amount of time a job is expected (or actually) runs  Resources : computation resources that can be given to our jobs in order to run them.  e.g. CPU Cores, Memory, and Time.  Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available.  Job scripts contain details of the \nresources that our commands need to run.  Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal to a file; this file \nis called the output file.", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/hpc/#exercises_2", 
            "text": "Useful Commands :  man, sinfo, cat, sbatch, squeue, cp, module, prime    3.1) Which nodes could a \u2018compute\u2019 job go on?      Hint   Try the  sinfo  command    more   Have a look at the PARTITION and NODELIST columns.  The  lims-hpc-[2-4]  is shorthand for  lims-hpc-2 lims-hpc-3 \nand lims-hpc-4  $ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncompute*     up 200-00:00:      3    mix lims-hpc-[2-4]\ncompute*     up 200-00:00:      2   idle lims-hpc-[1,5]\nbigmem       up 200-00:00:      1   idle lims-hpc-1\n8hour        up   08:00:00      3    mix lims-hpc-[2-4]\n8hour        up   08:00:00      3   idle lims-hpc-[1,5],lims-hpc-m     \n    $(document).ready(function(){\n        $(\"#showablelink63\").click(function(e){\n            e.preventDefault();\n            $(\"#showable63\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable63\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink63\").text(\"more\");\n            } else {\n                $(\"#showablelink63\").text(\"less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink62\").click(function(e){\n            e.preventDefault();\n            $(\"#showable62\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   The  sinfo  command will list the  partitions .  It summaries the nodes by their current status so there may be more \nthat one line with  compute  in the partition column.  It lists the nodes in shorthand i.e. lims-hpc-[1,3-5] means lims-hpc-1, \nlims-hpc-3, lims-hpc-4, lims-hpc-5.  Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5     \n    $(document).ready(function(){\n        $(\"#showablelink64\").click(function(e){\n            e.preventDefault();\n            $(\"#showable64\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.2) What about an \u20188hour\u2019 job?      Hint   Use  sinfo  again but look at the 8hour rows     \n    $(document).ready(function(){\n        $(\"#showablelink66\").click(function(e){\n            e.preventDefault();\n            $(\"#showable66\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5, lims-hpc-m     \n    $(document).ready(function(){\n        $(\"#showablelink67\").click(function(e){\n            e.preventDefault();\n            $(\"#showable67\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Use the  cat  command to view the contents of  task01 ,  task02  and  task03  job script    3.3) How many  cpu cores  will each ask for?      Hint   Lookup the man page for  sbatch  command.   sbatch 's options match up with the  #SBATCH  comments at the top of each job \nscript.  Some will be affected by more than one option    More   Non-exclusive (shared) jobs :  It is  --cpus-per-task x --ntasks  but if  --ntasks  is not present it defaults to 1 so its  --cpus-per-task x 1  Exclusive jobs :  The  --nodes  options tells us how many nodes we ask for and the  --exclusive  option says give us all it has.  This\none is a bit tricky as we don't really know until it runs.     \n    $(document).ready(function(){\n        $(\"#showablelink70\").click(function(e){\n            e.preventDefault();\n            $(\"#showable70\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable70\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink70\").text(\"More\");\n            } else {\n                $(\"#showablelink70\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink69\").click(function(e){\n            e.preventDefault();\n            $(\"#showable69\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer :   task01:  1 cpu core  task02:  6 cpu cores  task03:  at least 1  as this has requested all cpu cores on the node its running on ( --exclusive ). \nHowever, since we know that all nodes on LIMS-HPC have 16, we know it will get 16.      \n    $(document).ready(function(){\n        $(\"#showablelink71\").click(function(e){\n            e.preventDefault();\n            $(\"#showable71\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.4) What about total memory?      Hint   Lookup the man page for  sbatch  command.   sbatch 's options match up with the  #SBATCH  comments at the top of each job \nscript.  Some will be affected by more than one option    More   The  --mem-per-cpu  OR  --mem  options are holding the answer to total memory.  For task01 and task02 the calculation is  --mem-per-cpu x --cpus-per-task x --ntasks  For task03, like with the cpus cores question, we get all the memory available on the node we get allocated     \n    $(document).ready(function(){\n        $(\"#showablelink74\").click(function(e){\n            e.preventDefault();\n            $(\"#showable74\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable74\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink74\").text(\"More\");\n            } else {\n                $(\"#showablelink74\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink73\").click(function(e){\n            e.preventDefault();\n            $(\"#showable73\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   The  --mem-per-cpu  OR  --mem  options are holding the answer to total memory.  For task01 and task02 the calculation is  --mem-per-cpu x --ntasks x --cpus-per-task  For task03, like with the cpus cores question, we get all the memory available on the node we get allocated  NOTE : it might be tempting to use the  --mem  option on non-exclusive (i.e.  --shared ) jobs \nhowever this will  NOT  work since the meaning of  --mem  is  \"go on a node with at least X MB of memory\" ; it does \nnot actually allocate any of it to you so your job will get terminated once it tries to use any memory.  Answer :   task01:  1024MB  (1GB) i.e. 1024 x 1 x 1  task02:  12288MB  (12GB) i.e. 2048 x 3 x 2  task03:  at least 1024MB  (1GB). The actual amount could be 128GB (nodes 2 to 5) or 256GB (node 1)      \n    $(document).ready(function(){\n        $(\"#showablelink75\").click(function(e){\n            e.preventDefault();\n            $(\"#showable75\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.5) How long can each run for?      Hint   Use the  man sbatch  command to look up the time specification.  If you search for  --time  it will describe the formats it uses (i.e. type  /--time  and press enter)     \n    $(document).ready(function(){\n        $(\"#showablelink77\").click(function(e){\n            e.preventDefault();\n            $(\"#showable77\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   The  --time  option is what tells slurm how long your job will run for.  Answer :   task01: requests  30:00 (30mins 0secs) , uses ~30secs  task02: requests  5:00 (5mins 0secs) , uses ~5secs  task03: requests  1:00 (1min 0secs) , uses ~30secs      \n    $(document).ready(function(){\n        $(\"#showablelink78\").click(function(e){\n            e.preventDefault();\n            $(\"#showable78\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.6) Is this maximum, minimum or both runtime?      Hint   Use the  man sbatch  command to look up the time specification.  If you search for  --time  it will describe the formats it uses (i.e. type  /--time  and press enter)     \n    $(document).ready(function(){\n        $(\"#showablelink80\").click(function(e){\n            e.preventDefault();\n            $(\"#showable80\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   This is a maximum time.  Your job may finish early, at which point it hands back the resources for the next job.  However if it \ntries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to \nwhat it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough \ntime.  If you think your job may not complete in time you can ask the system administrator of your HPC to add more time.     \n    $(document).ready(function(){\n        $(\"#showablelink81\").click(function(e){\n            e.preventDefault();\n            $(\"#showable81\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.7) Calculate the  --time  specification for the following runtimes:    1h30m: --time=  1m20s: --time=  1.5days: --time=  30m: --time=      \n    $(document).ready(function(){\n        $(\"#showablelink82\").click(function(e){\n            e.preventDefault();\n            $(\"#showable82\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   Use the  man sbatch  command to look up the time specification.  If you search for  --time  it will describe the formats it uses (i.e. type  /--time  and press enter)     \n    $(document).ready(function(){\n        $(\"#showablelink83\").click(function(e){\n            e.preventDefault();\n            $(\"#showable83\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer    1h30m: --time=01:30:00 (alternatively: 0-01:30)  1m20s: --time=01:20  1.5days: --time=1-12  30m: --time=30      \n    $(document).ready(function(){\n        $(\"#showablelink84\").click(function(e){\n            e.preventDefault();\n            $(\"#showable84\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.8) What do the following --time specifications mean?    --time=12-00:20  --time=45  --time=00:30      \n    $(document).ready(function(){\n        $(\"#showablelink85\").click(function(e){\n            e.preventDefault();\n            $(\"#showable85\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   Use the  man sbatch  command to look up the time specification.  If you search for  --time  it will describe the formats it uses (i.e. type  /--time  and press enter)     \n    $(document).ready(function(){\n        $(\"#showablelink86\").click(function(e){\n            e.preventDefault();\n            $(\"#showable86\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer    --time=12-00:20 12 days and 20 minutes  --time=45 45 minutes  --time=00:30 30 seconds      \n    $(document).ready(function(){\n        $(\"#showablelink87\").click(function(e){\n            e.preventDefault();\n            $(\"#showable87\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Now use sbatch to submit the  task01  job:     3.9) What job id was your job given?      Hint   Use the man page for the sbatch command.  The  Synopsis  at the top will give you an idea how to run it.     \n    $(document).ready(function(){\n        $(\"#showablelink89\").click(function(e){\n            e.preventDefault();\n            $(\"#showable89\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   $ sbatch task01 \nSubmitted batch job 9998  Answer : it's unique for each job; in the above example mine was  9998     \n    $(document).ready(function(){\n        $(\"#showablelink90\").click(function(e){\n            e.preventDefault();\n            $(\"#showable90\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.10) Which node did your job go on?      Hint   The  squeue  command shows you the currently running jobs.  If its been longer than 30 seconds since you submitted it you might have to resubmit it.     \n    $(document).ready(function(){\n        $(\"#showablelink92\").click(function(e){\n            e.preventDefault();\n            $(\"#showable92\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Use the  squeue  command to show all jobs.  Search for your  jobid  and look in the  NODELIST  column.  NOTE : if there are lots of jobs you can use  squeue -u YOUR_USERNAME  to only show your jobs, where \nYOUR_USERNAME is replaced with your actual username.  $ sbatch task01\nSubmitted batch job 9999\n$ squeue -u training01\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n  9999   compute   task01 training  R       0:05      1 lims-hpc-2  Answer : it's dependent on node availability at time; in the above example mine was  lims-hpc-2     \n    $(document).ready(function(){\n        $(\"#showablelink93\").click(function(e){\n            e.preventDefault();\n            $(\"#showable93\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Exercises"
        }, 
        {
            "location": "/tutorials/hpc/#advanced", 
            "text": "3.11) Make a copy of  task01  and call it  prime_numbers .  Make it load the training module and use the  prime  command calculate prime \nnumbers for 20 seconds.      Hint   You can find the  prime  command in the  training/1.0  module     \n    $(document).ready(function(){\n        $(\"#showablelink95\").click(function(e){\n            e.preventDefault();\n            $(\"#showable95\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   The key points to change in the task01 script are:   adding the  module load training/1.0  replacing the  sleep  (and  echo ) statements with a call to  prime 20 .   #!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1024\n#SBATCH --partition=training\n#SBATCH --time=30:00\n\nmodule load training/1.0\n\necho  Starting at: $(date) \nprime 20\necho  Finished at: $(date)   Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will \nensure that the same version is loaded next time you run this script which will mean you get the same results.  Date your work : It's also good practice to include the date command in the output so you have a permanent record \nof when this job was run.  If you have one before and after your main program you will get a record of how long it ran for as well.     \n    $(document).ready(function(){\n        $(\"#showablelink96\").click(function(e){\n            e.preventDefault();\n            $(\"#showable96\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.12) Submit the job.  What was the  largest  prime number it found in 20 seconds?      Hint   The output from the program will provide the results that we are after.  For HPC jobs this will be placed in the  SLURM output file ; this is called slurm-JOBID.out  where JOBID is replaced by the actual job id.     \n    $(document).ready(function(){\n        $(\"#showablelink98\").click(function(e){\n            e.preventDefault();\n            $(\"#showable98\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by \nthe amount of other jobs running on the HPC  $ sbatch prime_numbers\nSubmitted batch job 9304\n$ cat slurm-9304.out \nStarting at: Fri May  8 16:11:07 AEST 2015\n\nPrimes:        710119\nLast trial:    10733927\nLargest prime: 10733873\nRuntime:       20 seconds\nFinished at: Fri May  8 16:11:27 AEST 2015     \n    $(document).ready(function(){\n        $(\"#showablelink99\").click(function(e){\n            e.preventDefault();\n            $(\"#showable99\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.13) Modify your prime_numbers script to notify you via email when it starts and ends.  Submit it again    Did it start immediately or have some delay?  How long did it actually run for?      \n    $(document).ready(function(){\n        $(\"#showablelink100\").click(function(e){\n            e.preventDefault();\n            $(\"#showable100\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   There are two options that you will need to set.  See sbatch manpage for details.    More   Both start with  --mail     \n    $(document).ready(function(){\n        $(\"#showablelink102\").click(function(e){\n            e.preventDefault();\n            $(\"#showable102\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable102\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink102\").text(\"More\");\n            } else {\n                $(\"#showablelink102\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink101\").click(function(e){\n            e.preventDefault();\n            $(\"#showable101\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   #!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1024\n#SBATCH --partition=training\n#SBATCH --time=30:00\n#SBATCH --mail-user=name@email.address\n#SBATCH --mail-type=ALL\n\nmodule load training/1.0\n\necho  Starting at: $(date) \nprime 20\necho  Finished at: $(date)   Answers :   Did it start immediately or have some delay?  The  Queued time  value in the subject of start email will tell you how long it waited.  How long did it actually run for?  The  Run time  value in the subject of the end email will tell you how long it ran for which should \nbe ~20 seconds.      \n    $(document).ready(function(){\n        $(\"#showablelink103\").click(function(e){\n            e.preventDefault();\n            $(\"#showable103\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Advanced"
        }, 
        {
            "location": "/tutorials/hpc/#topic-4-job-monitoring", 
            "text": "It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based \non a number of factors (data set, number of CPU's, processing step etc.).  In this topic we will cover some of the tools that are available to you to  watch  what is happening so we can make better predictions\nin the future.", 
            "title": "Topic 4: Job Monitoring"
        }, 
        {
            "location": "/tutorials/hpc/#exercises_3", 
            "text": "4.1) What does the  top  command show?      Hint   When all else fails, try  man ; specifically, the description section     \n    $(document).ready(function(){\n        $(\"#showablelink105\").click(function(e){\n            e.preventDefault();\n            $(\"#showable105\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   $ man top\n...\nDESCRIPTION\n       The top program provides a dynamic real-time view of a running system.\n...  Answer : in lay-person terms  \"Continually updating CPU and Memory usage\"       \n    $(document).ready(function(){\n        $(\"#showablelink106\").click(function(e){\n            e.preventDefault();\n            $(\"#showable106\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Run the  top  command.  Above the black line it shows some  system-wide statistics  and below are statistics specific to a single \nprocess (a.k.a, tasks OR software applications).    4.2) How much total memory does this HPC (head-node) have?      Hint   This would be a system-wide statistic.     \n    $(document).ready(function(){\n        $(\"#showablelink108\").click(function(e){\n            e.preventDefault();\n            $(\"#showable108\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer : If you look at the first value on the  Mem  line (line 4) it will tell you the total memory on this computer (node).   LIMS-HPC : 132085396k or ~128 GigaBytes  MERRI : 49413840k or ~48 GigaBytes  BARCOO : 65942760k or ~64 GigaBytes   To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again.     \n    $(document).ready(function(){\n        $(\"#showablelink109\").click(function(e){\n            e.preventDefault();\n            $(\"#showable109\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.3) What is the current total CPU usage?      Hint   This might be easier to work out what is not used and subtract it from 100%    More   Idle  is another term for not used (or  id  for short)     \n    $(document).ready(function(){\n        $(\"#showablelink112\").click(function(e){\n            e.preventDefault();\n            $(\"#showable112\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable112\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink112\").text(\"More\");\n            } else {\n                $(\"#showablelink112\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink111\").click(function(e){\n            e.preventDefault();\n            $(\"#showable111\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer : If you subtract the  %id  value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage     \n    $(document).ready(function(){\n        $(\"#showablelink113\").click(function(e){\n            e.preventDefault();\n            $(\"#showable113\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.4) What column does it appear to be sorting the processes by? Is this  low-to-high  OR  high-to-low ?      Hint   Its not PID but from time to time it might be ordered sequentially.     \n    $(document).ready(function(){\n        $(\"#showablelink115\").click(function(e){\n            e.preventDefault();\n            $(\"#showable115\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer :  %CPU  which gives you an indication of how much CPU time each process uses     \n    $(document).ready(function(){\n        $(\"#showablelink116\").click(function(e){\n            e.preventDefault();\n            $(\"#showable116\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time.  NOTE: you may need to quit  top  (by pressing q) so you can compare before it updates.    4.5) Why might the numbers disagree?      Hint   It might have something to do with the total number of CPU Cores on the system.     \n    $(document).ready(function(){\n        $(\"#showablelink118\").click(function(e){\n            e.preventDefault();\n            $(\"#showable118\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer :  %CPU  column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top \nare exactly that, how much the entire system is utilised.  i.e. if you could see all processes in  top  (excluding round errors) \nthey would add up 100% x the number of cpu cores available; on LIMS-HPC this would be 0-1600% in the individual processes and 0-100% \non the system-wide section.     \n    $(document).ready(function(){\n        $(\"#showablelink119\").click(function(e){\n            e.preventDefault();\n            $(\"#showable119\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.6) What command-line flag instructs  top  to sort results by  %MEM ?   Can you think of a reason that this might be useful?     \n    $(document).ready(function(){\n        $(\"#showablelink120\").click(function(e){\n            e.preventDefault();\n            $(\"#showable120\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   Use the  top  manpage.    More   \"m is for memory!\"     \n    $(document).ready(function(){\n        $(\"#showablelink122\").click(function(e){\n            e.preventDefault();\n            $(\"#showable122\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable122\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink122\").text(\"More\");\n            } else {\n                $(\"#showablelink122\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink121\").click(function(e){\n            e.preventDefault();\n            $(\"#showable121\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer :  top -m  will cause  top  to sort the processes by memory usage.  Can you think of a reason that this might be useful?  Your program might be using a lot of memory and you want to know how much, by sorting by memory will cause your program to stay at the top.     \n    $(document).ready(function(){\n        $(\"#showablelink123\").click(function(e){\n            e.preventDefault();\n            $(\"#showable123\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.7) Run  \"top -c\" .  What does it do?  How might this be helpful?      Hint   Use the  top  manpage.    More   \"c is for complete!\"  \"c is also for command!\"  which is another name for program     \n    $(document).ready(function(){\n        $(\"#showablelink126\").click(function(e){\n            e.preventDefault();\n            $(\"#showable126\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable126\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink126\").text(\"More\");\n            } else {\n                $(\"#showablelink126\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink125\").click(function(e){\n            e.preventDefault();\n            $(\"#showable125\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   What does it do? \nIt changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options.  How might this be helpful? \nSometimes you might be running a lot of commands with the same name that only differ by the command-line options.  In this case it is hard \nto tell which ones are still running unless you use the  -c  flag to show the complete command.  NOTE : \nIf  top  is running you can press the  c  key to toggle show/hide complete command     \n    $(document).ready(function(){\n        $(\"#showablelink127\").click(function(e){\n            e.preventDefault();\n            $(\"#showable127\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.8) How can you get  top  to only show your processes?  Why might this be useful?      Hint   Use the  top  manpage.    More   \"u is for user[name]!\"     \n    $(document).ready(function(){\n        $(\"#showablelink130\").click(function(e){\n            e.preventDefault();\n            $(\"#showable130\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable130\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink130\").text(\"More\");\n            } else {\n                $(\"#showablelink130\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink129\").click(function(e){\n            e.preventDefault();\n            $(\"#showable129\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   How can you get  top  to only show your processes?  Answer 1 :  top -u YOURUSERNAME  Answer 2 : while running  top  press the  u  key, type YOURUSERNAME and press   key   Why might this be useful? \nWhen you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard\nto quickly identify yours.     \n    $(document).ready(function(){\n        $(\"#showablelink131\").click(function(e){\n            e.preventDefault();\n            $(\"#showable131\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Exercises"
        }, 
        {
            "location": "/tutorials/hpc/#lims-hpc-specific", 
            "text": "LIMS-HPC has an extra monitoring and graphing tool called Munin.  Open the munin webpage and have a look at the graphs  Munin :  http://munin-lims.latrobe.edu.au/lims-hpc.html    4.9) What are the graphs showing?      Hint   Take a look at the title on the graphs.  Then the style of graphs.     \n    $(document).ready(function(){\n        $(\"#showablelink133\").click(function(e){\n            e.preventDefault();\n            $(\"#showable133\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   Answer : CPU usage (stacked by type of usage)     \n    $(document).ready(function(){\n        $(\"#showablelink134\").click(function(e){\n            e.preventDefault();\n            $(\"#showable134\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.10) How much is the Compute Node 5 being used currently?      Show  Answer   You can either:   estimate this off the right most position on the graph (everything except mid-yellow is the CPU doing something) or   look at the  cur  value for  idle  and subtract it from 1600 (the maximum value for a 16 core server)      \n    $(document).ready(function(){\n        $(\"#showablelink136\").click(function(e){\n            e.preventDefault();\n            $(\"#showable136\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.11) And at midday yesterday?      Hint   Its easiest to think in reverse (i.e. What is not being used?)     \n    $(document).ready(function(){\n        $(\"#showablelink138\").click(function(e){\n            e.preventDefault();\n            $(\"#showable138\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   You have to estimate system idle at the point on the graph indicating 12:00 (yesterday).     \n    $(document).ready(function(){\n        $(\"#showablelink139\").click(function(e){\n            e.preventDefault();\n            $(\"#showable139\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "LIMS-HPC Specific"
        }, 
        {
            "location": "/tutorials/hpc/#topic-5-all-together", 
            "text": "This topic will allow you to put all the skills that you learnt in this workshop to the test.  You might need to refer back to\nthe earlier topics if you have forgotten how to do these tasks.  Overview :   Write jobscript  Load/use software module  Submit job  Monitor job    NOTE (for later) : to complete this topic from your regular LIMS-HPC account you will need to:  first setup node logins.  This has already been done for your *training* account so no need to do this today LIMS-HPC Node Login Setup   use the *compute* partition instead of *training*", 
            "title": "Topic 5: All Together"
        }, 
        {
            "location": "/tutorials/hpc/#task-1-write-a-job-script", 
            "text": "Write a job script that requests the following resources:   Filename : monINITIALS.slurm  where INITIALS is replaced with your initials.  e.g. for me it would be monAR.slurm    Tasks : 1   CPUs : 1  Partition : training   Time : 5 mins   Memory : 1 GB (remember to specify it in MB)", 
            "title": "Task 1: Write a job script"
        }, 
        {
            "location": "/tutorials/hpc/#task-2-loaduse-software-module", 
            "text": "Edit your job script so that it:    Loads the training module  Runs the  fakejob  command with your name as the first parameter    NOTE : remember good practice here and add the date commands to print the date/time in your output.  You can copy them from the *task01* script.", 
            "title": "Task 2: Load/use software module"
        }, 
        {
            "location": "/tutorials/hpc/#task-3-submit-job", 
            "text": "Use  sbatch  to submit the job to the HPC.  Note down the job id it was given (for later).  Use squeue (or qs) to check that is started ok.  When it starts check which compute node it is running on (for the next task).", 
            "title": "Task 3: Submit job"
        }, 
        {
            "location": "/tutorials/hpc/#task-4-monitor-the-job", 
            "text": "Use the  top  command to check how much CPU and Memory the job is using.  Given that SLURM is running the job on your behalf on one of the compute \nnodes,  top  wont be able to see the job.  To be able to use top, you will first need to login to the compute node the is running your job.  To login:  $ ssh lims-hpc-X  Where X is the actual node number you were allocated (See task 3.4).  You are now connected from your computer to lims-hpc-m which is connected to lims-hpc-X.  +---------------+            +------------+            +------------+\n| YOUR COMPUTER | -- SSH --  | LIMS-HPC-M | -- SSH --  | LIMS-HPC-X |\n+---------------+            +------------+            +------------+  You can tell which node you are on by the text in the prompt  [10:00:06] USERNAME@lims-hpc-m ~ $ \n\nChanges to:\n\n[10:06:05] USERNAME@lims-hpc-1 ~ $  Once logged in to the relevent compute node you can run  top  to view you job.  Remember the  u  and  c  options we learnt earlier; they will be helpful \nhere when everyone is running the same jobs.     How does the CPU and Memory usage change over time?      Hint   It should vary (within the limits you set in the job script)     \n    $(document).ready(function(){\n        $(\"#showablelink141\").click(function(e){\n            e.preventDefault();\n            $(\"#showable141\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Show  Answer   The  fakejob  program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory (on lims-hpc-[m,2-5] this will equate to 0.4 to 0.8%)     \n    $(document).ready(function(){\n        $(\"#showablelink142\").click(function(e){\n            e.preventDefault();\n            $(\"#showable142\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Task 4: Monitor the job"
        }, 
        {
            "location": "/tutorials/hpc/#finished", 
            "text": "Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it \nwas a lot to take in.  From here you should be confortable to begin submitting real jobs to the HPC (in your real account, \nnot the training one).  You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this \nWorkshop for later reference.  Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it\nback to the Workshop facilitators.", 
            "title": "Finished"
        }, 
        {
            "location": "/tutorials/unix/", 
            "text": "em {font-style: normal; font-family: courier new;}\n\n\n\nIntroduction to Unix\n\n\nA hands-on-workshop covering the basics of the Unix/Linux command line interface\n\n\nHow to use this workshop\n\n\nThe workshop is broken up into a number of \nTopics\n each focusing on a particular aspect of Unix.  You should take a short break between \neach to refresh and relax before tackling the next.\n\n\nTopic\ns may start with some background followed by a number of \nexercises\n.  Each \nexercise\n begins with a \nquestion\n, then \nsometimes a \nhint\n (or two) and finishes with the suggested \nanswer\n.\n\n\nQuestion\n\n\nAn example question looks like:\n\n\n\n\n\n\nWhat is the Answer to Life?\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\nHint\n\n\nDepending on how much of a challenge you like, you may choose to use hints.  Even if you work out the answer without hints, its a good \nidea to read the hints afterwards because they contain extra information that is good to know.\n\n\nNote: \nhint\ns may be staged, that is, there may be a \nmore\n section within a hint for further hints\n\n\n\n\n\n\nHint\n \n- click here to reveal hint\n\n\n\n\nWhat is the answer to everything?\n\n\nAs featured in \"The Hitchhiker's Guide to the Galaxy\"\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nIt is probably a two digit number\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable2\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink2\").text(\"More\");\n            } else {\n                $(\"#showablelink2\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nAnswer\n\n\nOnce you have worked out the answer to the question expand the Answer section to check if you got it correct.\n\n\n\n\n\n\nAnswer\n \n- click here to reveal answer\n\n\n\n\nAnswer\n: 42\n\n\nRef: \nNumber 42 (Wikipedia)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nUsage Style\n\n\nThis workshop attempts to cater for two usage styles:\n\n\n\n\nProblem solver\n: for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional):\n\n\nAttempt to answer the question by yourself.\n\n\nUse hints when you get stuck.\n\n\nOnce solved, reveal the answer and read through our suggested solution.\n\n\nIts a good idea to read the hints and answer description as they often contain extra useful information.\n\n\n\n\n\n\nBy example\n:  for those who learn by following examples:  \nExpand\n all sections\n\n\nExpand the Answer section at the start of each question and follow along with the commands that are shown and check you get the\n  same (or similar) answers.\n\n\nIts a good idea to read the hints and answer description as they often contain extra useful information.\n\n\n\n\n\n\n\n\nTopic 1: Remote log in\n\n\nIn this topic we will learn how to connect to a \nUnix\n computer via a method called \nSSH\n and run a few basic commands.\n\n\nConnecting to a Unix computer\n\n\nTo begin this workshop you will need to connect to an HPC.  Today we will use the LIMS-HPC.  The computer called \n\nlims-hpc-m\n (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks.\n\n\nServer details\n:\n\n\n\n\nhost\n: lims-hpc-m.latrobe.edu.au\n\n\nport\n: 6022 \n\n\nusername\n: trainingXX (where XX is a two digit number, provided at workshop)\n\n\npassword\n: (provided at workshop) \n\n\n\n\n\n\n\n\nMac OS X / Linux\n\n\n\n\nBoth Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line.  To use OpenSSH you must \nfirst start a terminal program on your computer.  On OS X the standard terminal is called Terminal, and it is installed by default. \nOn Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good \ndefault).  When you've started the terminal you should see a command prompt.  To log into LIMS-HPC, for example, type this command at \nthe prompt and press return (where the word username is replaced with your LIMS-HPC username):\n\n\nssh -p 6022 username@lims-hpc-m.latrobe.edu.au\n\n\n\n\nThe same procedure works for any other machine where you have an account except most other HPCs will not need the \n-p 6022\n \n(which is telling ssh to connect on a non-standard port number).\n\n\nYou may be presented with a message along the lines of:\n\n\nThe authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be  established.\n...\nAre you sure you want to continue connecting (yes/no)?\n\n\n\n\nAlthough you should never ignore a warning, this particular one is nothing to be concerned about; type \nyes\n and then \npress enter\n. \nIf all goes well you will be asked to enter your password.  Assuming you type the correct username and password the system should \nthen display a welcome message, and then present you with a Unix prompt.  If you get this far then you are ready to start entering \nUnix commands and thus begin using the remote computer.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nWindows\n\n\n\n\nOn Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client.  PuTTY (putty.exe) can be downloaded \nfrom this web page:\n\n\nhttp://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\n\n\nDocumentation for using PuTTY is here:\n\n\nhttp://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html\n\n\nWhen you start PuTTY you should see a window which looks something like this:\n\n\n\n\nTo connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and \n6022\n in the port, \nthen click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the \nsame as they are in the picture above).\n\n\nIn some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines \nof \n\"The server's host key is not cached in the registry\"\n. This is nothing to worry about, and you should agree to continue (by \nclicking on Yes). You usually see this message the first time you try to connect to a particular remote computer.\n\n\nIf all goes well, a terminal window will open, showing a prompt with the text \n\"login as:\"\n. An example terminal window is shown \nbelow. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your \npassword. Assuming you type the correct username and password the system should then display a welcome message, and then \npresent you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using \nthe remote computer.\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nNote\n: for security reasons ssh will not display any characters when you enter your password. This \ncan be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be \nalarmed; type your password in and press return at the end.\n\n\nLIMS-HPC is a high performance computer for La Trobe Users.  Logging in connects your local computer \n(e.g. laptop) to LIMS-HPC, and allows you to type commands into the Unix prompt which are run on \nthe HPC, and have the results displayed on your local screen.\n\n\nYou will be allocated a training account on LIMS-HPC for the duration of the workshop. Your \nusername and password will be supplied at the start of the workshop.\n\n\nLog out of LIMS-HPC, and log back in again (to make sure you can repeat the process).\n\n\nAll the remaining parts assume that you are logged into LIMS-HPC over ssh.\n\n\nExercises\n\n\n\n\n\n\n1.1) When you\u2019ve logged into LIMS-HPC run the following commands and see what they do:\n\n\n\n\nwho\nwhoami\ndate\ncal\nhostname\n/home/group/common/training/Intro_to_Unix/hi\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nwho\n: displays a list of the users who are currently using this Unix computer.\n\n\nwhoami\n: displays your username (i.e. they person currently logged in).\n\n\ndate\n: displays the current date and time.\n\n\ncal\n: displays a calendar on the terminal.  It can be configured to display more than just \nthe current month.\n\n\nhostname\n: displays the name of the computer we are logged in to.\n\n\n/home/group/common/training/Intro_to_Unix/hi\n: displays the text \"Hello World\"\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink7\").click(function(e){\n            e.preventDefault();\n            $(\"#showable7\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 2: Exploring your home directory\n\n\nIn this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands. \n\n\nDuration\n: 20 minutes. \n\n\nRelevant commands\n: \nls\n, \npwd\n, \necho\n, \nman\n\n\nYour home directory contains your own private working space.  Your \ncurrent working directory\n is automatically set \nto your \nhome\n directory when you log into a Unix computer.\n\n\n\n\n\n\n2.1) Use the \nls\n command to list the files in your \nhome\n directory.  How many files are there?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nLiterally, type \nls\n and press the \nENTER\n key.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ ls\nexp01  file01  muscle.fq\n\n\n\n\nWhen running the \nls\n command with no options it will list files in your current working directory.  The place \nwhere you start when you first login is your \nHOME\n directory.\n\n\nAnswer\n: 3 (exp01, file01 and muscle.fq)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink10\").click(function(e){\n            e.preventDefault();\n            $(\"#showable10\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nThe above answer is not quite correct.  There are a number of \nhidden\n files in your home directory as well.\n\n\n\n\n\n\n2.2) What \nflag\n might you use to display \nall\n files with the \nls\n command?  How many files are really there?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTake the \nall\n quite literally.\n\n\n\n\n\n\nMore\n\n\n\n\nType \nls --all\n and press the \nENTER\n key.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable13\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink13\").text(\"More\");\n            } else {\n                $(\"#showablelink13\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAnswer 1\n: \n--all\n (or \n-a\n) flag\n\n\nNow you should see several files in your home directory whose names all begin with a dot. All these files are \ncreated automatically for your user account. They are mostly configuration options for various programs including \nthe shell. It is safe to ignore them for the moment.\n\n\n$ ls --all\n.              .bash_logout    exp01    .lesshst\n..             .bash_profile   file01   muscle.fq\n.bash_history  .bashrc         .kshrc   .viminfo\n\n\n\n\nThere are two trick files here; namely \n.\n and \n..\n which are not real files but instead, shortcuts.  \n.\n is a shortcut\nfor the current directory and \n..\n a shortcut for the directory above the current one.\n\n\nAnswer 2\n: 10 files (don't count \n.\n and \n..\n)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink14\").click(function(e){\n            e.preventDefault();\n            $(\"#showable14\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.3) What is the full path name of your \nhome\n directory?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember your \nCurrent Working Directory\n start's in your \nhome\n directory (and the hint from the slides).\n\n\n\n\n\n\nMore\n\n\n\n\nTry a shortened version of \nprint working directory\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink17\").click(function(e){\n            e.preventDefault();\n            $(\"#showable17\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable17\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink17\").text(\"More\");\n            } else {\n                $(\"#showablelink17\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou can find out the full path name of the current working directory with the \npwd\n command. Your home directory \nwill look something like this:\n\n\n$ pwd\n/home/trainingXY\n\n\n\n\nAnswer\n: \n/home/trainingXY\n\n\nwhere \nXY\n is replaced by some 2 digit sequence.\n\n\nAlternate method\n:\nYou can also find out the name of your home directory by printing the value of the \n$HOME\n shell variable:\n\n\necho $HOME\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.4) Run \nls\n using the long flag (\n-l\n), how did the output change?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRun \nls -l\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAnswer\n: it changed the output to place 1 file/directory per line.  It also added some extra information\nabout each.\n\n\n$ ls -l\ntotal 32\ndrwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01\n-rw-r----- 1 training01 training   97 Jun 14 11:28 file01\n-rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq\n\n\n\n\nDetails\n:\n\n\ndrwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01\n\\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/\npermission |  username   group   size    date       name\n       /---^---\\\n       linkcount\n\n\n\n\nWhere:\n\n\n\n\npermissions\n: 4 parts, file type, user perms, group perms and other perms\n\n\nfiletype\n: 1 character, \nd\n = directory and \n-\n regular file\n\n\nuser\n permissions: 3 characters, \nr\n = read, \nw\n = write, \nx\n = execute and \n-\n no permission\n\n\ngroup\n permissions: same as user except for users within the owner group\n\n\nother\n permissions: same as user except for users that are not in either user \nor\n \ngroup\n\n\n\n\n\n\nusername\n: the user who \nowns\n this file/directory\n\n\ngroup\n: the group name who \nowns\n this file/directory\n\n\nsize\n: the number of bytes this file/directory takes to store on disk\n\n\ndate\n: the date and time when this file/directory was \nlast edited\n\n\nname\n: name of the file\n\n\nlinkcount\n: technical detail which represents the number of links this file has in the file system (safe to ignore)\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.5) What type of file is \nexp01\n and \nmuscle.fq\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheck the output from the \nls -l\n.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink23\").click(function(e){\n            e.preventDefault();\n            $(\"#showable23\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAnswer\n:\n\n\n\n\nexp01\n: Directory (given the 'd' as the first letter of its permissions)\n\n\nmuscle.fq\n: Regular File (given the '-')\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.6) Who has permission to \nread\n, \nwrite\n and \nexecute\n your \nhome\n directory?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nYou can also give \nls\n a filename as the first option.\n\n\n\n\n\n\nMore\n\n\n\n\nls -l\n will show you the contents of the \nCWD\n; how might you see the contents of the \nparent\n directory? (remember\nthe slides)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink27\").click(function(e){\n            e.preventDefault();\n            $(\"#showable27\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable27\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink27\").text(\"More\");\n            } else {\n                $(\"#showablelink27\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nIf you pass the \n-l\n flag to ls it will display a \"long\" listing of file information including file permissions.\n\n\nThere are various ways you could find out the permissions on your home directory.\n\n\nMethod 1\n: given we know the \nCWD\n is our home directory.\n\n\n$ ls -l ..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...\n\n\n\n\nThe \n..\n refers to the parent directory.\n\n\nMethod 2\n: using $HOME.  This works no matter what our \nCWD\n is set to. \n\n\nYou could list the permissions of all files and directories in the parent directory of your home:\n\n\n$ ls -l $HOME/..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...\n\n\n\n\nIn this case we use the shell variable to refer to our home directory.\n\n\nMethod 3\n: using \n~\n (tilde) shortcut\n\n\nYou may also refer to your home directory using the \n~\n (tilde) character:\n\n\n$ ls -l ~/..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...\n\n\n\n\nAll 3 of the methods above mean the same thing.\n\n\nYou will see a list of files and directories in the parent directory of your home directory. One of them will \nbe the name of your home directory, something like \ntrainingXY\n.  Where \nXY\n is replaced by a two digit string\n\n\nAltername\n: using the \n-a\n flag and looking at the \n.\n (dot) special file.\n\n\n$ ls -la\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 .\n...\n\n\n\n\nAnswer\n: \ndrwxr-x---\n\n\n\n\nYou\n: read (see filenames), write (add, delete files), execute (change your CWD to this directory).\n\n\nTraining users\n: read, execute\n\n\nEveryone else\n: No access\n\n\n\n\nDiscussion on Permissions\n:\n\n\nThe permission string is \n\"drwxr-x---\"\n. The \nd\n means it is a directory. The \nrwx\n means that the owner of the directory \n(your user account) can \nread\n, \nwrite\n and \nexecute\n the directory. Execute permissions on a directory means that you \ncan \ncd\n into the directory. The \nr-x\n means that anyone in the same user group as \ntraining\n can read or execute the \ndirectory. The \n---\n means that nobody else (other users on the system) can do anything with the directory.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nman\n is for manual\n: and it will be your best friend!\n\n\nManual pages include a lot of detail about a command and its available flags/options.  It should be your first (or second) \nport of call when you are trying to work out what a command or option does.\n\n\nYou can scroll \nup\n and \ndown\n in the man page using the \narrow\n keys.\n\n\nYou can search in the man page using the forward \nslash followed by the search text followed by the \nENTER\n key. e.g. \ntype \n/hello\n and press \nENTER\n to search for the word \nhello\n.  Press \nn\n key to find next \noccurance of \nhello\n etc.\n\n\nYou can \nquit\n the man page by pressing \nq\n.\n\n\n\n\n\n\n\n\n\n\n\n2.7) Use the \nman\n command to find out what the \n-h\n flag does for \nls\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nGive \nls\n as an option to \nman\n command.\n\n\n\n\n\n\nMore\n\n\n\n\nman ls\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable31\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink31\").text(\"More\");\n            } else {\n                $(\"#showablelink31\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink30\").click(function(e){\n            e.preventDefault();\n            $(\"#showable30\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the following command to view the \nman\n page for \nls\n:\n\n\n$ man ls\n\n\n\n\nAnswer\n: You should discover that the \n-h\n option prints file sizes in human readable format\n\n\n-h, --human-readable\n              with -l, print sizes in human readable format (e.g., 1K 234M 2G)\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n2.8) Use the \n-h\n, how did the output change of \nmuscle.fq\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nDon't forget the \n-l\n option too.\n\n\n\n\n\n\nMore\n\n\n\n\nRun \nls -lh\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink35\").click(function(e){\n            e.preventDefault();\n            $(\"#showable35\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable35\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink35\").text(\"More\");\n            } else {\n                $(\"#showablelink35\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink34\").click(function(e){\n            e.preventDefault();\n            $(\"#showable34\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ ls -lh\n...\n-rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq\n\n\n\n\nAnswer\n: it changed the output so the \nfilesize\n of \nmuscle.fq\n is now \n2.5K\n instead of \n2461\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink36\").click(function(e){\n            e.preventDefault();\n            $(\"#showable36\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 3: Exploring the file system\n\n\nIn this topic we will learn how to move around the filesystem and see what is there.\n\n\nDuration\n: 30 minutes. \n\n\nRelevant commands\n: \npwd\n, \ncd\n, \nls\n, \nfile\n\n\n\n\n\n\n3.1) Print the value of your current working directory.\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nThe \npwd\n command prints the value of your current working directory.\n\n\n$ pwd\n/home/training01\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink38\").click(function(e){\n            e.preventDefault();\n            $(\"#showable38\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.2) List the contents of the root directory, called '\n/\n' (forward \nslash).\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nls\n expects a single option which is the directory to change too.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink40\").click(function(e){\n            e.preventDefault();\n            $(\"#showable40\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ ls /\napplications-merged  etc         media    root     tmp\nbin                  home        mnt      sbin     usr\nboot                 lib         oldhome  selinux  var\ndata                 lib64       opt      srv\ndev                  lost+found  proc     sys\n\n\n\n\nHere we see that \nls\n can take a filepath as its argument, which allows you to list the contents of directories \nother than your current working directory.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink41\").click(function(e){\n            e.preventDefault();\n            $(\"#showable41\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.3) Use the \ncd\n command to change your working directory to the root directory.  Did your prompt \nchange?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\ncd\n expects a single option which is the directory to change to\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink43\").click(function(e){\n            e.preventDefault();\n            $(\"#showable43\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe \ncd\n command changes the value of your current working directory. To change to the root directory use the \nfollowing command:\n\n\n$ cd /\n\n\n\n\nAnswer\n: Yes, it now says the CWD is \n/\n instead of \n~\n.\n\n\nSome people imagine that changing the working directory is akin to moving your focus within the file system. \nSo people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory.\n\n\nThe root directory is special in Unix. It is the topmost directory in the whole file system.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink44\").click(function(e){\n            e.preventDefault();\n            $(\"#showable44\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nOutput on ERROR only\n: Many Unix commands will not produce any output if everything went well; \ncd\n is one\nsuch command.  However, it will get grumpy if something went wrong by way of an error message on-screen.\n\n\n\n\n\n\n\n\n\n\n3.4) List the contents of the CWD and verify it matches the list in 3.2\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nls\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink46\").click(function(e){\n            e.preventDefault();\n            $(\"#showable46\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAssuming you have changed to the root directory then this can be achieved with \nls\n, or \nls -a\n (for all files) or \n\nls -la\n for a long listing of all files.\n\n\nIf you are not currently in the root directory then you can list its contents by passing it as an argument to ls:\n\n\n$ ls\napplications-merged  etc         media    root     tmp\nbin                  home        mnt      sbin     usr\nboot                 lib         oldhome  selinux  var\ndata                 lib64       opt      srv\ndev                  lost+found  proc     sys\n\n\n\n\nAnswer\n: Yes, we got the same output as exercise 3.2\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink47\").click(function(e){\n            e.preventDefault();\n            $(\"#showable47\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.5) Change your current working directory back to your home directory. What is the simplest Unix command that \nwill get you back to your home directory from anywhere else in the file system?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe answer to exercise 2.6 might give some hints on how to get back to the home directory\n\n\n\n\n\n\nMore\n\n\n\n\n$HOME\n, \n~\n, \n/home/trainingXY\n are all methods to name your home directory.  Yet there is a simpler method; the answer\nis buried in \nman cd\n however \ncd\n doesn't its own manpage so you will need to search for it.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink50\").click(function(e){\n            e.preventDefault();\n            $(\"#showable50\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable50\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink50\").text(\"More\");\n            } else {\n                $(\"#showablelink50\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink49\").click(function(e){\n            e.preventDefault();\n            $(\"#showable49\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \ncd\n command to change your working directory to your home directory. There are a number of ways to refer \nto your home directory:\n\n\ncd $HOME\n\n\n\n\nis equivalent to:\n\n\ncd ~\n\n\n\n\nThe simplest way to change your current working directory to your home directory is to run the \ncd\n command with \nno arguments:\n\n\nAnswer\n: the simplest for is cd with NO options.\n\n\ncd\n\n\n\n\nThis is a special-case behaviour which is built into \ncd\n for convenience.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink51\").click(function(e){\n            e.preventDefault();\n            $(\"#showable51\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.6) Change your working directory to \n/home/group/common/training/Intro_to_Unix/\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\ncd /home/group/common/training/Intro_to_Unix/\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink53\").click(function(e){\n            e.preventDefault();\n            $(\"#showable53\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.7) List the contents of that directory. How many files does it contain?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nls\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink55\").click(function(e){\n            e.preventDefault();\n            $(\"#showable55\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou can do this with \nls\n\n\n$ ls\nexpectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy\n\n\n\n\nAnswer\n: 7 files (expectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink56\").click(function(e){\n            e.preventDefault();\n            $(\"#showable56\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.8) What kind of \nfile\n is \n/home/group/common/training/Intro_to_Unix/sleepy\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTake the word \nfile\n quite literally.\n\n\n\n\n\n\nMore\n\n\n\n\nfile sleepy\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink59\").click(function(e){\n            e.preventDefault();\n            $(\"#showable59\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable59\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink59\").text(\"More\");\n            } else {\n                $(\"#showablelink59\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink58\").click(function(e){\n            e.preventDefault();\n            $(\"#showable58\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \nfile\n command to get extra information about the contents of a file:\n\n\nAssuming your current working directory is \n/home/group/common/training/Intro_to_Unix/\n\n\n$ file sleepy\nBourne-Again shell script text executable\n\n\n\n\nOtherwise specify the full path of sleepy:\n\n\n$ file /home/group/common/training/Intro_to_Unix/sleepy\nBourne-Again shell script text executable\n\n\n\n\nAnswer\n: Bourne-Again shell script text executable\n\n\nThe \"Bourne-Again shell\" is more commonly known as BASH. The \nfile\n command is telling us that sleepy \nis (probably) a shell script written in the language of BASH.\n\n\nThe file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works \nthen read the Unix manual page like so:\n\n\nman file\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink60\").click(function(e){\n            e.preventDefault();\n            $(\"#showable60\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.9) What kind of \nfile\n is \n/home/group/common/training/Intro_to_Unix/hi\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTake the word \nfile\n quite literally.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink62\").click(function(e){\n            e.preventDefault();\n            $(\"#showable62\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the file command again. If you are in the same directory as \nhi\n then:\n\n\n$ file hi\nELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux \n2.6.9, not stripped\n\n\n\n\nAnswer\n: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux \n\n\nThis rather complicated output is roughly saying that the file called \nhi\n contains a binary executable \nprogram (raw instructions that the computer can execute directly).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink63\").click(function(e){\n            e.preventDefault();\n            $(\"#showable63\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.10) What are the file permissions of \n/home/group/common/training/Intro_to_Unix/sleepy\n? \nWhat do they mean?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember the \nls\n command, and don't forget the \n-l\n flag\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink65\").click(function(e){\n            e.preventDefault();\n            $(\"#showable65\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou can find the permissions of \nsleepy\n using the \nls\n command with the \n-l\n flag. If you are in the same \ndirectory as \nsleepy\n then:\n\n\n$ ls -l sleepy\n-rw-r--r-- 1 arobinson common 183 Feb  9 16:36 sleepy\n\n\n\n\nAnswer\n: We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common \nuser group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is \nreadable to everyone, and writeable only to training01.  The digit '1' between the file permission string and \nthe owner indicates that there is one link to the file. The Unix file system allows files to be referred to \nby multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For \nfuture reference: links are created with the \nln\n command.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink66\").click(function(e){\n            e.preventDefault();\n            $(\"#showable66\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n3.11) Change your working directory back to your home directory ready for the next topic.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\ncd\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink68\").click(function(e){\n            e.preventDefault();\n            $(\"#showable68\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou should know how to do this with the cd command:\n\n\ncd\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink69\").click(function(e){\n            e.preventDefault();\n            $(\"#showable69\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 4: Working with files and directories\n\n\nIn this topic we will start to read, create, edit and delete files and directories.\n\n\nDuration\n: 50 minutes.  \n\n\nRelevant commands\n: \nmkdir\n, \ncp\n, \nls\n, \ndiff\n, \nwc\n, \nnano\n, \nmv\n, \nrm\n, \nrmdir\n, \nhead\n, \ntail\n, \ngrep\n, \ngzip\n, \ngunzip\n\n\n\n\nHint\n: Look at the commands above; you will need them roughly in order for this topic.  Use the \nman\n\ncommand find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections.\n\n\n\n\n\n\n\n\n\n\n4.1) In your home directory make a sub-directory called test.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nYou are trying to \nmake a directory\n, which of the above commands looks like a shortened version of this?\n\n\n\n\n\n\nMore\n\n\n\n\nmkdir\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink72\").click(function(e){\n            e.preventDefault();\n            $(\"#showable72\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable72\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink72\").text(\"More\");\n            } else {\n                $(\"#showablelink72\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink71\").click(function(e){\n            e.preventDefault();\n            $(\"#showable71\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nMake sure you are in your home directory first. If not \ncd\n to your home directory.\n\n\nUse the \nmkdir\n command to make new directories:\n\n\n$ mkdir test\n\n\n\n\nUse the \nls\n command to check that the new directory was created.\n\n\n$ ls\nexp01  file01  muscle.fq  test\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink73\").click(function(e){\n            e.preventDefault();\n            $(\"#showable73\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.2) Copy all the files from \n/home/group/common/training/Intro_to_Unix/\n into the newly created \ntest directory.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nYou are trying to \ncopy\n, which of the above commands looks like a shortened version of this?\n\n\n\n\n\n\nMore\n\n\n\n\n$ man cp\n...\nSYNOPSIS\n       cp [OPTION]... [-T] SOURCE DEST\n...\nDESCRIPTION\n       Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n\n\n\n\nwhich means \ncp\n expects zero or more flags, a SOURCE file followed by a DEST file or directory \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink76\").click(function(e){\n            e.preventDefault();\n            $(\"#showable76\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable76\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink76\").text(\"More\");\n            } else {\n                $(\"#showablelink76\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink75\").click(function(e){\n            e.preventDefault();\n            $(\"#showable75\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \ncp\n command to copy files. \n\n\nWildcards\n: You could copy them one-by-one, but that would be tedious, so use \nthe \n*\n wildcard to specify that you want to copy all the files.\n\n\n\n\nThere are a number of ways you could do this depending on how you specify the source and destination \npaths to \ncp\n. You only need to perform one of these ways, but we show multiple ones for your reference.\n\n\nAnswer 1\n: From your home directory:\n\n\n$ cp /home/group/common/training/Intro_to_Unix/* test\n\n\n\n\nAnswer 2\n: Change to the test directory and then copy (assuming you started in your home directory):\n\n\n$ cd test\n$ cp /home/group/common/training/Intro_to_Unix/* .\n\n\n\n\nIn the example above the '\n.\n' (dot) character refers to the current working directory. It should be \nthe test subdirectory of your home directory.\n\n\nAnswer 3\n: Change to the /home/group/common/training/Intro_to_Unix/ directory and then copy:\n\n\ncd /home/group/common/training/Intro_to_Unix/\ncp * ~/test\n\n\n\n\nRemember that ~ is a shortcut reference to your home directory.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink77\").click(function(e){\n            e.preventDefault();\n            $(\"#showable77\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nNote\n: This exercise assumes that the copy command from the previous exercise was successful. \n\n\n\n\n\n\n4.3) Check that the file size of \nexpectations.txt\n is the same in both the directory that you copied \nit from and the directory that you copied it to.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember \nls\n can show you the file size (with one of its flags)\n\n\n\n\n\n\nMore\n\n\n\n\nls -l\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink80\").click(function(e){\n            e.preventDefault();\n            $(\"#showable80\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable80\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink80\").text(\"More\");\n            } else {\n                $(\"#showablelink80\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink79\").click(function(e){\n            e.preventDefault();\n            $(\"#showable79\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse \nls -l\n to check the size of files.\n\n\nYou could do this in many ways depending on the value of your working directory. We just show one possible \nway for each file:\n\n\n$ ls -l /home/group/common/training/Intro_to_Unix/expectations.txt\n\n$ ls -l ~/test/expectations.txt\n\n\n\n\nFrom the output of the above commands you should be able to see the size of each file and check that they \nare the same. \n\n\nAnswer\n: They should each be \n1033773\n bytes\n\n\nAlternate\n: Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is \ntrue then try the \n-h\n option for ls:\n\n\n$ ls -lh /home/group/common/training/Intro_to_Unix/expectations.txt\n-rw-r--r-- 1 arobinson common 1010K Mar 26  2012 /home/group/common/training/Intro_to_Unix/expectations.txt\n\n\n\n\nIn this case the size is reported in kilobytes as \n1010K\n. Larger files are reported in megabytes, gigabytes \netcetera.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink81\").click(function(e){\n            e.preventDefault();\n            $(\"#showable81\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nNote\n: this exercise assumes your working directory is \n~/test\n; if not run \ncd ~/test\n\n\n\n\n\n\n4.4) Check that the contents of expectations.txt are the same in both the directory that you copied \nit from and the directory that you copied it to.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWhat is the opposite of \nsame\n?\n\n\n\n\n\n\nMore\n\n\n\n\ndiff\nerence\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink84\").click(function(e){\n            e.preventDefault();\n            $(\"#showable84\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable84\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink84\").text(\"More\");\n            } else {\n                $(\"#showablelink84\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink83\").click(function(e){\n            e.preventDefault();\n            $(\"#showable83\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \ndiff\n command to compare the contents of two files.\n\n\n$ diff /home/group/common/training/Intro_to_Unix/expectations.txt expectations.txt\n\n\n\n\nIf the two files are identical the \ndiff\n command will NOT produce any output)\n\n\nAnswer\n: Yes, they are the same since no output was given.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink85\").click(function(e){\n            e.preventDefault();\n            $(\"#showable85\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.5) How many lines, words and characters are in expectations.txt?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nInitialisms are key\n\n\n\n\n\n\nMore\n\n\n\n\nw\nord \nc\nount\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink88\").click(function(e){\n            e.preventDefault();\n            $(\"#showable88\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable88\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink88\").text(\"More\");\n            } else {\n                $(\"#showablelink88\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink87\").click(function(e){\n            e.preventDefault();\n            $(\"#showable87\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \nwc\n (for \"word count\") to count the number of characters, lines and words in a file:\n\n\n$ wc expectations.txt\n  20415  187465 1033773 expectations.txt\n\n\n\n\nAnswer\n: There are \n20415\n lines, \n187465\n words and \n1033773\n characters in expectations.txt.\n\n\nTo get just the line, word or character count:\n\n\n$ wc -l expectations.txt\n20415 expectations.txt\n$ wc -w expectations.txt\n187465 expectations.txt\n$ wc -c expectations.txt\n1033773 expectations.txt\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink89\").click(function(e){\n            e.preventDefault();\n            $(\"#showable89\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.6) Open \n~/test/expectations.txt\n in the \nnano\n text editor, delete the first line of text, and \nsave your changes to the file. Exit \nnano\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nnano FILENAME\n\n\nOnce \nnano\n is open it displays some command hints along the bottom of the screen.\n\n\n\n\n\n\nMore\n\n\n\n\n^O\n means hold the \nControl\n (or CTRL) key while pressing the \no\n.  Dispite what it displays, you need to type \nthe lower-case letter that follows the \n^\n character.\n\n\nWriteOut is another name for Save.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink92\").click(function(e){\n            e.preventDefault();\n            $(\"#showable92\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable92\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink92\").text(\"More\");\n            } else {\n                $(\"#showablelink92\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink91\").click(function(e){\n            e.preventDefault();\n            $(\"#showable91\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nTake some time to play around with the \nnano\n text editor.\n\n\nNano\n is a very simple text editor which is easy to use but limited in features. More powerful \neditors exist such as \nvim\n and \nemacs\n, however they take a substantial amount of time to learn.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink93\").click(function(e){\n            e.preventDefault();\n            $(\"#showable93\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.7) Did the changes you made to \n~/test/expectations.txt\n have any effect on \n\n/home/group/common/training/Intro_to_Unix/expectations.txt\n? How can you tell if two files are the \nsame or different in their contents?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember exercise 4.4\n\n\n\n\n\n\nMore\n\n\n\n\nUse \ndiff\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink96\").click(function(e){\n            e.preventDefault();\n            $(\"#showable96\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable96\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink96\").text(\"More\");\n            } else {\n                $(\"#showablelink96\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink95\").click(function(e){\n            e.preventDefault();\n            $(\"#showable95\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse \ndiff\n to check that the two files are different after you have made the change to the copy of \n\nexpectations.txt\n in your \n~/test\n directory.\n\n\ndiff ~/test/expectations.txt \\\n/home/group/common/training/Intro_to_Unix/expectations.txt\n\n\n\n\nYou could also use \nls\n to check that the files have different sizes.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink97\").click(function(e){\n            e.preventDefault();\n            $(\"#showable97\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.8) In your \ntest\n subdirectory, rename \nexpectations.txt\n to \nfoo.txt\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nAnother way to think of it is \nmoving\n it from \nexpectations.txt\n to \nfoo.txt\n\n\n\n\n\n\nMore\n\n\n\n\nmv\n\n\nUse \nman mv\n if you need to work out how to use it.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink100\").click(function(e){\n            e.preventDefault();\n            $(\"#showable100\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable100\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink100\").text(\"More\");\n            } else {\n                $(\"#showablelink100\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink99\").click(function(e){\n            e.preventDefault();\n            $(\"#showable99\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \nmv\n command to rename the file:\n\n\n$ mv expectations.txt foo.txt\n$ ls\nfoo.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink101\").click(function(e){\n            e.preventDefault();\n            $(\"#showable101\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.9) Rename foo.txt back to expectations.txt.\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \nmv\n command to rename the file:\n\n\n$ mv foo.txt expectations.txt\n$ ls\nexpectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy\n\n\n\n\nUse \nls\n to check that the file is in fact renamed.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink103\").click(function(e){\n            e.preventDefault();\n            $(\"#showable103\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.10) Remove the file \nexpectations.txt\n from your \ntest\n directory.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWe are trying to \nremove\n a file, check the commands at the top of this topic.\n\n\n\n\n\n\nMore\n\n\n\n\nrm\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink106\").click(function(e){\n            e.preventDefault();\n            $(\"#showable106\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable106\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink106\").text(\"More\");\n            } else {\n                $(\"#showablelink106\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink105\").click(function(e){\n            e.preventDefault();\n            $(\"#showable105\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \nrm\n command to remove files (carefully):\n\n\n$ rm expectations.txt\n$ ls\nhello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink107\").click(function(e){\n            e.preventDefault();\n            $(\"#showable107\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.11) Remove the entire \ntest\n directory and all the files within it.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWe are trying to \nremove a directory\n.\n\n\n\n\n\n\nMore\n\n\n\n\nYou could use \nrmdir\n but there is an easier way using just \nrm\n and a flag.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink110\").click(function(e){\n            e.preventDefault();\n            $(\"#showable110\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable110\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink110\").text(\"More\");\n            } else {\n                $(\"#showablelink110\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink109\").click(function(e){\n            e.preventDefault();\n            $(\"#showable109\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou could use the \nrm\n command to remove each file individually, and then use the \nrmdir\n command \nto remove the directory. Note that \nrmdir\n will only remove directories that are empty (i.e. do not \ncontain files or subdirectories).\n\n\nA faster way is to pass the \n-r\n (for recursive) flag to \nrm\n to remove all the files and the \ndirectory in one go:\n\n\nLogical Answer\n:\n\n\ncd ~\nrm test/*\nrmdir test\n\n\n\n\nEasier Answer\n:\n\n\ncd ~\nrm -r test\n\n\n\n\nWarning\n: Be very careful with \nrm -r\n, it will remove all files \nand all subdirectories underneath the specified directory. This could be catastrophic if you do it \nin the wrong location! Now is a good moment to pause and think about file backup strategies.\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink111\").click(function(e){\n            e.preventDefault();\n            $(\"#showable111\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.12) Recreate the test directory in your home directory and copy all the files from \n\n/home/group/common/training/Intro_to_Unix/\n back into the test directory.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nSee exercises 4.1 and 4.2\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink113\").click(function(e){\n            e.preventDefault();\n            $(\"#showable113\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nRepeat exercises 4.1 and 4.2.\n\n\n$ cd ~\n$ mkdir test\n$ cp /home/group/common/training/Intro_to_Unix/* test\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink114\").click(function(e){\n            e.preventDefault();\n            $(\"#showable114\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.13) Change directories to \n~/test\n and use the \ncat\n command to display the entire contents \nof the file \nhello.c\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse \nman\n if you can't guess how it might work.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink116\").click(function(e){\n            e.preventDefault();\n            $(\"#showable116\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ cd ~/test\n$ cat hello.c\n#include \nstdio.h\n\nint main(void) {\n    printf (\nHello World\\n\n);\n    return 0;\n}\n\n\n\n\nhello.c\n contains the source code of a C program. The compiled executable version of this code \nis in the file called \nhi\n, which you can run like so:\n\n\n$ ./hi\nHello World\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink117\").click(function(e){\n            e.preventDefault();\n            $(\"#showable117\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.14) Use the \nhead\n command to view the first \n20\n lines of the file \nsample_1.fastq\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember your \nbest\n friend!\n\n\n\n\n\n\nMore\n\n\n\n\nUse \nman\n to find out what option you need to add to display a given number of \nlines\n.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink120\").click(function(e){\n            e.preventDefault();\n            $(\"#showable120\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable120\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink120\").text(\"More\");\n            } else {\n                $(\"#showablelink120\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink119\").click(function(e){\n            e.preventDefault();\n            $(\"#showable119\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ head -20 sample_1.fastq\n@IRIS:7:1:17:394#0/1\nGTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n+IRIS:7:1:17:394#0/1\naaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n@IRIS:7:1:17:800#0/1\nGGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n+IRIS:7:1:17:800#0/1\nababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n@IRIS:7:1:17:1757#0/1\nTTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC\n+IRIS:7:1:17:1757#0/1\naaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y\n@IRIS:7:1:17:1479#0/1\nCATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT\n+IRIS:7:1:17:1479#0/1\nabaaaaa`a```^aaaaa`_]aaa`aaa__a_X]``\n@IRIS:7:1:17:150#0/1\nTGATGTACTATGCATATGAACTTGTATGCAAAGTGG\n+IRIS:7:1:17:150#0/1\nabaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink121\").click(function(e){\n            e.preventDefault();\n            $(\"#showable121\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.15) Use the \ntail\n command to view the last \n8\n lines of the file \nsample_1.fastq\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIts very much like \nhead\n.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink123\").click(function(e){\n            e.preventDefault();\n            $(\"#showable123\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\ntail -8 sample_1.fastq\n@IRIS:7:32:731:717#0/1\nTAATAATTGGAGCCAAATCATGAATCAAAGGACATA\n+IRIS:7:32:731:717#0/1\nababbababbab]abbaa`babaaabbb`bbbabbb\n@IRIS:7:32:731:1228#0/1\nCTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG\n+IRIS:7:32:731:1228#0/1\n`aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink124\").click(function(e){\n            e.preventDefault();\n            $(\"#showable124\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.16) Use the \ngrep\n command to find out all the lines in \nmoby.txt\n that contain the word \n\"Ahab\"\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nOne might say we are 'looking for the \npattern\n \"Ahab\"'\n\n\n\n\n\n\nMore\n\n\n\n\n$ man grep\n...\nSYNOPSIS\n       grep [OPTIONS] PATTERN [FILE...]\n...\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink127\").click(function(e){\n            e.preventDefault();\n            $(\"#showable127\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable127\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink127\").text(\"More\");\n            } else {\n                $(\"#showablelink127\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink126\").click(function(e){\n            e.preventDefault();\n            $(\"#showable126\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ grep Ahab moby.txt\n\nWant to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\n\n\nWho is Captain Ahab, sir?\n\n\nAye, aye, I thought so. Captain Ahab is the Captain of this ship.\n\n... AND MUCH MUCH MORE ...\n\n\n\n\nIf you want to know how many lines are in the output of the above command you can \"pipe\" it \ninto the \nwc -l\n command:\n\n\n$ grep Ahab moby.txt | wc -l\n491\n\n\n\n\nwhich shows that there are \n491\n lines in \nmoby.txt\n that contain the word Ahab.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink128\").click(function(e){\n            e.preventDefault();\n            $(\"#showable128\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.17) Use the \ngrep\n command to find out all the lines in \nexpectations.txt\n that contain the \nword \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera)\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nOne might say we are \nignoring case\n.\n\n\n\n\n\n\nMore\n\n\n\n\n$ man grep\n...\n       -i, --ignore-case\n              Ignore case distinctions in both the PATTERN and the input files.  (-i is specified by POSIX.)\n...\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink131\").click(function(e){\n            e.preventDefault();\n            $(\"#showable131\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable131\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink131\").text(\"More\");\n            } else {\n                $(\"#showablelink131\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink130\").click(function(e){\n            e.preventDefault();\n            $(\"#showable130\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \n-i\n flag to \ngrep\n to make it perform case insensitive search:\n\n\n$ grep -i the expectations.txt\nThe Project Gutenberg EBook of Great Expectations, by Charles Dickens\nThis eBook is for the use of anyone anywhere at no cost and with\nre-use it under the terms of the Project Gutenberg License included\n[Project Gutenberg Editor's Note: There is also another version of\n... AND MUCH MUCH MORE ...\n\n\n\n\nAgain, \"pipe\" the output to \nwc -l\n to count the number of lines:\n\n\n$ grep -i the expectations.txt  | wc -l\n8165\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink132\").click(function(e){\n            e.preventDefault();\n            $(\"#showable132\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n4.18) Use the \ngzip\n command to compress the file \nsample_1.fastq\n. Use \ngunzip\n to decompress it \nback to the original contents.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the above commands along with \nman\n and \nls\n to see what happens to the file.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink134\").click(function(e){\n            e.preventDefault();\n            $(\"#showable134\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nCheck the file size of sample_1.fastq before compressing it:\n\n\n# check filesize\n$ ls -l sample_1.fastq\n-rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq\n\n# compress it (takes a few seconds)\n$ gzip sample_1.fastq\n\n# check filesize (Note: its name changed)\n$ ls -l sample_1.fastq.gz\n-rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz\n\n# decompress it\n$ gunzip sample_1.fastq.gz\n\n$ ls -l sample_1.fastq\n-rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq\n\n\n\n\nYou will see that when it was compressed it is \n26997595\n bytes in size, making it about \n0.3\n times the size of the \noriginal file.\n\n\nNote\n: in the above section the lines starting with \n#\n are comments so don't need to be copied but if you\ndo then they wont do anything.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink135\").click(function(e){\n            e.preventDefault();\n            $(\"#showable135\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nTopic 5: Pipes, output redirection and shell scripts\n\n\nIn this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see\nthe power of Unix.  I say \nstart\n because this is only the \"tip of the iceberg\".\n\n\nDuration\n: 50 minutes. \n\n\nRelevant commands\n: \nwc\n, \npaste\n, \ngrep\n, \nsort\n, \nuniq\n, \nnano\n, \ncut\n\n\n\n\n\n\n5.1) How many \nreads\n are contained in the file \nsample_1.fastq\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nExamine some of the file to work out how many lines each \nread\n takes up.\n\n\n\n\n\n\nMore\n\n\n\n\nCount the number of lines\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink138\").click(function(e){\n            e.preventDefault();\n            $(\"#showable138\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable138\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink138\").text(\"More\");\n            } else {\n                $(\"#showablelink138\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink137\").click(function(e){\n            e.preventDefault();\n            $(\"#showable137\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nWe can answer this question by counting the number of lines in the file and dividing by 4:\n\n\n$ wc -l sample_1.fastq\n3000000\n\n\n\n\nAnswer\n: There are \n3000000\n lines in the file representing \n750000\n reads.\n\n\nIf you want to do simple arithmetic at the command line then you can use the \"basic calculator\" \ncalled \nbc\n:\n\n\n$ echo \n3000000 / 4\n | bc\n750000\n\n\n\n\nNote\n: that the vertical bar character \"|\" is the Unix pipe (and is often \ncalled the \"pipe symbol\"). It is used for connecting the output of one command into the input of \nanother command. We'll see more examples soon.\n\n\n\nbc\n is suitable for small calculations, but it becomes cumbersome for more complex examples. If \nyou want to do more sophisticated calculations then we recommend to use a more general purpose \nprogramming language (such as Python etcetera).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink139\").click(function(e){\n            e.preventDefault();\n            $(\"#showable139\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.2) How many reads in \nsample_1.fastq\n contain the sequence \nGATTACA\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheck out exercise 4.16\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink141\").click(function(e){\n            e.preventDefault();\n            $(\"#showable141\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse \ngrep\n to find all the lines that contain \nGATTACA\n and \"pipe\" the output to \nwc -l\n to count them:\n\n\n$ grep GATTACA sample_1.fastq | wc -l\n1119\n\n\n\n\nAnswer\n: \n1119\n\n\nIf you are unsure about the possibility of upper and lower case characters then consider using \nthe \n-i\n (ignore case option for grep).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink142\").click(function(e){\n            e.preventDefault();\n            $(\"#showable142\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.3) On what line numbers do the sequences containing \nGATTACA\n occur?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nWe are looking for the \nline numbers\n.\n\n\n\n\n\n\nMore\n\n\n\n\nCheck out the manpage for \ngrep\n and/or \nnl\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink145\").click(function(e){\n            e.preventDefault();\n            $(\"#showable145\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable145\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink145\").text(\"More\");\n            } else {\n                $(\"#showablelink145\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink144\").click(function(e){\n            e.preventDefault();\n            $(\"#showable144\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nYou can use the \n-n\n flag to grep to make it prefix each line with a line number:\n\n\nAnswer 1\n: \n\n\n$ grep -n GATTACA sample_1.fastq\n5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC\n7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC\n8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC\n... AND MUCH MUCH MORE ...\n\n\n\n\nAnswer 2\n: Or you can use the \nnl\n command to number each line of sample_1.fastq and then search for \nGATTACA\n \nin the numbered lines:\n\n\n$ nl sample_1.fastq | grep GATTACA\n  5078  AGGAAGATTACAACTCCAAGACACCAAACAAATTCC\n  7170  AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC\n  8238  ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC\n... AND MUCH MUCH MORE ...\n\n\n\n\nJust the line numbers\n:\n\n\nIf you just want to see the line numbers then you can \"pipe\" the output of the above command into \n\ncut -f 1\n:\n\n\n$ nl sample_1.fastq | grep GATTACA | cut -f 1\n  5078\n  7170\n  8238\n... AND MUCH MUCH MORE ...\n\n\n\n\ncut\n will remove certain columns from the input; in this case it will remove all except column 1\n(a.k.a. field 1, hence the \n-f 1\n option)\n\n\n$ grep -n GATTACA sample_1.fastq | cut -d: -f 1\n5078\n7170\n8238\n... AND MUCH MUCH MORE ...\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink146\").click(function(e){\n            e.preventDefault();\n            $(\"#showable146\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.4) Use the \nnl\n command to print each line of \nsample_1.fastq\n with its corresponding line \nnumber at the beginning.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheck answer to 5.3.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink148\").click(function(e){\n            e.preventDefault();\n            $(\"#showable148\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ nl sample_1.fastq\n     1  @IRIS:7:1:17:394#0/1\n     2  GTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n     3  +IRIS:7:1:17:394#0/1\n     4  aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n     5  @IRIS:7:1:17:800#0/1\n     6  GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n     7  +IRIS:7:1:17:800#0/1\n     8  ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n... AND MUCH MUCH MORE ...\n\n\n\n\nThere are a lot of lines in that file so this command might take a while to print all its output. \nIf you get tired of looking at the output you can kill the command with \ncontrol-c\n (hold the \n\ncontrol\n key down and simultaneously press the \"\nc\n\" character).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink149\").click(function(e){\n            e.preventDefault();\n            $(\"#showable149\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.5) Redirect the output of the previous command to a file called \nsample_1.fastq.nl\n. Check \nthe first \n20\n lines of \nsample_1.fastq.nl\n with the \nhead\n command. Use the \nless\n command to \ninteractively view the contents of \nsample_1.fastq.nl\n (use the arrow keys to navigate up and down, \n\nq\n to quit and '\n/\n' to search). Use the search facility in less to find occurrences of \n\nGATTACA\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nOk that one was tough, \n FILENAME\n is how you do it if you didn't break out an internet search for \n\"redirect the output in Unix\"\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink151\").click(function(e){\n            e.preventDefault();\n            $(\"#showable151\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ nl sample_1.fastq \n sample_1.fastq.nl\n\n\n\n\nThe greater-than sign \"\n\" is the file redirection operator. It causes the standard output of the \ncommand on the left-hand-side to be written to the file on the right-hand-side.\n\n\nYou should notice that the above command is much faster than printing the output to the screen. \nThis is because writing to disk can be performed much more quickly than rendering the output on \na terminal.\n\n\nTo check that the first 20 lines of the file look reasonable you can use the \nhead\n command like so:\n\n\n$ head -20 sample_1.fastq.nl\n     1  @IRIS:7:1:17:394#0/1\n     2  GTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n     3  +IRIS:7:1:17:394#0/1\n     4  aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n     5  @IRIS:7:1:17:800#0/1\n     6  GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n     7  +IRIS:7:1:17:800#0/1\n     8  ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n...\n\n\n\n\nThe \nless\n command allows you to interactively view a file. The arrow keys move the page up and \ndown. You can search using the '\n/\n' followed by the search term. You can quit by pressing \"\nq\n\". Note \nthat the \nless\n command is used by default to display man pages.\n\n\n$ less sample_1.fastq.nl\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink152\").click(function(e){\n            e.preventDefault();\n            $(\"#showable152\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be \npreferable if we could convert it to tab-separated-value (TSV) format, such that each read appears \non a single line with each of its fields separated by tabs. Use the following command to convert \nsample_1.fastq.nl into TSV format:\n\n\n\n\n$ cat sample_1.fastq | paste - - - - \n sample_1.tsv\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink153\").click(function(e){\n            e.preventDefault();\n            $(\"#showable153\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe \n'-'\n (dash) character has a special meaning when used in place of a file; it means use the standard\ninput instead of a real file.  Note: while it is fairly common in most Unix programs, not all wil support it.\n\n\nThe \npaste\n command is useful for merging multiple files together line-by-line, such that the \nNth\n \nline from each file is joined together into one line in the output, separated by default with a \n\ntab\n character. In the above example we give paste 4 copies of the contents of \nsample_1.fastq\n, \nwhich causes it to join consecutive groups of 4 lines from the file into one line of output.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink154\").click(function(e){\n            e.preventDefault();\n            $(\"#showable154\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.7) Do you expect the output of the following command to produce the same output as above? and why?\n\n\n\n\n$ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq \n sample_1b.tsv\n\n\n\n\nTry it, see what ends up in sample_1b.tsv (maybe use \nless\n)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink155\").click(function(e){\n            e.preventDefault();\n            $(\"#showable155\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nUse \nless\n to examine it.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink156\").click(function(e){\n            e.preventDefault();\n            $(\"#showable156\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nAnswer\n: No, in the second instance we get 4 copies of each line.\n\n\nWhy\n: In the first command \npaste\n will use the input file (standard input) 4 times since the \ncat\n \ncommand will only give one copy of the file to \npaste\n, where as, in the second command \npaste\n will open \nthe file 4 times.  Note: this is quite confusing and is not necessory to remember; its just an interesting\nside point.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink157\").click(function(e){\n            e.preventDefault();\n            $(\"#showable157\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.8) Check that \nsample_1.tsv\n has the correct number of lines. Use the \nhead\n command to view \nthe first \n20\n lines of the file.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nRemember the \nwc\n command.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink159\").click(function(e){\n            e.preventDefault();\n            $(\"#showable159\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nWe can count the number of lines in \nsample_1.tsv\n using \nwc\n:\n\n\n$ wc -l sample_1.tsv\n\n\n\n\nThe output should be \n750000\n as expected (1/4 of the number of lines in sample_1.fastq).\n\n\nTo view the first \n20\n lines of \nsample_1.tsv\n use the \nhead\n command:\n\n\n$ head -20 sample_1.tsv\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink160\").click(function(e){\n            e.preventDefault();\n            $(\"#showable160\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.9) Use the \ncut\n command to print out the second column of \nsample_1.tsv\n. Redirect the \noutput to a file called \nsample_1.dna.txt\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nSee exercise 5.3 (for cut) and 5.5 (redirection)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink162\").click(function(e){\n            e.preventDefault();\n            $(\"#showable162\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe file sample_1.tsv is in column format. The cut command can be used to select certain columns \nfrom the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag \n(the f stands for \"field\").\n\n\ncut -f 2 sample_1.tsv \n sample_1.dna.txt\n\n\n\n\nCheck that the output file looks reasonable using \nhead\n or \nless\n.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink163\").click(function(e){\n            e.preventDefault();\n            $(\"#showable163\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.10) Use the \nsort\n command to sort the lines of \nsample_1.dna.txt\n and redirect the output to \n\nsample_1.dna.sorted.txt\n. Use \nhead\n to look at the first few lines of the output file. You should \nsee a lot of repeated sequences of As.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse \nman\n (sort) and see exercise 5.5 (redirection)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink165\").click(function(e){\n            e.preventDefault();\n            $(\"#showable165\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ sort sample_1.dna.txt \n sample_1.dna.sorted.txt\n\n\n\n\nRunning \nhead\n on the output file reveals that there are duplicate DNA sequences in the input FASTQ \nfile.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink166\").click(function(e){\n            e.preventDefault();\n            $(\"#showable166\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.11) Use the \nuniq\n command to remove duplicate consecutive lines from \nsample_1.dna.sorted.txt\n, \nredirect the result to \nsample_1.dna.uniq.txt\n. Compare the number of lines in sample1_dna.txt to \nthe number of lines in \nsample_1.dna.uniq.txt\n.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nI am pretty sure you have already used \nman\n (or just guessed how to use \nuniq\n).  You're also a gun at \nredirection now.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink168\").click(function(e){\n            e.preventDefault();\n            $(\"#showable168\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n$ uniq sample_1.dna.sorted.txt \n sample_1.dna.uniq.txt\n\n\n\n\nCompare the outputs of:\n\n\n$ wc -l sample_1.dna.sorted.txt\n750000\n$ wc -l sample_1.dna.uniq.txt\n614490\n\n\n\n\nView the contents of \nsample_1.dna.uniq.txt\n to check that the duplicate DNA sequences have been \nremoved.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink169\").click(function(e){\n            e.preventDefault();\n            $(\"#showable169\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.12) Can you modify the command from above to produce \nonly\n those sequences of DNA which were \nduplicated in \nsample_1.dna.sorted.txt\n?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheckout the \nuniq\n manpage\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink171\").click(function(e){\n            e.preventDefault();\n            $(\"#showable171\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nHint\n\n\n\n\nLook at the man page for uniq.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink172\").click(function(e){\n            e.preventDefault();\n            $(\"#showable172\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nUse the \n-d\n flag to \nuniq\n to print out only the duplicated lines from the file:\n\n\n$ uniq -d sample_1.dna.sorted.txt \n sample_1.dna.dup.txt\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink173\").click(function(e){\n            e.preventDefault();\n            $(\"#showable173\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.13) Write a \nshell pipeline\n which will print the number of duplicated DNA sequences in \nsample_1.fastq.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThat is, \npiping\n most of the commands you used above instead of redirecting to file\n\n\n\n\n\n\nMore\n\n\n\n\nI.e. 6 commands (\ncat\n, \npaste\n, \ncut\n, \nsort\n, \nuniq\n, \nwc\n)\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink176\").click(function(e){\n            e.preventDefault();\n            $(\"#showable176\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable176\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink176\").text(\"More\");\n            } else {\n                $(\"#showablelink176\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink175\").click(function(e){\n            e.preventDefault();\n            $(\"#showable175\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nFinally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a \nFASTQ file and ends with a list of duplicated DNA sequences:\n\n\nAnswer\n:\n\n\n$ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\n56079\n\n\n\n\nThe output file should have \n56079\n lines.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink177\").click(function(e){\n            e.preventDefault();\n            $(\"#showable177\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences \nin sample_1.fastq.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nCheck out the \nsleepy\n file (with \ncat\n or \nnano\n); there is a bit of magic on the first line that you will need. \n\n\nYou also need to tell bash that this file can be executed (check out \nchmod\n command).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink179\").click(function(e){\n            e.preventDefault();\n            $(\"#showable179\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nPut the answer to \n5.13\n into a file called \nsample_1_dups.sh\n (or whatever you want). Use \nnano\n to \ncreate the file. \n\n\nAnswer\n: the contents of the file will look like this:\n\n\n#!/bin/bash\n\ncat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\n\n\n\n\nNote\n: the first line has special meaning.  If it starts with '\n#!\n' (Hash \nthen exclamation mark) then it tells bash this file is a script that can be interpreted.  The command \n(including full path) used to intepret the script is placed right after the magic code.\n\n\n\nGive everyone execute permissions on the file with chmod:\n\n\n$ chmod +x sample_1_dups.sh \n\n\n\n\nYou can run the script like so:\n\n\n$ ./sample_1_dups.sh\n\n\n\n\nIf all goes well the script should behave in exactly the same way as the answer to 5.13.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink180\").click(function(e){\n            e.preventDefault();\n            $(\"#showable180\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file \nas a command line parameter.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nShell scripts can refer to command line arguments by their position using special variables called \n\n$0\n, \n$1\n, \n$2\n and so on. \n\n\n\n\n\n\nMore\n\n\n\n\n$0\n refers to the name of the script as it was called on the command line. \n\n$1\n refers to the first command line argument, and so on.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink183\").click(function(e){\n            e.preventDefault();\n            $(\"#showable183\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable183\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink183\").text(\"More\");\n            } else {\n                $(\"#showablelink183\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink182\").click(function(e){\n            e.preventDefault();\n            $(\"#showable182\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\nCopy the shell script from \n5.14\n into a new file:\n\n\n$ cp sample_1_dups.sh fastq_dups.sh\n\n\n\n\nEdit the new shell script file and change it to use the command line parameters:\n\n\n#!/bin/bash\n\ncat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\n\n\n\n\nYou can run the new script like so:\n\n\n$ ./fastq_dups.sh sample_1.fastq\n\n\n\n\nIn the above example the script takes \nsample_1.fastq\n as input and prints the number of duplicated \nsequences as output.\n\n\nA better Answer\n:\n\n\nIdeally we would write our shell script to be more robust. At the moment it just assumes there \nwill be at least one command line argument. However, it would be better to check and produce an \nerror message if insufficient arguments were given:\n\n\n#!/bin/bash\nif [ $# -eq 1 ]; then\n    cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\nelse\n    echo \nUsage: $0 \nfastq_filename\n\n    exit 1\nfi\n\n\n\n\nThe '\nif ...; then\n' line means: do the following line(s) ONLY if the \n...\n (called condition) bit is true.\n\n\nThe '\nelse\n' line means: otherwise do the following line(s) instead.  Note: it is optional.\n\n\nThe '\nfi\n' line means: this marks the end of the current \nif\n or \nelse\n section.\n\n\nThe '\n[ $# -eq 1 ]\n' part is the condition:\n\n\n\n\n$#\n: is a special shell variable that indicates how many command line arguments were given. \n\n\n-eq\n: checks if the numbers on either side if it are equal.\n\n\n1\n: is a number one\n\n\n\n\nSpaces in conditions\n:\nBash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely \n(without warning).  You MUST put a space near the share brackets and between each part of the condition!\n\n\n\nSo in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\".\n\n\nExit-status\n:\nIt is a Unix standard that when the user provides incorrect commandline arguments we print a usage message \nand return a *non-zero* exit status.  The *exit status* is a standard way for other programs to know if\nour program ran correctly; 0 means everything went as expected, any other number is an error.  If you don't\nprovide an *exit ..* line then it automatically returns a 0 for you.\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink184\").click(function(e){\n            e.preventDefault();\n            $(\"#showable184\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n\n\n5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the \ncommand line argument and outputs the number of duplicated DNA sequences in each file.\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nWe can add a loop to our script to accept multiple input FASTQ files:\n\n\n#!/bin/bash\nfor file in $@; do\n    dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l)\n    echo \n$file $dups\n\ndone\n\n\n\n\nThere's a lot going on in this script.\n\n\nThe \n$@\n is a sequence of all command line arguments.\n\n\nThe '\nfor ...; do\n' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in \nthe sequence to the variable called \nfile\n.\n\n\nThe \n$(...)\n allow us to capture the output of another command (in-place of the \n...\n).  In this \ncase we capture the output of the pipeline and save it to the variable called \ndups\n.\n\n\nIf you had multiple FASTQ files available you could run the script like so:\n\n\n./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq\n\n\n\n\nAnd it would produce output like:\n\n\nsample_1.fastq 56079\nsample_2.fastq XXXXX\nsample_3.fastq YYYYY\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink186\").click(function(e){\n            e.preventDefault();\n            $(\"#showable186\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nFinished\n\n\nWell done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it \nwas a lot to take in.\n\n\nFrom here you should be confortable around the Unix command line and ready to take on the HPC \nWorkshop.\n\n\nYou will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this \nWorkshop for later reference.\n\n\nThank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it\nback to the Workshop facilitators.", 
            "title": "Introduction to Unix"
        }, 
        {
            "location": "/tutorials/unix/#introduction-to-unix", 
            "text": "A hands-on-workshop covering the basics of the Unix/Linux command line interface", 
            "title": "Introduction to Unix"
        }, 
        {
            "location": "/tutorials/unix/#how-to-use-this-workshop", 
            "text": "The workshop is broken up into a number of  Topics  each focusing on a particular aspect of Unix.  You should take a short break between \neach to refresh and relax before tackling the next.  Topic s may start with some background followed by a number of  exercises .  Each  exercise  begins with a  question , then \nsometimes a  hint  (or two) and finishes with the suggested  answer .", 
            "title": "How to use this workshop"
        }, 
        {
            "location": "/tutorials/unix/#question", 
            "text": "An example question looks like:    What is the Answer to Life?    (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)", 
            "title": "Question"
        }, 
        {
            "location": "/tutorials/unix/#hint", 
            "text": "Depending on how much of a challenge you like, you may choose to use hints.  Even if you work out the answer without hints, its a good \nidea to read the hints afterwards because they contain extra information that is good to know.  Note:  hint s may be staged, that is, there may be a  more  section within a hint for further hints    Hint   - click here to reveal hint   What is the answer to everything?  As featured in \"The Hitchhiker's Guide to the Galaxy\"    More   - and here to show more   It is probably a two digit number     \n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable2\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink2\").text(\"More\");\n            } else {\n                $(\"#showablelink2\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Hint"
        }, 
        {
            "location": "/tutorials/unix/#answer", 
            "text": "Once you have worked out the answer to the question expand the Answer section to check if you got it correct.    Answer   - click here to reveal answer   Answer : 42  Ref:  Number 42 (Wikipedia)     \n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Answer"
        }, 
        {
            "location": "/tutorials/unix/#usage-style", 
            "text": "This workshop attempts to cater for two usage styles:   Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional):  Attempt to answer the question by yourself.  Use hints when you get stuck.  Once solved, reveal the answer and read through our suggested solution.  Its a good idea to read the hints and answer description as they often contain extra useful information.    By example :  for those who learn by following examples:   Expand  all sections  Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the\n  same (or similar) answers.  Its a good idea to read the hints and answer description as they often contain extra useful information.", 
            "title": "Usage Style"
        }, 
        {
            "location": "/tutorials/unix/#topic-1-remote-log-in", 
            "text": "In this topic we will learn how to connect to a  Unix  computer via a method called  SSH  and run a few basic commands.", 
            "title": "Topic 1: Remote log in"
        }, 
        {
            "location": "/tutorials/unix/#connecting-to-a-unix-computer", 
            "text": "To begin this workshop you will need to connect to an HPC.  Today we will use the LIMS-HPC.  The computer called  lims-hpc-m  (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks.  Server details :   host : lims-hpc-m.latrobe.edu.au  port : 6022   username : trainingXX (where XX is a two digit number, provided at workshop)  password : (provided at workshop)      Mac OS X / Linux   Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line.  To use OpenSSH you must \nfirst start a terminal program on your computer.  On OS X the standard terminal is called Terminal, and it is installed by default. \nOn Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good \ndefault).  When you've started the terminal you should see a command prompt.  To log into LIMS-HPC, for example, type this command at \nthe prompt and press return (where the word username is replaced with your LIMS-HPC username):  ssh -p 6022 username@lims-hpc-m.latrobe.edu.au  The same procedure works for any other machine where you have an account except most other HPCs will not need the  -p 6022  \n(which is telling ssh to connect on a non-standard port number).  You may be presented with a message along the lines of:  The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be  established.\n...\nAre you sure you want to continue connecting (yes/no)?  Although you should never ignore a warning, this particular one is nothing to be concerned about; type  yes  and then  press enter . \nIf all goes well you will be asked to enter your password.  Assuming you type the correct username and password the system should \nthen display a welcome message, and then present you with a Unix prompt.  If you get this far then you are ready to start entering \nUnix commands and thus begin using the remote computer.     \n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Windows   On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client.  PuTTY (putty.exe) can be downloaded \nfrom this web page:  http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html  Documentation for using PuTTY is here:  http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html  When you start PuTTY you should see a window which looks something like this:   To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and  6022  in the port, \nthen click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the \nsame as they are in the picture above).  In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines \nof  \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by \nclicking on Yes). You usually see this message the first time you try to connect to a particular remote computer.  If all goes well, a terminal window will open, showing a prompt with the text  \"login as:\" . An example terminal window is shown \nbelow. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your \npassword. Assuming you type the correct username and password the system should then display a welcome message, and then \npresent you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using \nthe remote computer.      \n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n        });\n    });\n      Note : for security reasons ssh will not display any characters when you enter your password. This \ncan be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be \nalarmed; type your password in and press return at the end.  LIMS-HPC is a high performance computer for La Trobe Users.  Logging in connects your local computer \n(e.g. laptop) to LIMS-HPC, and allows you to type commands into the Unix prompt which are run on \nthe HPC, and have the results displayed on your local screen.  You will be allocated a training account on LIMS-HPC for the duration of the workshop. Your \nusername and password will be supplied at the start of the workshop.  Log out of LIMS-HPC, and log back in again (to make sure you can repeat the process).  All the remaining parts assume that you are logged into LIMS-HPC over ssh.", 
            "title": "Connecting to a Unix computer"
        }, 
        {
            "location": "/tutorials/unix/#exercises", 
            "text": "1.1) When you\u2019ve logged into LIMS-HPC run the following commands and see what they do:   who\nwhoami\ndate\ncal\nhostname\n/home/group/common/training/Intro_to_Unix/hi     \n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    who : displays a list of the users who are currently using this Unix computer.  whoami : displays your username (i.e. they person currently logged in).  date : displays the current date and time.  cal : displays a calendar on the terminal.  It can be configured to display more than just \nthe current month.  hostname : displays the name of the computer we are logged in to.  /home/group/common/training/Intro_to_Unix/hi : displays the text \"Hello World\"      \n    $(document).ready(function(){\n        $(\"#showablelink7\").click(function(e){\n            e.preventDefault();\n            $(\"#showable7\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Exercises"
        }, 
        {
            "location": "/tutorials/unix/#topic-2-exploring-your-home-directory", 
            "text": "In this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands.   Duration : 20 minutes.   Relevant commands :  ls ,  pwd ,  echo ,  man  Your home directory contains your own private working space.  Your  current working directory  is automatically set \nto your  home  directory when you log into a Unix computer.    2.1) Use the  ls  command to list the files in your  home  directory.  How many files are there?      Hint   Literally, type  ls  and press the  ENTER  key.     \n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ ls\nexp01  file01  muscle.fq  When running the  ls  command with no options it will list files in your current working directory.  The place \nwhere you start when you first login is your  HOME  directory.  Answer : 3 (exp01, file01 and muscle.fq)     \n    $(document).ready(function(){\n        $(\"#showablelink10\").click(function(e){\n            e.preventDefault();\n            $(\"#showable10\").toggleClass(\"showable-hidden\");\n        });\n    });\n       The above answer is not quite correct.  There are a number of  hidden  files in your home directory as well.    2.2) What  flag  might you use to display  all  files with the  ls  command?  How many files are really there?      Hint   Take the  all  quite literally.    More   Type  ls --all  and press the  ENTER  key.     \n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable13\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink13\").text(\"More\");\n            } else {\n                $(\"#showablelink13\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Answer 1 :  --all  (or  -a ) flag  Now you should see several files in your home directory whose names all begin with a dot. All these files are \ncreated automatically for your user account. They are mostly configuration options for various programs including \nthe shell. It is safe to ignore them for the moment.  $ ls --all\n.              .bash_logout    exp01    .lesshst\n..             .bash_profile   file01   muscle.fq\n.bash_history  .bashrc         .kshrc   .viminfo  There are two trick files here; namely  .  and  ..  which are not real files but instead, shortcuts.   .  is a shortcut\nfor the current directory and  ..  a shortcut for the directory above the current one.  Answer 2 : 10 files (don't count  .  and  .. )     \n    $(document).ready(function(){\n        $(\"#showablelink14\").click(function(e){\n            e.preventDefault();\n            $(\"#showable14\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.3) What is the full path name of your  home  directory?      Hint   Remember your  Current Working Directory  start's in your  home  directory (and the hint from the slides).    More   Try a shortened version of  print working directory     \n    $(document).ready(function(){\n        $(\"#showablelink17\").click(function(e){\n            e.preventDefault();\n            $(\"#showable17\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable17\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink17\").text(\"More\");\n            } else {\n                $(\"#showablelink17\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You can find out the full path name of the current working directory with the  pwd  command. Your home directory \nwill look something like this:  $ pwd\n/home/trainingXY  Answer :  /home/trainingXY  where  XY  is replaced by some 2 digit sequence.  Alternate method :\nYou can also find out the name of your home directory by printing the value of the  $HOME  shell variable:  echo $HOME     \n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.4) Run  ls  using the long flag ( -l ), how did the output change?      Hint   Run  ls -l     \n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Answer : it changed the output to place 1 file/directory per line.  It also added some extra information\nabout each.  $ ls -l\ntotal 32\ndrwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01\n-rw-r----- 1 training01 training   97 Jun 14 11:28 file01\n-rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq  Details :  drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01\n\\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/\npermission |  username   group   size    date       name\n       /---^---\\\n       linkcount  Where:   permissions : 4 parts, file type, user perms, group perms and other perms  filetype : 1 character,  d  = directory and  -  regular file  user  permissions: 3 characters,  r  = read,  w  = write,  x  = execute and  -  no permission  group  permissions: same as user except for users within the owner group  other  permissions: same as user except for users that are not in either user  or   group    username : the user who  owns  this file/directory  group : the group name who  owns  this file/directory  size : the number of bytes this file/directory takes to store on disk  date : the date and time when this file/directory was  last edited  name : name of the file  linkcount : technical detail which represents the number of links this file has in the file system (safe to ignore)      \n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.5) What type of file is  exp01  and  muscle.fq ?      Hint   Check the output from the  ls -l .     \n    $(document).ready(function(){\n        $(\"#showablelink23\").click(function(e){\n            e.preventDefault();\n            $(\"#showable23\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Answer :   exp01 : Directory (given the 'd' as the first letter of its permissions)  muscle.fq : Regular File (given the '-')      \n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.6) Who has permission to  read ,  write  and  execute  your  home  directory?      Hint   You can also give  ls  a filename as the first option.    More   ls -l  will show you the contents of the  CWD ; how might you see the contents of the  parent  directory? (remember\nthe slides)     \n    $(document).ready(function(){\n        $(\"#showablelink27\").click(function(e){\n            e.preventDefault();\n            $(\"#showable27\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable27\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink27\").text(\"More\");\n            } else {\n                $(\"#showablelink27\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   If you pass the  -l  flag to ls it will display a \"long\" listing of file information including file permissions.  There are various ways you could find out the permissions on your home directory.  Method 1 : given we know the  CWD  is our home directory.  $ ls -l ..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...  The  ..  refers to the parent directory.  Method 2 : using $HOME.  This works no matter what our  CWD  is set to.   You could list the permissions of all files and directories in the parent directory of your home:  $ ls -l $HOME/..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...  In this case we use the shell variable to refer to our home directory.  Method 3 : using  ~  (tilde) shortcut  You may also refer to your home directory using the  ~  (tilde) character:  $ ls -l ~/..\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 trainingXY\n...  All 3 of the methods above mean the same thing.  You will see a list of files and directories in the parent directory of your home directory. One of them will \nbe the name of your home directory, something like  trainingXY .  Where  XY  is replaced by a two digit string  Altername : using the  -a  flag and looking at the  .  (dot) special file.  $ ls -la\n...\ndrwxr-x--- 4 trainingXY training  512 Feb  9 14:18 .\n...  Answer :  drwxr-x---   You : read (see filenames), write (add, delete files), execute (change your CWD to this directory).  Training users : read, execute  Everyone else : No access   Discussion on Permissions :  The permission string is  \"drwxr-x---\" . The  d  means it is a directory. The  rwx  means that the owner of the directory \n(your user account) can  read ,  write  and  execute  the directory. Execute permissions on a directory means that you \ncan  cd  into the directory. The  r-x  means that anyone in the same user group as  training  can read or execute the \ndirectory. The  ---  means that nobody else (other users on the system) can do anything with the directory.     \n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n        man  is for manual : and it will be your best friend!  Manual pages include a lot of detail about a command and its available flags/options.  It should be your first (or second) \nport of call when you are trying to work out what a command or option does.  You can scroll  up  and  down  in the man page using the  arrow  keys.  You can search in the man page using the forward \nslash followed by the search text followed by the  ENTER  key. e.g. \ntype  /hello  and press  ENTER  to search for the word  hello .  Press  n  key to find next \noccurance of  hello  etc.  You can  quit  the man page by pressing  q .      2.7) Use the  man  command to find out what the  -h  flag does for  ls      Hint   Give  ls  as an option to  man  command.    More   man ls     \n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable31\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink31\").text(\"More\");\n            } else {\n                $(\"#showablelink31\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink30\").click(function(e){\n            e.preventDefault();\n            $(\"#showable30\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the following command to view the  man  page for  ls :  $ man ls  Answer : You should discover that the  -h  option prints file sizes in human readable format  -h, --human-readable\n              with -l, print sizes in human readable format (e.g., 1K 234M 2G)     \n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });\n         2.8) Use the  -h , how did the output change of  muscle.fq ?      Hint   Don't forget the  -l  option too.    More   Run  ls -lh     \n    $(document).ready(function(){\n        $(\"#showablelink35\").click(function(e){\n            e.preventDefault();\n            $(\"#showable35\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable35\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink35\").text(\"More\");\n            } else {\n                $(\"#showablelink35\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink34\").click(function(e){\n            e.preventDefault();\n            $(\"#showable34\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ ls -lh\n...\n-rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq  Answer : it changed the output so the  filesize  of  muscle.fq  is now  2.5K  instead of  2461     \n    $(document).ready(function(){\n        $(\"#showablelink36\").click(function(e){\n            e.preventDefault();\n            $(\"#showable36\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Topic 2: Exploring your home directory"
        }, 
        {
            "location": "/tutorials/unix/#topic-3-exploring-the-file-system", 
            "text": "In this topic we will learn how to move around the filesystem and see what is there.  Duration : 30 minutes.   Relevant commands :  pwd ,  cd ,  ls ,  file    3.1) Print the value of your current working directory.      Answer   The  pwd  command prints the value of your current working directory.  $ pwd\n/home/training01     \n    $(document).ready(function(){\n        $(\"#showablelink38\").click(function(e){\n            e.preventDefault();\n            $(\"#showable38\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.2) List the contents of the root directory, called ' / ' (forward \nslash).      Hint   ls  expects a single option which is the directory to change too.     \n    $(document).ready(function(){\n        $(\"#showablelink40\").click(function(e){\n            e.preventDefault();\n            $(\"#showable40\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ ls /\napplications-merged  etc         media    root     tmp\nbin                  home        mnt      sbin     usr\nboot                 lib         oldhome  selinux  var\ndata                 lib64       opt      srv\ndev                  lost+found  proc     sys  Here we see that  ls  can take a filepath as its argument, which allows you to list the contents of directories \nother than your current working directory.     \n    $(document).ready(function(){\n        $(\"#showablelink41\").click(function(e){\n            e.preventDefault();\n            $(\"#showable41\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.3) Use the  cd  command to change your working directory to the root directory.  Did your prompt \nchange?      Hint   cd  expects a single option which is the directory to change to     \n    $(document).ready(function(){\n        $(\"#showablelink43\").click(function(e){\n            e.preventDefault();\n            $(\"#showable43\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The  cd  command changes the value of your current working directory. To change to the root directory use the \nfollowing command:  $ cd /  Answer : Yes, it now says the CWD is  /  instead of  ~ .  Some people imagine that changing the working directory is akin to moving your focus within the file system. \nSo people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory.  The root directory is special in Unix. It is the topmost directory in the whole file system.     \n    $(document).ready(function(){\n        $(\"#showablelink44\").click(function(e){\n            e.preventDefault();\n            $(\"#showable44\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Output on ERROR only : Many Unix commands will not produce any output if everything went well;  cd  is one\nsuch command.  However, it will get grumpy if something went wrong by way of an error message on-screen.     3.4) List the contents of the CWD and verify it matches the list in 3.2      Hint   ls     \n    $(document).ready(function(){\n        $(\"#showablelink46\").click(function(e){\n            e.preventDefault();\n            $(\"#showable46\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Assuming you have changed to the root directory then this can be achieved with  ls , or  ls -a  (for all files) or  ls -la  for a long listing of all files.  If you are not currently in the root directory then you can list its contents by passing it as an argument to ls:  $ ls\napplications-merged  etc         media    root     tmp\nbin                  home        mnt      sbin     usr\nboot                 lib         oldhome  selinux  var\ndata                 lib64       opt      srv\ndev                  lost+found  proc     sys  Answer : Yes, we got the same output as exercise 3.2     \n    $(document).ready(function(){\n        $(\"#showablelink47\").click(function(e){\n            e.preventDefault();\n            $(\"#showable47\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.5) Change your current working directory back to your home directory. What is the simplest Unix command that \nwill get you back to your home directory from anywhere else in the file system?      Hint   The answer to exercise 2.6 might give some hints on how to get back to the home directory    More   $HOME ,  ~ ,  /home/trainingXY  are all methods to name your home directory.  Yet there is a simpler method; the answer\nis buried in  man cd  however  cd  doesn't its own manpage so you will need to search for it.     \n    $(document).ready(function(){\n        $(\"#showablelink50\").click(function(e){\n            e.preventDefault();\n            $(\"#showable50\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable50\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink50\").text(\"More\");\n            } else {\n                $(\"#showablelink50\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink49\").click(function(e){\n            e.preventDefault();\n            $(\"#showable49\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  cd  command to change your working directory to your home directory. There are a number of ways to refer \nto your home directory:  cd $HOME  is equivalent to:  cd ~  The simplest way to change your current working directory to your home directory is to run the  cd  command with \nno arguments:  Answer : the simplest for is cd with NO options.  cd  This is a special-case behaviour which is built into  cd  for convenience.     \n    $(document).ready(function(){\n        $(\"#showablelink51\").click(function(e){\n            e.preventDefault();\n            $(\"#showable51\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.6) Change your working directory to  /home/group/common/training/Intro_to_Unix/      Answer   cd /home/group/common/training/Intro_to_Unix/     \n    $(document).ready(function(){\n        $(\"#showablelink53\").click(function(e){\n            e.preventDefault();\n            $(\"#showable53\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.7) List the contents of that directory. How many files does it contain?      Hint   ls     \n    $(document).ready(function(){\n        $(\"#showablelink55\").click(function(e){\n            e.preventDefault();\n            $(\"#showable55\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You can do this with  ls  $ ls\nexpectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy  Answer : 7 files (expectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy)     \n    $(document).ready(function(){\n        $(\"#showablelink56\").click(function(e){\n            e.preventDefault();\n            $(\"#showable56\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.8) What kind of  file  is  /home/group/common/training/Intro_to_Unix/sleepy ?      Hint   Take the word  file  quite literally.    More   file sleepy     \n    $(document).ready(function(){\n        $(\"#showablelink59\").click(function(e){\n            e.preventDefault();\n            $(\"#showable59\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable59\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink59\").text(\"More\");\n            } else {\n                $(\"#showablelink59\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink58\").click(function(e){\n            e.preventDefault();\n            $(\"#showable58\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  file  command to get extra information about the contents of a file:  Assuming your current working directory is  /home/group/common/training/Intro_to_Unix/  $ file sleepy\nBourne-Again shell script text executable  Otherwise specify the full path of sleepy:  $ file /home/group/common/training/Intro_to_Unix/sleepy\nBourne-Again shell script text executable  Answer : Bourne-Again shell script text executable  The \"Bourne-Again shell\" is more commonly known as BASH. The  file  command is telling us that sleepy \nis (probably) a shell script written in the language of BASH.  The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works \nthen read the Unix manual page like so:  man file     \n    $(document).ready(function(){\n        $(\"#showablelink60\").click(function(e){\n            e.preventDefault();\n            $(\"#showable60\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.9) What kind of  file  is  /home/group/common/training/Intro_to_Unix/hi ?      Hint   Take the word  file  quite literally.     \n    $(document).ready(function(){\n        $(\"#showablelink62\").click(function(e){\n            e.preventDefault();\n            $(\"#showable62\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the file command again. If you are in the same directory as  hi  then:  $ file hi\nELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux \n2.6.9, not stripped  Answer : ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux   This rather complicated output is roughly saying that the file called  hi  contains a binary executable \nprogram (raw instructions that the computer can execute directly).     \n    $(document).ready(function(){\n        $(\"#showablelink63\").click(function(e){\n            e.preventDefault();\n            $(\"#showable63\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.10) What are the file permissions of  /home/group/common/training/Intro_to_Unix/sleepy ? \nWhat do they mean?      Hint   Remember the  ls  command, and don't forget the  -l  flag     \n    $(document).ready(function(){\n        $(\"#showablelink65\").click(function(e){\n            e.preventDefault();\n            $(\"#showable65\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You can find the permissions of  sleepy  using the  ls  command with the  -l  flag. If you are in the same \ndirectory as  sleepy  then:  $ ls -l sleepy\n-rw-r--r-- 1 arobinson common 183 Feb  9 16:36 sleepy  Answer : We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common \nuser group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is \nreadable to everyone, and writeable only to training01.  The digit '1' between the file permission string and \nthe owner indicates that there is one link to the file. The Unix file system allows files to be referred to \nby multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For \nfuture reference: links are created with the  ln  command.     \n    $(document).ready(function(){\n        $(\"#showablelink66\").click(function(e){\n            e.preventDefault();\n            $(\"#showable66\").toggleClass(\"showable-hidden\");\n        });\n    });\n         3.11) Change your working directory back to your home directory ready for the next topic.      Hint   cd     \n    $(document).ready(function(){\n        $(\"#showablelink68\").click(function(e){\n            e.preventDefault();\n            $(\"#showable68\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You should know how to do this with the cd command:  cd     \n    $(document).ready(function(){\n        $(\"#showablelink69\").click(function(e){\n            e.preventDefault();\n            $(\"#showable69\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Topic 3: Exploring the file system"
        }, 
        {
            "location": "/tutorials/unix/#topic-4-working-with-files-and-directories", 
            "text": "In this topic we will start to read, create, edit and delete files and directories.  Duration : 50 minutes.    Relevant commands :  mkdir ,  cp ,  ls ,  diff ,  wc ,  nano ,  mv ,  rm ,  rmdir ,  head ,  tail ,  grep ,  gzip ,  gunzip   Hint : Look at the commands above; you will need them roughly in order for this topic.  Use the  man \ncommand find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections.     4.1) In your home directory make a sub-directory called test.      Hint   You are trying to  make a directory , which of the above commands looks like a shortened version of this?    More   mkdir     \n    $(document).ready(function(){\n        $(\"#showablelink72\").click(function(e){\n            e.preventDefault();\n            $(\"#showable72\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable72\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink72\").text(\"More\");\n            } else {\n                $(\"#showablelink72\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink71\").click(function(e){\n            e.preventDefault();\n            $(\"#showable71\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Make sure you are in your home directory first. If not  cd  to your home directory.  Use the  mkdir  command to make new directories:  $ mkdir test  Use the  ls  command to check that the new directory was created.  $ ls\nexp01  file01  muscle.fq  test     \n    $(document).ready(function(){\n        $(\"#showablelink73\").click(function(e){\n            e.preventDefault();\n            $(\"#showable73\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.2) Copy all the files from  /home/group/common/training/Intro_to_Unix/  into the newly created \ntest directory.      Hint   You are trying to  copy , which of the above commands looks like a shortened version of this?    More   $ man cp\n...\nSYNOPSIS\n       cp [OPTION]... [-T] SOURCE DEST\n...\nDESCRIPTION\n       Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.  which means  cp  expects zero or more flags, a SOURCE file followed by a DEST file or directory      \n    $(document).ready(function(){\n        $(\"#showablelink76\").click(function(e){\n            e.preventDefault();\n            $(\"#showable76\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable76\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink76\").text(\"More\");\n            } else {\n                $(\"#showablelink76\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink75\").click(function(e){\n            e.preventDefault();\n            $(\"#showable75\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  cp  command to copy files.   Wildcards : You could copy them one-by-one, but that would be tedious, so use \nthe  *  wildcard to specify that you want to copy all the files.  There are a number of ways you could do this depending on how you specify the source and destination \npaths to  cp . You only need to perform one of these ways, but we show multiple ones for your reference.  Answer 1 : From your home directory:  $ cp /home/group/common/training/Intro_to_Unix/* test  Answer 2 : Change to the test directory and then copy (assuming you started in your home directory):  $ cd test\n$ cp /home/group/common/training/Intro_to_Unix/* .  In the example above the ' . ' (dot) character refers to the current working directory. It should be \nthe test subdirectory of your home directory.  Answer 3 : Change to the /home/group/common/training/Intro_to_Unix/ directory and then copy:  cd /home/group/common/training/Intro_to_Unix/\ncp * ~/test  Remember that ~ is a shortcut reference to your home directory.     \n    $(document).ready(function(){\n        $(\"#showablelink77\").click(function(e){\n            e.preventDefault();\n            $(\"#showable77\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Note : This exercise assumes that the copy command from the previous exercise was successful.     4.3) Check that the file size of  expectations.txt  is the same in both the directory that you copied \nit from and the directory that you copied it to.      Hint   Remember  ls  can show you the file size (with one of its flags)    More   ls -l     \n    $(document).ready(function(){\n        $(\"#showablelink80\").click(function(e){\n            e.preventDefault();\n            $(\"#showable80\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable80\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink80\").text(\"More\");\n            } else {\n                $(\"#showablelink80\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink79\").click(function(e){\n            e.preventDefault();\n            $(\"#showable79\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use  ls -l  to check the size of files.  You could do this in many ways depending on the value of your working directory. We just show one possible \nway for each file:  $ ls -l /home/group/common/training/Intro_to_Unix/expectations.txt\n\n$ ls -l ~/test/expectations.txt  From the output of the above commands you should be able to see the size of each file and check that they \nare the same.   Answer : They should each be  1033773  bytes  Alternate : Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is \ntrue then try the  -h  option for ls:  $ ls -lh /home/group/common/training/Intro_to_Unix/expectations.txt\n-rw-r--r-- 1 arobinson common 1010K Mar 26  2012 /home/group/common/training/Intro_to_Unix/expectations.txt  In this case the size is reported in kilobytes as  1010K . Larger files are reported in megabytes, gigabytes \netcetera.     \n    $(document).ready(function(){\n        $(\"#showablelink81\").click(function(e){\n            e.preventDefault();\n            $(\"#showable81\").toggleClass(\"showable-hidden\");\n        });\n    });\n       Note : this exercise assumes your working directory is  ~/test ; if not run  cd ~/test    4.4) Check that the contents of expectations.txt are the same in both the directory that you copied \nit from and the directory that you copied it to.      Hint   What is the opposite of  same ?    More   diff erence     \n    $(document).ready(function(){\n        $(\"#showablelink84\").click(function(e){\n            e.preventDefault();\n            $(\"#showable84\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable84\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink84\").text(\"More\");\n            } else {\n                $(\"#showablelink84\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink83\").click(function(e){\n            e.preventDefault();\n            $(\"#showable83\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  diff  command to compare the contents of two files.  $ diff /home/group/common/training/Intro_to_Unix/expectations.txt expectations.txt  If the two files are identical the  diff  command will NOT produce any output)  Answer : Yes, they are the same since no output was given.     \n    $(document).ready(function(){\n        $(\"#showablelink85\").click(function(e){\n            e.preventDefault();\n            $(\"#showable85\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.5) How many lines, words and characters are in expectations.txt?      Hint   Initialisms are key    More   w ord  c ount     \n    $(document).ready(function(){\n        $(\"#showablelink88\").click(function(e){\n            e.preventDefault();\n            $(\"#showable88\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable88\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink88\").text(\"More\");\n            } else {\n                $(\"#showablelink88\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink87\").click(function(e){\n            e.preventDefault();\n            $(\"#showable87\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  wc  (for \"word count\") to count the number of characters, lines and words in a file:  $ wc expectations.txt\n  20415  187465 1033773 expectations.txt  Answer : There are  20415  lines,  187465  words and  1033773  characters in expectations.txt.  To get just the line, word or character count:  $ wc -l expectations.txt\n20415 expectations.txt\n$ wc -w expectations.txt\n187465 expectations.txt\n$ wc -c expectations.txt\n1033773 expectations.txt     \n    $(document).ready(function(){\n        $(\"#showablelink89\").click(function(e){\n            e.preventDefault();\n            $(\"#showable89\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.6) Open  ~/test/expectations.txt  in the  nano  text editor, delete the first line of text, and \nsave your changes to the file. Exit  nano .      Hint   nano FILENAME  Once  nano  is open it displays some command hints along the bottom of the screen.    More   ^O  means hold the  Control  (or CTRL) key while pressing the  o .  Dispite what it displays, you need to type \nthe lower-case letter that follows the  ^  character.  WriteOut is another name for Save.     \n    $(document).ready(function(){\n        $(\"#showablelink92\").click(function(e){\n            e.preventDefault();\n            $(\"#showable92\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable92\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink92\").text(\"More\");\n            } else {\n                $(\"#showablelink92\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink91\").click(function(e){\n            e.preventDefault();\n            $(\"#showable91\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Take some time to play around with the  nano  text editor.  Nano  is a very simple text editor which is easy to use but limited in features. More powerful \neditors exist such as  vim  and  emacs , however they take a substantial amount of time to learn.     \n    $(document).ready(function(){\n        $(\"#showablelink93\").click(function(e){\n            e.preventDefault();\n            $(\"#showable93\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.7) Did the changes you made to  ~/test/expectations.txt  have any effect on  /home/group/common/training/Intro_to_Unix/expectations.txt ? How can you tell if two files are the \nsame or different in their contents?      Hint   Remember exercise 4.4    More   Use  diff     \n    $(document).ready(function(){\n        $(\"#showablelink96\").click(function(e){\n            e.preventDefault();\n            $(\"#showable96\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable96\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink96\").text(\"More\");\n            } else {\n                $(\"#showablelink96\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink95\").click(function(e){\n            e.preventDefault();\n            $(\"#showable95\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use  diff  to check that the two files are different after you have made the change to the copy of  expectations.txt  in your  ~/test  directory.  diff ~/test/expectations.txt \\\n/home/group/common/training/Intro_to_Unix/expectations.txt  You could also use  ls  to check that the files have different sizes.     \n    $(document).ready(function(){\n        $(\"#showablelink97\").click(function(e){\n            e.preventDefault();\n            $(\"#showable97\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.8) In your  test  subdirectory, rename  expectations.txt  to  foo.txt .      Hint   Another way to think of it is  moving  it from  expectations.txt  to  foo.txt    More   mv  Use  man mv  if you need to work out how to use it.     \n    $(document).ready(function(){\n        $(\"#showablelink100\").click(function(e){\n            e.preventDefault();\n            $(\"#showable100\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable100\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink100\").text(\"More\");\n            } else {\n                $(\"#showablelink100\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink99\").click(function(e){\n            e.preventDefault();\n            $(\"#showable99\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  mv  command to rename the file:  $ mv expectations.txt foo.txt\n$ ls\nfoo.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy     \n    $(document).ready(function(){\n        $(\"#showablelink101\").click(function(e){\n            e.preventDefault();\n            $(\"#showable101\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.9) Rename foo.txt back to expectations.txt.      Answer   Use the  mv  command to rename the file:  $ mv foo.txt expectations.txt\n$ ls\nexpectations.txt  hello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy  Use  ls  to check that the file is in fact renamed.     \n    $(document).ready(function(){\n        $(\"#showablelink103\").click(function(e){\n            e.preventDefault();\n            $(\"#showable103\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.10) Remove the file  expectations.txt  from your  test  directory.      Hint   We are trying to  remove  a file, check the commands at the top of this topic.    More   rm     \n    $(document).ready(function(){\n        $(\"#showablelink106\").click(function(e){\n            e.preventDefault();\n            $(\"#showable106\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable106\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink106\").text(\"More\");\n            } else {\n                $(\"#showablelink106\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink105\").click(function(e){\n            e.preventDefault();\n            $(\"#showable105\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  rm  command to remove files (carefully):  $ rm expectations.txt\n$ ls\nhello.c  hi  jude.txt  moby.txt  sample_1.fastq  sleepy     \n    $(document).ready(function(){\n        $(\"#showablelink107\").click(function(e){\n            e.preventDefault();\n            $(\"#showable107\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.11) Remove the entire  test  directory and all the files within it.      Hint   We are trying to  remove a directory .    More   You could use  rmdir  but there is an easier way using just  rm  and a flag.     \n    $(document).ready(function(){\n        $(\"#showablelink110\").click(function(e){\n            e.preventDefault();\n            $(\"#showable110\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable110\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink110\").text(\"More\");\n            } else {\n                $(\"#showablelink110\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink109\").click(function(e){\n            e.preventDefault();\n            $(\"#showable109\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You could use the  rm  command to remove each file individually, and then use the  rmdir  command \nto remove the directory. Note that  rmdir  will only remove directories that are empty (i.e. do not \ncontain files or subdirectories).  A faster way is to pass the  -r  (for recursive) flag to  rm  to remove all the files and the \ndirectory in one go:  Logical Answer :  cd ~\nrm test/*\nrmdir test  Easier Answer :  cd ~\nrm -r test  Warning : Be very careful with  rm -r , it will remove all files \nand all subdirectories underneath the specified directory. This could be catastrophic if you do it \nin the wrong location! Now is a good moment to pause and think about file backup strategies.     \n    $(document).ready(function(){\n        $(\"#showablelink111\").click(function(e){\n            e.preventDefault();\n            $(\"#showable111\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.12) Recreate the test directory in your home directory and copy all the files from  /home/group/common/training/Intro_to_Unix/  back into the test directory.      Hint   See exercises 4.1 and 4.2     \n    $(document).ready(function(){\n        $(\"#showablelink113\").click(function(e){\n            e.preventDefault();\n            $(\"#showable113\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Repeat exercises 4.1 and 4.2.  $ cd ~\n$ mkdir test\n$ cp /home/group/common/training/Intro_to_Unix/* test     \n    $(document).ready(function(){\n        $(\"#showablelink114\").click(function(e){\n            e.preventDefault();\n            $(\"#showable114\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.13) Change directories to  ~/test  and use the  cat  command to display the entire contents \nof the file  hello.c      Hint   Use  man  if you can't guess how it might work.     \n    $(document).ready(function(){\n        $(\"#showablelink116\").click(function(e){\n            e.preventDefault();\n            $(\"#showable116\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ cd ~/test\n$ cat hello.c\n#include  stdio.h \nint main(void) {\n    printf ( Hello World\\n );\n    return 0;\n}  hello.c  contains the source code of a C program. The compiled executable version of this code \nis in the file called  hi , which you can run like so:  $ ./hi\nHello World     \n    $(document).ready(function(){\n        $(\"#showablelink117\").click(function(e){\n            e.preventDefault();\n            $(\"#showable117\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.14) Use the  head  command to view the first  20  lines of the file  sample_1.fastq      Hint   Remember your  best  friend!    More   Use  man  to find out what option you need to add to display a given number of  lines .     \n    $(document).ready(function(){\n        $(\"#showablelink120\").click(function(e){\n            e.preventDefault();\n            $(\"#showable120\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable120\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink120\").text(\"More\");\n            } else {\n                $(\"#showablelink120\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink119\").click(function(e){\n            e.preventDefault();\n            $(\"#showable119\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ head -20 sample_1.fastq\n@IRIS:7:1:17:394#0/1\nGTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n+IRIS:7:1:17:394#0/1\naaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n@IRIS:7:1:17:800#0/1\nGGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n+IRIS:7:1:17:800#0/1\nababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n@IRIS:7:1:17:1757#0/1\nTTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC\n+IRIS:7:1:17:1757#0/1\naaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y\n@IRIS:7:1:17:1479#0/1\nCATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT\n+IRIS:7:1:17:1479#0/1\nabaaaaa`a```^aaaaa`_]aaa`aaa__a_X]``\n@IRIS:7:1:17:150#0/1\nTGATGTACTATGCATATGAACTTGTATGCAAAGTGG\n+IRIS:7:1:17:150#0/1\nabaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa     \n    $(document).ready(function(){\n        $(\"#showablelink121\").click(function(e){\n            e.preventDefault();\n            $(\"#showable121\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.15) Use the  tail  command to view the last  8  lines of the file  sample_1.fastq      Hint   Its very much like  head .     \n    $(document).ready(function(){\n        $(\"#showablelink123\").click(function(e){\n            e.preventDefault();\n            $(\"#showable123\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   tail -8 sample_1.fastq\n@IRIS:7:32:731:717#0/1\nTAATAATTGGAGCCAAATCATGAATCAAAGGACATA\n+IRIS:7:32:731:717#0/1\nababbababbab]abbaa`babaaabbb`bbbabbb\n@IRIS:7:32:731:1228#0/1\nCTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG\n+IRIS:7:32:731:1228#0/1\n`aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__     \n    $(document).ready(function(){\n        $(\"#showablelink124\").click(function(e){\n            e.preventDefault();\n            $(\"#showable124\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.16) Use the  grep  command to find out all the lines in  moby.txt  that contain the word \n\"Ahab\"      Hint   One might say we are 'looking for the  pattern  \"Ahab\"'    More   $ man grep\n...\nSYNOPSIS\n       grep [OPTIONS] PATTERN [FILE...]\n...     \n    $(document).ready(function(){\n        $(\"#showablelink127\").click(function(e){\n            e.preventDefault();\n            $(\"#showable127\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable127\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink127\").text(\"More\");\n            } else {\n                $(\"#showablelink127\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink126\").click(function(e){\n            e.preventDefault();\n            $(\"#showable126\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ grep Ahab moby.txt Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?  Who is Captain Ahab, sir?  Aye, aye, I thought so. Captain Ahab is the Captain of this ship. \n... AND MUCH MUCH MORE ...  If you want to know how many lines are in the output of the above command you can \"pipe\" it \ninto the  wc -l  command:  $ grep Ahab moby.txt | wc -l\n491  which shows that there are  491  lines in  moby.txt  that contain the word Ahab.     \n    $(document).ready(function(){\n        $(\"#showablelink128\").click(function(e){\n            e.preventDefault();\n            $(\"#showable128\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.17) Use the  grep  command to find out all the lines in  expectations.txt  that contain the \nword \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera)\n.      Hint   One might say we are  ignoring case .    More   $ man grep\n...\n       -i, --ignore-case\n              Ignore case distinctions in both the PATTERN and the input files.  (-i is specified by POSIX.)\n...     \n    $(document).ready(function(){\n        $(\"#showablelink131\").click(function(e){\n            e.preventDefault();\n            $(\"#showable131\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable131\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink131\").text(\"More\");\n            } else {\n                $(\"#showablelink131\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink130\").click(function(e){\n            e.preventDefault();\n            $(\"#showable130\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  -i  flag to  grep  to make it perform case insensitive search:  $ grep -i the expectations.txt\nThe Project Gutenberg EBook of Great Expectations, by Charles Dickens\nThis eBook is for the use of anyone anywhere at no cost and with\nre-use it under the terms of the Project Gutenberg License included\n[Project Gutenberg Editor's Note: There is also another version of\n... AND MUCH MUCH MORE ...  Again, \"pipe\" the output to  wc -l  to count the number of lines:  $ grep -i the expectations.txt  | wc -l\n8165     \n    $(document).ready(function(){\n        $(\"#showablelink132\").click(function(e){\n            e.preventDefault();\n            $(\"#showable132\").toggleClass(\"showable-hidden\");\n        });\n    });\n         4.18) Use the  gzip  command to compress the file  sample_1.fastq . Use  gunzip  to decompress it \nback to the original contents.      Hint   Use the above commands along with  man  and  ls  to see what happens to the file.     \n    $(document).ready(function(){\n        $(\"#showablelink134\").click(function(e){\n            e.preventDefault();\n            $(\"#showable134\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Check the file size of sample_1.fastq before compressing it:  # check filesize\n$ ls -l sample_1.fastq\n-rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq\n\n# compress it (takes a few seconds)\n$ gzip sample_1.fastq\n\n# check filesize (Note: its name changed)\n$ ls -l sample_1.fastq.gz\n-rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz\n\n# decompress it\n$ gunzip sample_1.fastq.gz\n\n$ ls -l sample_1.fastq\n-rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq  You will see that when it was compressed it is  26997595  bytes in size, making it about  0.3  times the size of the \noriginal file.  Note : in the above section the lines starting with  #  are comments so don't need to be copied but if you\ndo then they wont do anything.     \n    $(document).ready(function(){\n        $(\"#showablelink135\").click(function(e){\n            e.preventDefault();\n            $(\"#showable135\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Topic 4: Working with files and directories"
        }, 
        {
            "location": "/tutorials/unix/#topic-5-pipes-output-redirection-and-shell-scripts", 
            "text": "In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see\nthe power of Unix.  I say  start  because this is only the \"tip of the iceberg\".  Duration : 50 minutes.   Relevant commands :  wc ,  paste ,  grep ,  sort ,  uniq ,  nano ,  cut    5.1) How many  reads  are contained in the file  sample_1.fastq ?      Hint   Examine some of the file to work out how many lines each  read  takes up.    More   Count the number of lines     \n    $(document).ready(function(){\n        $(\"#showablelink138\").click(function(e){\n            e.preventDefault();\n            $(\"#showable138\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable138\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink138\").text(\"More\");\n            } else {\n                $(\"#showablelink138\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink137\").click(function(e){\n            e.preventDefault();\n            $(\"#showable137\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   We can answer this question by counting the number of lines in the file and dividing by 4:  $ wc -l sample_1.fastq\n3000000  Answer : There are  3000000  lines in the file representing  750000  reads.  If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" \ncalled  bc :  $ echo  3000000 / 4  | bc\n750000  Note : that the vertical bar character \"|\" is the Unix pipe (and is often \ncalled the \"pipe symbol\"). It is used for connecting the output of one command into the input of \nanother command. We'll see more examples soon.  bc  is suitable for small calculations, but it becomes cumbersome for more complex examples. If \nyou want to do more sophisticated calculations then we recommend to use a more general purpose \nprogramming language (such as Python etcetera).     \n    $(document).ready(function(){\n        $(\"#showablelink139\").click(function(e){\n            e.preventDefault();\n            $(\"#showable139\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.2) How many reads in  sample_1.fastq  contain the sequence  GATTACA ?      Hint   Check out exercise 4.16     \n    $(document).ready(function(){\n        $(\"#showablelink141\").click(function(e){\n            e.preventDefault();\n            $(\"#showable141\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use  grep  to find all the lines that contain  GATTACA  and \"pipe\" the output to  wc -l  to count them:  $ grep GATTACA sample_1.fastq | wc -l\n1119  Answer :  1119  If you are unsure about the possibility of upper and lower case characters then consider using \nthe  -i  (ignore case option for grep).     \n    $(document).ready(function(){\n        $(\"#showablelink142\").click(function(e){\n            e.preventDefault();\n            $(\"#showable142\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.3) On what line numbers do the sequences containing  GATTACA  occur?      Hint   We are looking for the  line numbers .    More   Check out the manpage for  grep  and/or  nl     \n    $(document).ready(function(){\n        $(\"#showablelink145\").click(function(e){\n            e.preventDefault();\n            $(\"#showable145\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable145\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink145\").text(\"More\");\n            } else {\n                $(\"#showablelink145\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink144\").click(function(e){\n            e.preventDefault();\n            $(\"#showable144\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   You can use the  -n  flag to grep to make it prefix each line with a line number:  Answer 1 :   $ grep -n GATTACA sample_1.fastq\n5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC\n7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC\n8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC\n... AND MUCH MUCH MORE ...  Answer 2 : Or you can use the  nl  command to number each line of sample_1.fastq and then search for  GATTACA  \nin the numbered lines:  $ nl sample_1.fastq | grep GATTACA\n  5078  AGGAAGATTACAACTCCAAGACACCAAACAAATTCC\n  7170  AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC\n  8238  ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC\n... AND MUCH MUCH MORE ...  Just the line numbers :  If you just want to see the line numbers then you can \"pipe\" the output of the above command into  cut -f 1 :  $ nl sample_1.fastq | grep GATTACA | cut -f 1\n  5078\n  7170\n  8238\n... AND MUCH MUCH MORE ...  cut  will remove certain columns from the input; in this case it will remove all except column 1\n(a.k.a. field 1, hence the  -f 1  option)  $ grep -n GATTACA sample_1.fastq | cut -d: -f 1\n5078\n7170\n8238\n... AND MUCH MUCH MORE ...     \n    $(document).ready(function(){\n        $(\"#showablelink146\").click(function(e){\n            e.preventDefault();\n            $(\"#showable146\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.4) Use the  nl  command to print each line of  sample_1.fastq  with its corresponding line \nnumber at the beginning.      Hint   Check answer to 5.3.     \n    $(document).ready(function(){\n        $(\"#showablelink148\").click(function(e){\n            e.preventDefault();\n            $(\"#showable148\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ nl sample_1.fastq\n     1  @IRIS:7:1:17:394#0/1\n     2  GTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n     3  +IRIS:7:1:17:394#0/1\n     4  aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n     5  @IRIS:7:1:17:800#0/1\n     6  GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n     7  +IRIS:7:1:17:800#0/1\n     8  ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n... AND MUCH MUCH MORE ...  There are a lot of lines in that file so this command might take a while to print all its output. \nIf you get tired of looking at the output you can kill the command with  control-c  (hold the  control  key down and simultaneously press the \" c \" character).     \n    $(document).ready(function(){\n        $(\"#showablelink149\").click(function(e){\n            e.preventDefault();\n            $(\"#showable149\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.5) Redirect the output of the previous command to a file called  sample_1.fastq.nl . Check \nthe first  20  lines of  sample_1.fastq.nl  with the  head  command. Use the  less  command to \ninteractively view the contents of  sample_1.fastq.nl  (use the arrow keys to navigate up and down,  q  to quit and ' / ' to search). Use the search facility in less to find occurrences of  GATTACA .      Hint   Ok that one was tough,   FILENAME  is how you do it if you didn't break out an internet search for \n\"redirect the output in Unix\"     \n    $(document).ready(function(){\n        $(\"#showablelink151\").click(function(e){\n            e.preventDefault();\n            $(\"#showable151\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ nl sample_1.fastq   sample_1.fastq.nl  The greater-than sign \" \" is the file redirection operator. It causes the standard output of the \ncommand on the left-hand-side to be written to the file on the right-hand-side.  You should notice that the above command is much faster than printing the output to the screen. \nThis is because writing to disk can be performed much more quickly than rendering the output on \na terminal.  To check that the first 20 lines of the file look reasonable you can use the  head  command like so:  $ head -20 sample_1.fastq.nl\n     1  @IRIS:7:1:17:394#0/1\n     2  GTCAGGACAAGAAAGACAANTCCAATTNACATTATG\n     3  +IRIS:7:1:17:394#0/1\n     4  aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^\n     5  @IRIS:7:1:17:800#0/1\n     6  GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG\n     7  +IRIS:7:1:17:800#0/1\n     8  ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT\n...  The  less  command allows you to interactively view a file. The arrow keys move the page up and \ndown. You can search using the ' / ' followed by the search term. You can quit by pressing \" q \". Note \nthat the  less  command is used by default to display man pages.  $ less sample_1.fastq.nl     \n    $(document).ready(function(){\n        $(\"#showablelink152\").click(function(e){\n            e.preventDefault();\n            $(\"#showable152\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be \npreferable if we could convert it to tab-separated-value (TSV) format, such that each read appears \non a single line with each of its fields separated by tabs. Use the following command to convert \nsample_1.fastq.nl into TSV format:   $ cat sample_1.fastq | paste - - - -   sample_1.tsv     \n    $(document).ready(function(){\n        $(\"#showablelink153\").click(function(e){\n            e.preventDefault();\n            $(\"#showable153\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The  '-'  (dash) character has a special meaning when used in place of a file; it means use the standard\ninput instead of a real file.  Note: while it is fairly common in most Unix programs, not all wil support it.  The  paste  command is useful for merging multiple files together line-by-line, such that the  Nth  \nline from each file is joined together into one line in the output, separated by default with a  tab  character. In the above example we give paste 4 copies of the contents of  sample_1.fastq , \nwhich causes it to join consecutive groups of 4 lines from the file into one line of output.     \n    $(document).ready(function(){\n        $(\"#showablelink154\").click(function(e){\n            e.preventDefault();\n            $(\"#showable154\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.7) Do you expect the output of the following command to produce the same output as above? and why?   $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq   sample_1b.tsv  Try it, see what ends up in sample_1b.tsv (maybe use  less )     \n    $(document).ready(function(){\n        $(\"#showablelink155\").click(function(e){\n            e.preventDefault();\n            $(\"#showable155\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   Use  less  to examine it.     \n    $(document).ready(function(){\n        $(\"#showablelink156\").click(function(e){\n            e.preventDefault();\n            $(\"#showable156\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Answer : No, in the second instance we get 4 copies of each line.  Why : In the first command  paste  will use the input file (standard input) 4 times since the  cat  \ncommand will only give one copy of the file to  paste , where as, in the second command  paste  will open \nthe file 4 times.  Note: this is quite confusing and is not necessory to remember; its just an interesting\nside point.     \n    $(document).ready(function(){\n        $(\"#showablelink157\").click(function(e){\n            e.preventDefault();\n            $(\"#showable157\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.8) Check that  sample_1.tsv  has the correct number of lines. Use the  head  command to view \nthe first  20  lines of the file.      Hint   Remember the  wc  command.     \n    $(document).ready(function(){\n        $(\"#showablelink159\").click(function(e){\n            e.preventDefault();\n            $(\"#showable159\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   We can count the number of lines in  sample_1.tsv  using  wc :  $ wc -l sample_1.tsv  The output should be  750000  as expected (1/4 of the number of lines in sample_1.fastq).  To view the first  20  lines of  sample_1.tsv  use the  head  command:  $ head -20 sample_1.tsv     \n    $(document).ready(function(){\n        $(\"#showablelink160\").click(function(e){\n            e.preventDefault();\n            $(\"#showable160\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.9) Use the  cut  command to print out the second column of  sample_1.tsv . Redirect the \noutput to a file called  sample_1.dna.txt .      Hint   See exercise 5.3 (for cut) and 5.5 (redirection)     \n    $(document).ready(function(){\n        $(\"#showablelink162\").click(function(e){\n            e.preventDefault();\n            $(\"#showable162\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   The file sample_1.tsv is in column format. The cut command can be used to select certain columns \nfrom the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag \n(the f stands for \"field\").  cut -f 2 sample_1.tsv   sample_1.dna.txt  Check that the output file looks reasonable using  head  or  less .     \n    $(document).ready(function(){\n        $(\"#showablelink163\").click(function(e){\n            e.preventDefault();\n            $(\"#showable163\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.10) Use the  sort  command to sort the lines of  sample_1.dna.txt  and redirect the output to  sample_1.dna.sorted.txt . Use  head  to look at the first few lines of the output file. You should \nsee a lot of repeated sequences of As.      Hint   Use  man  (sort) and see exercise 5.5 (redirection)     \n    $(document).ready(function(){\n        $(\"#showablelink165\").click(function(e){\n            e.preventDefault();\n            $(\"#showable165\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ sort sample_1.dna.txt   sample_1.dna.sorted.txt  Running  head  on the output file reveals that there are duplicate DNA sequences in the input FASTQ \nfile.     \n    $(document).ready(function(){\n        $(\"#showablelink166\").click(function(e){\n            e.preventDefault();\n            $(\"#showable166\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.11) Use the  uniq  command to remove duplicate consecutive lines from  sample_1.dna.sorted.txt , \nredirect the result to  sample_1.dna.uniq.txt . Compare the number of lines in sample1_dna.txt to \nthe number of lines in  sample_1.dna.uniq.txt .      Hint   I am pretty sure you have already used  man  (or just guessed how to use  uniq ).  You're also a gun at \nredirection now.     \n    $(document).ready(function(){\n        $(\"#showablelink168\").click(function(e){\n            e.preventDefault();\n            $(\"#showable168\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   $ uniq sample_1.dna.sorted.txt   sample_1.dna.uniq.txt  Compare the outputs of:  $ wc -l sample_1.dna.sorted.txt\n750000\n$ wc -l sample_1.dna.uniq.txt\n614490  View the contents of  sample_1.dna.uniq.txt  to check that the duplicate DNA sequences have been \nremoved.     \n    $(document).ready(function(){\n        $(\"#showablelink169\").click(function(e){\n            e.preventDefault();\n            $(\"#showable169\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.12) Can you modify the command from above to produce  only  those sequences of DNA which were \nduplicated in  sample_1.dna.sorted.txt ?      Hint   Checkout the  uniq  manpage     \n    $(document).ready(function(){\n        $(\"#showablelink171\").click(function(e){\n            e.preventDefault();\n            $(\"#showable171\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Hint   Look at the man page for uniq.     \n    $(document).ready(function(){\n        $(\"#showablelink172\").click(function(e){\n            e.preventDefault();\n            $(\"#showable172\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Use the  -d  flag to  uniq  to print out only the duplicated lines from the file:  $ uniq -d sample_1.dna.sorted.txt   sample_1.dna.dup.txt     \n    $(document).ready(function(){\n        $(\"#showablelink173\").click(function(e){\n            e.preventDefault();\n            $(\"#showable173\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.13) Write a  shell pipeline  which will print the number of duplicated DNA sequences in \nsample_1.fastq.      Hint   That is,  piping  most of the commands you used above instead of redirecting to file    More   I.e. 6 commands ( cat ,  paste ,  cut ,  sort ,  uniq ,  wc )     \n    $(document).ready(function(){\n        $(\"#showablelink176\").click(function(e){\n            e.preventDefault();\n            $(\"#showable176\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable176\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink176\").text(\"More\");\n            } else {\n                $(\"#showablelink176\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink175\").click(function(e){\n            e.preventDefault();\n            $(\"#showable175\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a \nFASTQ file and ends with a list of duplicated DNA sequences:  Answer :  $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\n56079  The output file should have  56079  lines.     \n    $(document).ready(function(){\n        $(\"#showablelink177\").click(function(e){\n            e.preventDefault();\n            $(\"#showable177\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences \nin sample_1.fastq.      Hint   Check out the  sleepy  file (with  cat  or  nano ); there is a bit of magic on the first line that you will need.   You also need to tell bash that this file can be executed (check out  chmod  command).     \n    $(document).ready(function(){\n        $(\"#showablelink179\").click(function(e){\n            e.preventDefault();\n            $(\"#showable179\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Put the answer to  5.13  into a file called  sample_1_dups.sh  (or whatever you want). Use  nano  to \ncreate the file.   Answer : the contents of the file will look like this:  #!/bin/bash\n\ncat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l  Note : the first line has special meaning.  If it starts with ' #! ' (Hash \nthen exclamation mark) then it tells bash this file is a script that can be interpreted.  The command \n(including full path) used to intepret the script is placed right after the magic code.  Give everyone execute permissions on the file with chmod:  $ chmod +x sample_1_dups.sh   You can run the script like so:  $ ./sample_1_dups.sh  If all goes well the script should behave in exactly the same way as the answer to 5.13.     \n    $(document).ready(function(){\n        $(\"#showablelink180\").click(function(e){\n            e.preventDefault();\n            $(\"#showable180\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file \nas a command line parameter.      Hint   Shell scripts can refer to command line arguments by their position using special variables called  $0 ,  $1 ,  $2  and so on.     More   $0  refers to the name of the script as it was called on the command line.  $1  refers to the first command line argument, and so on.     \n    $(document).ready(function(){\n        $(\"#showablelink183\").click(function(e){\n            e.preventDefault();\n            $(\"#showable183\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable183\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink183\").text(\"More\");\n            } else {\n                $(\"#showablelink183\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink182\").click(function(e){\n            e.preventDefault();\n            $(\"#showable182\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer   Copy the shell script from  5.14  into a new file:  $ cp sample_1_dups.sh fastq_dups.sh  Edit the new shell script file and change it to use the command line parameters:  #!/bin/bash\n\ncat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l  You can run the new script like so:  $ ./fastq_dups.sh sample_1.fastq  In the above example the script takes  sample_1.fastq  as input and prints the number of duplicated \nsequences as output.  A better Answer :  Ideally we would write our shell script to be more robust. At the moment it just assumes there \nwill be at least one command line argument. However, it would be better to check and produce an \nerror message if insufficient arguments were given:  #!/bin/bash\nif [ $# -eq 1 ]; then\n    cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l\nelse\n    echo  Usage: $0  fastq_filename \n    exit 1\nfi  The ' if ...; then ' line means: do the following line(s) ONLY if the  ...  (called condition) bit is true.  The ' else ' line means: otherwise do the following line(s) instead.  Note: it is optional.  The ' fi ' line means: this marks the end of the current  if  or  else  section.  The ' [ $# -eq 1 ] ' part is the condition:   $# : is a special shell variable that indicates how many command line arguments were given.   -eq : checks if the numbers on either side if it are equal.  1 : is a number one   Spaces in conditions :\nBash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely \n(without warning).  You MUST put a space near the share brackets and between each part of the condition!  So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\".  Exit-status :\nIt is a Unix standard that when the user provides incorrect commandline arguments we print a usage message \nand return a *non-zero* exit status.  The *exit status* is a standard way for other programs to know if\nour program ran correctly; 0 means everything went as expected, any other number is an error.  If you don't\nprovide an *exit ..* line then it automatically returns a 0 for you.     \n    $(document).ready(function(){\n        $(\"#showablelink184\").click(function(e){\n            e.preventDefault();\n            $(\"#showable184\").toggleClass(\"showable-hidden\");\n        });\n    });\n         5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the \ncommand line argument and outputs the number of duplicated DNA sequences in each file.      Answer   We can add a loop to our script to accept multiple input FASTQ files:  #!/bin/bash\nfor file in $@; do\n    dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l)\n    echo  $file $dups \ndone  There's a lot going on in this script.  The  $@  is a sequence of all command line arguments.  The ' for ...; do ' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in \nthe sequence to the variable called  file .  The  $(...)  allow us to capture the output of another command (in-place of the  ... ).  In this \ncase we capture the output of the pipeline and save it to the variable called  dups .  If you had multiple FASTQ files available you could run the script like so:  ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq  And it would produce output like:  sample_1.fastq 56079\nsample_2.fastq XXXXX\nsample_3.fastq YYYYY     \n    $(document).ready(function(){\n        $(\"#showablelink186\").click(function(e){\n            e.preventDefault();\n            $(\"#showable186\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Topic 5: Pipes, output redirection and shell scripts"
        }, 
        {
            "location": "/tutorials/unix/#finished", 
            "text": "Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it \nwas a lot to take in.  From here you should be confortable around the Unix command line and ready to take on the HPC \nWorkshop.  You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this \nWorkshop for later reference.  Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it\nback to the Workshop facilitators.", 
            "title": "Finished"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/", 
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\nLaunching a Personal GVL Server on the NeCTAR Research Cloud\n\n\n\n\nTutorial Overview\n\n\nThis document guides you to launch your own GVL Analysis platform, with Galaxy,\nusing your default NeCTAR allocation.\n\n\nThe tutorial will go over:\n\n\n\n\nAccessing the NeCTAR dashboard using Australian Access Federation (AAF)\n   credentials\n\n\nGetting your NeCTAR Research Cloud credentials\n\n\nLaunching the GVL image\n\n\nAccessing your GVL instance\n\n\nGVL services\n\n\nShutting your machine down\n\n\n\n\n\n\nBackground\n\n\nWhat is the GVL?\n\n\nThe Genomics Virtual Laboratory (GVL) is a research platform that can be\ndeployed on the cloud.\n\n\n\n\nA private GVL server is a virtual machine running on the cloud and contains a\npre-installed suite of tools for performing bioinformatics analyses. It differs\nfrom public GVL servers (such as the\n\nGalaxy Tutorial Server\n,\n\nGalaxy Melbourne\n, and\n\nGalaxy Queensland\n) by providing full\nadministrative access to the server, as well as the full suite of GVL services,\nwhereas public GVL servers provide restricted access for security reasons.\nFor example, public GVL servers do not provide access to the Ubuntu desktop,\nthe Linux command line or JupyterHub at present.\n\n\nAccessing the GVL server is completely free on the Australian NeCTAR Research\nCloud, provided that you have a account with NeCTAR with allocated resources.\n\n\nWhat is NeCTAR?\n\n\nThe \nNational eResearch Collaboration Tools and Resources project\n\n(NeCTAR) is an Australian programme that provides computing infrastructure\nand services to Australian researchers. The NeCTAR Cloud allows us to\ndeploy virtual machines as a platform for research.\n\n\nWhile it is possible to launch the GVL on \nAmazon\n,\nyou may have to pay Amazon usage charges (the GVL software itself is free).\n\n\n\n\nSection 1: Access the NeCTAR dashboard\n\n\n\n\n\n\nLogin into the NeCTAR dashboard at\n    \ndashboard.rc.nectar.org.au\n.\n\n    Only members of the Australian Access Federation (AAF) can access Australian\n    Research Cloud resources. Most Australian universities are members of the\n    AAF, however, if you belong to an institution that is not a member of AAF,\n    \nGVL Help\n may be able to provide you\n    with credentials. You also have the option of using a commercial cloud\n    provider such as \nAWS\n to host your server.\n\n\n\n\n\n\nChoose your organisation from the list and login using your credentials.\n    If you are from the University of Melbourne, choose 'The University of\n    Melbourne' and not 'The University of Melbourne (with ECP)'.\n\n\n\n\n\n\nLogin with your institutional username and password. If this is the first\n    time you have accessed the Australian Research Cloud, you must agree to some\n    terms and conditions.\n\n\nWhen you log in for the first time, you are automatically allocated a trial\nproject which lasts for 3 months. This trial project allows you to launch a\nmedium instance (with 2 cores) which is sufficient for launching a GVL\ninstance.\n\n\n\n\nIf you have projects which require more compute resources, you can apply\nfor more allocation \nhere\n.\n\n\n\n\n\n\n\n\nSection 2: Get your cloud credentials\n\n\nLaunching a GVL instance requires EC2 API keys from NeCTAR. Obtaining these keys\nis a simple 3 step process.\n\n\n\n\n\n\n\n\nFrom the NeCTAR dashboard, on the left sidebar, navigate to\n\n    Project \n Compute \n Access \n Security\n\n\n\n\n\n\nClick on the top \n'API Access'\n tab.\n\n\n\n\n\n\nClick on the \n'+ View Credentials'\n button on the top right. A window\n    containing your API access key (1) and your secret key (2) will appear.\n    Click on the eye icon to view your secret key. Keep this key secret and\n    secure.\n\n\n\n\nKeep this page open for later reference.\n\n\n\n\n\n\n\n\nSection 3: Launch your personal GVL instance\n\n\n\n\n\n\nIn a new browser tab, go to \nlaunch.genome.edu.au\n\n\n\n\n\n\nFill in the required fields:\n\n\n\n\nCloud:\n Keep the default: NeCTAR (OpenStack)\n\n\nAccess key / Secret key:\n Copy and paste your access key and your secret\n  key you obtained in the previous step in the required fields.\n\n\nInstitutional email:\n Enter your institutional email address\n\n\nCluster name:\n Choose 'Specify a new name' and enter a name for your\n  instance (eg. GVL_workshop). It is recommended you choose a unique name\n  if you launch multiple instances.\n\n\nPassword:\n Choose a strong password and remember it. This is the\n  password you will use later to log into your instance.\n\n\nInstance type:\n Keep the default: Medium (2 vcpu / 8GB RAM)\n\n\nCluster type:\n Keep the default: Cluster with Galaxy\n\n\nStorage type:\n Keep the default: Transient instance storage\n\n\n\n\n\n\n\n\n\n\nOptional advanced options\n\n    Toggle the 'Show advanced startup options' option to see more options. For\n    this tutorial, it is not necessary to modify any of the advanced options.\n\n\n\n\nPlacement:\n You can also choose the region of where your server is\n  hosted. If you are doing lots of data transfer, it may be beneficial to\n  pick a location close to your physical location. More information about\n  zones can be found \nhere\n.\n\n\nKey pair:\n Key pairs are SSH credentials that can be used to access\n  your instance. You can create and import key pairs in the NeCTAR dashboard\n  by navigating to Project \n Compute \n Access \n Security \n Key Pairs and\n  creating or importing a key pair.\n\n\nImage:\n The image to start. The default option is usually the latest,\n  most stable GVL image.\n\n\nFlavor:\n Flavours are versions of the GVL with slightly customised\n  toolsets. These toolsets are optimised for different usage patterns. More\n  information about the different flavours can be found\n  \nhere\n. For this tutorial, keep\n  the default option of 'GVL + Tutorial Indices'\n\n\n\n\n\n\n\n\n\n\nClick \n'Create a cluster'\n to launch the image.\n\n    The launch process takes 2-5 minutes to start the machine and another 5\n    minutes to start and configure Galaxy.\n\n\n\n\nIf your progress bar seems stuck on the 'Requesting' stage for \n 5 minutes,\nnavigate back to the launch page and try selecting a different availability\nzone under the advanced startup options \nPlacement\n field. You will need\nre-enter your secret key and your password.\n\n\n\n\n\n\n\n\nSection 4: Access your GVL instance\n\n\n\n\n\n\nOnce your instance has finished launching, click on the cluster IP address\n    to access your GVL dashboard.\n\n\n\n\nIf you accidentally closed the launch page, you can find your cluster's\naddress on the \nNeCTAR dashboard\n\nby navigating to Project \n Compute \n Instances on the left panel. This page\ncontains a list of your instances and can be used to terminate your instance\nif anything goes wrong. Copy and paste the instance's IP address into your\nbrowser's URL navigation bar.\n\n\n\n\n\n\nExplore the GVL dashboard.\n    Have a read through of the services provided by the GVL.\n\n\n\n\n\n\n\n\n\n\nSection 5: GVL services\n\n\nListed below are short descriptions of the services the GVL provides.\n\n\nGalaxy\n\n\nGalaxy\n is a web-based platform for computational\nbiomedical research. The GVL has a number of Galaxy tutorials available\n\nhere\n.\n\n\n\n\nTo begin using Galaxy, register as a user by navigating to User \n Register\non the top Galaxy bar.\n\n\nCloudMan\n\n\nCloudMan is a cloud manager that manages services on your GVL instance. Use\nCloudman to start and manage your Galaxy service and to add additional nodes\nto your compute cluster (if you have enough resources).\n\n\nYou can log into CloudMan by using the username 'ubuntu' and your cluster\npassword.\n\n\n\n\nYou can also shut down your instance (permanently) with CloudMan.\n\n\nLubuntu Desktop\n\n\nLubuntu is a lightweight desktop environment through which you can run desktop\napplications on your virtual machine through your web browser. You can also\naccess the GVL command line utilities through the desktop.\n\n\nYou can log into Lubuntu Desktop using the username 'ubuntu' and your cluster\npassword.\n\n\n\n\nSSH\n\n\nSecure Shell (SSH) is a network protocol that allows us to connect to a remotely\nmachine. You can login to your virtual machine remotely through an SSH client.\n\n\nIf you are using Windows, you will need to download an SSH client such as\n\nPuTTY\n. If you are using\nOSX, open up a Terminal window.\n\n\nIf you are unfamiliar with the command line and UNIX, many\n\ntutorials\n\n\non UNIX\n\n\ncan be\n\n\nfound online\n.\n\n\nYou can ssh into your machine using the either the username 'ubuntu' or the\nusername 'researcher' and using your cluster password. It is recommended to use\nthe researcher account when you are doing your computational research and use\nthe ubuntu account when you need administrative powers (such as installing\nsoftware).\n\n\n\n\nJupyterHub\n\n\nJupyterHub\n is a web-based interactive computational\nenvironment where you can combine code execution, text, mathematics, plots and\nrich media into a single document. Currently, JupyterHub can connect to Python2\nand Python3 kernals.\n\n\nIf you are unfamiliar with Python, there are many\n\ntutorials\n\n\navailable\n\n\nonline\n.\n\n\nYou can log into JupyterHub with the username 'researcher' and your cluster\npassword.\n\n\nYou may need to install Python packages you intend to use via the command line\nbeforehand.\n\n\n\n\nRStudio\n\n\nRStudio Server gives browser-based access to RStudio, the popular programming\nand analysis environment for the R programming language. You can find out more\nabout RStudio \nhere\n, and the R programming\nlanguage \nhere\n.\n\n\nYou can log into RStudio with the username 'researcher' and your cluster\npassword.\n\n\n\n\nPublic HTML\n\n\nThis is a shared web-accessible folder. Any files you place in the directory\n\n/home/researcher/public_html\n will be publicly accessible.\n\n\nPacBio SMRT Portal\n\n\nPacBio's SMRT Portal\n is an open source software suite for the analysis of single molecule, real-time sequencing data.\n\n\nBefore you use SMRT Portal, you need to firstly install it through the Admin console. \nPlease note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package.\n\n\nTo install SMRT Portal from the GVL dashboard:\n\n\n\n\n\n\nClick on 'Admin' in the top navigation bar.\n\n\n\n\n\n\nLog in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed.\n\n\n\n\n\n\nScroll down to 'SMRT Analysis', and click 'install'.\n\n\n\n\n\n\nThe install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed.\n\n\n\n\n\n\nWhen launching SMRT Portal for the first time, you will need to register yourself as a new user.\n\n\n\n\nSection 6: Shutting your machine down\n\n\nThere are two ways to terminate your instance. Terminating your instance is\npermanent and all data will be deleted (unless you have persistent\n\nvolume storage\n which you will\nneed to apply for).\n\n\n\n\n\n\nVia Cloudman\n\n    Log into CloudMan by using the username 'ubuntu' and your cluster password.\n    Click the \nShut down...\n button under Cluster Controls.\n\n\n\n\n\n\nVia the NeCTAR dashboard\n\n    Navigate to the \nInstances\n\n    page by navigating to Project \n Compute \n Instances on the left panel. Find\n    the instance you want to terminate, and on the right-most column (Actions),\n    click on the arrow button, and select \nTerminate Instance\n.", 
            "title": "Launching a Personal GVL Server"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#launching-a-personal-gvl-server-on-the-nectar-research-cloud", 
            "text": "", 
            "title": "Launching a Personal GVL Server on the NeCTAR Research Cloud"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#tutorial-overview", 
            "text": "This document guides you to launch your own GVL Analysis platform, with Galaxy,\nusing your default NeCTAR allocation.  The tutorial will go over:   Accessing the NeCTAR dashboard using Australian Access Federation (AAF)\n   credentials  Getting your NeCTAR Research Cloud credentials  Launching the GVL image  Accessing your GVL instance  GVL services  Shutting your machine down", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#background", 
            "text": "", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#what-is-the-gvl", 
            "text": "The Genomics Virtual Laboratory (GVL) is a research platform that can be\ndeployed on the cloud.   A private GVL server is a virtual machine running on the cloud and contains a\npre-installed suite of tools for performing bioinformatics analyses. It differs\nfrom public GVL servers (such as the Galaxy Tutorial Server , Galaxy Melbourne , and Galaxy Queensland ) by providing full\nadministrative access to the server, as well as the full suite of GVL services,\nwhereas public GVL servers provide restricted access for security reasons.\nFor example, public GVL servers do not provide access to the Ubuntu desktop,\nthe Linux command line or JupyterHub at present.  Accessing the GVL server is completely free on the Australian NeCTAR Research\nCloud, provided that you have a account with NeCTAR with allocated resources.", 
            "title": "What is the GVL?"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#what-is-nectar", 
            "text": "The  National eResearch Collaboration Tools and Resources project \n(NeCTAR) is an Australian programme that provides computing infrastructure\nand services to Australian researchers. The NeCTAR Cloud allows us to\ndeploy virtual machines as a platform for research.  While it is possible to launch the GVL on  Amazon ,\nyou may have to pay Amazon usage charges (the GVL software itself is free).", 
            "title": "What is NeCTAR?"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-1-access-the-nectar-dashboard", 
            "text": "Login into the NeCTAR dashboard at\n     dashboard.rc.nectar.org.au . \n    Only members of the Australian Access Federation (AAF) can access Australian\n    Research Cloud resources. Most Australian universities are members of the\n    AAF, however, if you belong to an institution that is not a member of AAF,\n     GVL Help  may be able to provide you\n    with credentials. You also have the option of using a commercial cloud\n    provider such as  AWS  to host your server.    Choose your organisation from the list and login using your credentials.\n    If you are from the University of Melbourne, choose 'The University of\n    Melbourne' and not 'The University of Melbourne (with ECP)'.    Login with your institutional username and password. If this is the first\n    time you have accessed the Australian Research Cloud, you must agree to some\n    terms and conditions.  When you log in for the first time, you are automatically allocated a trial\nproject which lasts for 3 months. This trial project allows you to launch a\nmedium instance (with 2 cores) which is sufficient for launching a GVL\ninstance.   If you have projects which require more compute resources, you can apply\nfor more allocation  here .", 
            "title": "Section 1: Access the NeCTAR dashboard"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-2-get-your-cloud-credentials", 
            "text": "Launching a GVL instance requires EC2 API keys from NeCTAR. Obtaining these keys\nis a simple 3 step process.     From the NeCTAR dashboard, on the left sidebar, navigate to \n    Project   Compute   Access   Security    Click on the top  'API Access'  tab.    Click on the  '+ View Credentials'  button on the top right. A window\n    containing your API access key (1) and your secret key (2) will appear.\n    Click on the eye icon to view your secret key. Keep this key secret and\n    secure.   Keep this page open for later reference.", 
            "title": "Section 2: Get your cloud credentials"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-3-launch-your-personal-gvl-instance", 
            "text": "In a new browser tab, go to  launch.genome.edu.au    Fill in the required fields:   Cloud:  Keep the default: NeCTAR (OpenStack)  Access key / Secret key:  Copy and paste your access key and your secret\n  key you obtained in the previous step in the required fields.  Institutional email:  Enter your institutional email address  Cluster name:  Choose 'Specify a new name' and enter a name for your\n  instance (eg. GVL_workshop). It is recommended you choose a unique name\n  if you launch multiple instances.  Password:  Choose a strong password and remember it. This is the\n  password you will use later to log into your instance.  Instance type:  Keep the default: Medium (2 vcpu / 8GB RAM)  Cluster type:  Keep the default: Cluster with Galaxy  Storage type:  Keep the default: Transient instance storage      Optional advanced options \n    Toggle the 'Show advanced startup options' option to see more options. For\n    this tutorial, it is not necessary to modify any of the advanced options.   Placement:  You can also choose the region of where your server is\n  hosted. If you are doing lots of data transfer, it may be beneficial to\n  pick a location close to your physical location. More information about\n  zones can be found  here .  Key pair:  Key pairs are SSH credentials that can be used to access\n  your instance. You can create and import key pairs in the NeCTAR dashboard\n  by navigating to Project   Compute   Access   Security   Key Pairs and\n  creating or importing a key pair.  Image:  The image to start. The default option is usually the latest,\n  most stable GVL image.  Flavor:  Flavours are versions of the GVL with slightly customised\n  toolsets. These toolsets are optimised for different usage patterns. More\n  information about the different flavours can be found\n   here . For this tutorial, keep\n  the default option of 'GVL + Tutorial Indices'      Click  'Create a cluster'  to launch the image. \n    The launch process takes 2-5 minutes to start the machine and another 5\n    minutes to start and configure Galaxy.   If your progress bar seems stuck on the 'Requesting' stage for   5 minutes,\nnavigate back to the launch page and try selecting a different availability\nzone under the advanced startup options  Placement  field. You will need\nre-enter your secret key and your password.", 
            "title": "Section 3: Launch your personal GVL instance"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-4-access-your-gvl-instance", 
            "text": "Once your instance has finished launching, click on the cluster IP address\n    to access your GVL dashboard.   If you accidentally closed the launch page, you can find your cluster's\naddress on the  NeCTAR dashboard \nby navigating to Project   Compute   Instances on the left panel. This page\ncontains a list of your instances and can be used to terminate your instance\nif anything goes wrong. Copy and paste the instance's IP address into your\nbrowser's URL navigation bar.    Explore the GVL dashboard.\n    Have a read through of the services provided by the GVL.", 
            "title": "Section 4: Access your GVL instance"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-5-gvl-services", 
            "text": "Listed below are short descriptions of the services the GVL provides.", 
            "title": "Section 5: GVL services"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#galaxy", 
            "text": "Galaxy  is a web-based platform for computational\nbiomedical research. The GVL has a number of Galaxy tutorials available here .   To begin using Galaxy, register as a user by navigating to User   Register\non the top Galaxy bar.", 
            "title": "Galaxy"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#cloudman", 
            "text": "CloudMan is a cloud manager that manages services on your GVL instance. Use\nCloudman to start and manage your Galaxy service and to add additional nodes\nto your compute cluster (if you have enough resources).  You can log into CloudMan by using the username 'ubuntu' and your cluster\npassword.   You can also shut down your instance (permanently) with CloudMan.", 
            "title": "CloudMan"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#lubuntu-desktop", 
            "text": "Lubuntu is a lightweight desktop environment through which you can run desktop\napplications on your virtual machine through your web browser. You can also\naccess the GVL command line utilities through the desktop.  You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster\npassword.", 
            "title": "Lubuntu Desktop"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#ssh", 
            "text": "Secure Shell (SSH) is a network protocol that allows us to connect to a remotely\nmachine. You can login to your virtual machine remotely through an SSH client.  If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using\nOSX, open up a Terminal window.  If you are unfamiliar with the command line and UNIX, many tutorials  on UNIX  can be  found online .  You can ssh into your machine using the either the username 'ubuntu' or the\nusername 'researcher' and using your cluster password. It is recommended to use\nthe researcher account when you are doing your computational research and use\nthe ubuntu account when you need administrative powers (such as installing\nsoftware).", 
            "title": "SSH"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#jupyterhub", 
            "text": "JupyterHub  is a web-based interactive computational\nenvironment where you can combine code execution, text, mathematics, plots and\nrich media into a single document. Currently, JupyterHub can connect to Python2\nand Python3 kernals.  If you are unfamiliar with Python, there are many tutorials  available  online .  You can log into JupyterHub with the username 'researcher' and your cluster\npassword.  You may need to install Python packages you intend to use via the command line\nbeforehand.", 
            "title": "JupyterHub"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#rstudio", 
            "text": "RStudio Server gives browser-based access to RStudio, the popular programming\nand analysis environment for the R programming language. You can find out more\nabout RStudio  here , and the R programming\nlanguage  here .  You can log into RStudio with the username 'researcher' and your cluster\npassword.", 
            "title": "RStudio"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#public-html", 
            "text": "This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html  will be publicly accessible.", 
            "title": "Public HTML"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#pacbio-smrt-portal", 
            "text": "PacBio's SMRT Portal  is an open source software suite for the analysis of single molecule, real-time sequencing data.  Before you use SMRT Portal, you need to firstly install it through the Admin console.  Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package.  To install SMRT Portal from the GVL dashboard:    Click on 'Admin' in the top navigation bar.    Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed.    Scroll down to 'SMRT Analysis', and click 'install'.    The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed.    When launching SMRT Portal for the first time, you will need to register yourself as a new user.", 
            "title": "PacBio SMRT Portal"
        }, 
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-6-shutting-your-machine-down", 
            "text": "There are two ways to terminate your instance. Terminating your instance is\npermanent and all data will be deleted (unless you have persistent volume storage  which you will\nneed to apply for).    Via Cloudman \n    Log into CloudMan by using the username 'ubuntu' and your cluster password.\n    Click the  Shut down...  button under Cluster Controls.    Via the NeCTAR dashboard \n    Navigate to the  Instances \n    page by navigating to Project   Compute   Instances on the left panel. Find\n    the instance you want to terminate, and on the right-most column (Actions),\n    click on the arrow button, and select  Terminate Instance .", 
            "title": "Section 6: Shutting your machine down"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/", 
            "text": ".image-header-gvl {\n    -webkit-column-count: 3;\n    -moz-column-count: 3;\n    column-count: 3;\n\n    -webkit-column-gap: 140px;\n    -moz-column-gap: 140px;\n    column-gap: 140px;\n  }\n\n\n\n\n\n    \n\n    \n\n    \n\n\n\n\n\nIntroduction to Galaxy\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nBackground\n\n\nGalaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc.\n\n\nThere are some introductory slides available \nhere\n.\n\n\nBasically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial.\n\n\n\n\nThis workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics:\n\n\n\n\nLogging in to the server\n\n\nGetting data into galaxy\n\n\nHow to access the tools\n\n\nUsing to use some common tools\n\n\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe able to register on and login to a Galaxy server.\n\n\nBe able to upload data to a Galaxy server from:\n\n\nA file on your local computer\n\n\nA file on a remote datastore with an accessible URL.\n\n\n\n\n\n\nBe able use tools in Galaxy by:\n\n\nAccessing the tool via the tool menu\n\n\nUsing the tool interface to run the particular tool\n\n\nViewing/accessing the tool output.\n\n\n\n\n\n\n\n\n\n\nSection 1: Preparation.\n\n\nThe purpose of this section is to get you to log in to the server..\n\n\n\n\n\n\nGo to the ip address of your GVL Galaxy server (or if you don\u2019t have one, open \nGalaxy-Tut\n server) in Firefox or Chrome (your choice) - Please don\u2019t use Internet Explorer or Safari.\n\n\n\n\n\n\nIf you have previously registered on this server just log in:\n\n\n\n\nOn the top menu select: \nUser -\n Login\n\n\nEnter your password\n\n\nClick \nSubmit\n\n\n\n\n\n\n\n\nIf you haven\u2019t registered on this server, you\u2019ll need to now.\n\n\n\n\nOn the top menu select: \nUser -\n Register\n\n\nEnter your email, choose a password, repeat it and add a (all lower case) one word name\n\n\nClick \nSubmit\n\n\n\n\n\n\n\n\n\n\nSection 2: Getting data into Galaxy\n\n\nThere are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then work use those 3 files for the rest of the workshop.\n\n\n\n\n\n\nStart a new history for this workshop. To do this:\n\n\n\n\nClick on the history menu button (the \n icon) at the top of the Histories panel.\n\n\nSelect \nCreate New\n\n\n\n\n\n\n\n\nIt is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc.\n\n\nWe will tell Galaxy what type of file each one is as we upload it.\n\n\nMethod 1: Upload a file from your own computer\n\n\nWith this method you can get most of the files on your own computer into Galaxy. (there is a size limit)\n\n\n\n\n\n\nDownload the following file to your computer: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz\n\n\n\n\nFrom the Galaxy tool panel, click on \nGet Data -\n Upload File\n\n\nClick the \nChoose File\n button\n\n\nFind and select the \nContig_stats.txt.gz\n file you downloaded and click \nOpen\n\n\nSet the \"file format\" to \ntabular\n\n\nClick the \nStart\n button\n\n\nOnce the progress bar reaches 100%, click the \nClose\n button\n\n\n\n\n\n\n\n\nThe file will now upload to your current history.\n\n\nMethod 2: Upload a file from a URL\n\n\nIf a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy.\n\n\n\n\n\n\nFrom the tool panel, click on \nGet Data -\n Upload File\n\n\n\n\nClick on the \nPaste/Fetch Data\n button\n\n\nCopy and paste the following web address into the URL/Text box: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz\n\n\nSet the file format to \nfastqsanger\n (not fastqcsanger)\n\n\nClick \nStart\n\n\nOnce the progress bar has reached 100%, click \nClose\n\n\n\n\n\n\n\n\nNote that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.\n\n\nMethod 2 (again): Get data from a public URL\n\n\nNow we are going to upload another file from the remote data source.\n\n\nRepeat the above for: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna\n\n\nNote that this file is a \nfasta\n file and not a \nfastqsanger\n file.\n\n\n\n\n\n\nReveal detailed instructions\n\n\n\n\n\n\n\n\nFrom the tool panel, click on \nGet Data -\n Upload File\n\n\n\n\nClick on the \nPaste/Fetch Data\n button\n\n\nCopy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna\n\n\nSet the file format to \nfasta\n.\n\n\nClick \nStart\n\n\nOnce the progress bar has reached 100%, click \nClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nThe DNA sequence of \nStaphlococcus aureus MRSA252\n will be loaded into your history as a fasta file.\n\n\nYour history should now look like this.\n\n\n\n\nThe data\n\n\nThough we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one.\n\n\n\n\n\n\nContigs_stats.txt\n\n\n\n\nthis file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. )\n\n\nThe columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbacterial_std_err_1.fastq.gz\n\n\n\n\nThis file contains sequence reads as they would come off an Illunina sequencing machine. They are in \nfastq\n format.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMRSA0252.fna\n\n\n\n\nThis file contains the genome sequence of \nStaphylococcus aureus MRSA252\n. It is in \nfasta\n format.\n\n\n\n\n\n\n\n\n\n\nSection 3: Play with the tools\n\n\nThe purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools.\n\n\nFirstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So:\n\n\n\n\nClick on the \n icon (edit) next to the file in the history called: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq\n\n\nIn the \"Name\" text box, give it a new name. Call it: \nTypical Fastq File\n\n\nClick the \nSave\n button.\n\n\n\n\nRepeat the process for the MRSA252 fasta file. Rename it to \nMRSA252.fna\n\n\nNow that\u2019s better. There was a lot of other functionality hidden behind that edit (\n) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date.\n\n\nOk, back to the tools..\n\n\nExample 1: Histogram and summary statistics\n\n\nThe first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools.\n\n\nClick on the \n icon of the \nContig_stats.txt\n file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this:\n\n\n1. Cut out column 1 and column 6.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -\n Cut\n and set the following:\n\n\nSet \"Cut Columns\" to: \nc1,c6\n\n\n\"Delimited by\": \nTab\n\n\n\"Cut from\": \nContig_stats.txt\n\n\nClick \nExecute\n\n\n\n\nExamine the new file by clicking on it\u2019s \n icon. We now have 2 columns instead of the 18 in the original file.\n\n\n2. Remove the Header lines of the new file.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -\n Remove beginning\n and set the following:\n\n\n\"Remove First\": \n1\n\n\n\"from\": \nCut on data1\n\n\nclick \nExecute\n\n\n\n\nNote the the new file is the same as the previous one without the header line.\n\n\n3. Make a histogram.\n\n\n\n\nFrom the tool panel, click on \nGraph/Display Data -\n Histogram\n and set the following:\n\n\n\"Dataset\": \nRemove beginning on Data X\n\n\n\"Numerical column for X axis\": \nc2\n\n\n\"Number of breaks\": \n25\n\n\n\"Plot title\": \nHistogram of Contig Coverage\n\n\n\"Label for X axis\": \nCoverage depth\n\n\nClick \nExecute\n\n\n\n\nClick on the \n icon of the histogram to have a look at it. Note there are a few peaks.. Maybe these correspond to single, double and triple copy number of these contigs.\n\n\n4. Calculate summary statistics for contig coverage depth.\n\n\n\n\nFrom the tool panel, click on \nStatistics -\n Summary Statisitics\n and set the following:\n\n\n\"Summary statistics on\": \nRemove beginning on Data X\n\n\n\"Column or expression\": \nc2\n\n\nClick \nExecute\n\n\n\n\nYou\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug \n symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator.\n\n\nExample 2: Convert Fastq to Fasta\n\n\nThis shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data.\n\n\nConverter tool\n\n\n\n\nFrom the tool panel, click on \nConvert Formats -\n FASTQ to FASTA\n and set the following:\n\n\n\"FASTQ file to convert\": \nTypical Fastq File\n\n\nClick \nExecute\n\n\n\n\nThis will have created a new Fasta file called FASTQ to FASTA on data 2.\n\n\nExample 3: Find Ribosomal RNA Features in a DNA Sequence\n\n\nThis example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence.\n\n\n1. Find all of the ribosomal RNA's in a sequence\n\n\n\n\nFrom the tool panel, click on \nAnnotation -\n barrnap\n and set the following:\n\n\n\"Fasta file\": MRSA252.fna\n\n\nClick \nExecute\n\n\n\n\nA new file called \nbarrnap on data 3\n will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change it\u2019s name to something more appropriate (click on the \n icon.) There is also a STDERR output file from this tool - just ignore this one.\n\n\nNow lets say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool.\n\n\n2. Filter the annotations to get the 23S RNAs\n\n\n\n\nFrom the tool panel, click on \nFilter and Sort -\n Select\n and set the following:\n\n\n\"Select lines from\":  (whatever you called the barrnap gff3 output)\n\n\n\"the pattern\": \n23S\n     (this will look for all the lines in the file that contain \u201c23S\u201d)\n\n\nClick \nExecute\n\n\n\n\nNow you have a gff3 file with just the 23S annotations!\n\n\nWhat now?\n\n\nRemember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button \n at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server.\n\n\nThat's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see \nhttp://genome.edu.au/learn", 
            "title": "Introduction to Galaxy"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#introduction-to-galaxy", 
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)", 
            "title": "Introduction to Galaxy"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#background", 
            "text": "Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc.  There are some introductory slides available  here .  Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial.   This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics:   Logging in to the server  Getting data into galaxy  How to access the tools  Using to use some common tools", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#learning-objectives", 
            "text": "At the end of this tutorial you should:   Be able to register on and login to a Galaxy server.  Be able to upload data to a Galaxy server from:  A file on your local computer  A file on a remote datastore with an accessible URL.    Be able use tools in Galaxy by:  Accessing the tool via the tool menu  Using the tool interface to run the particular tool  Viewing/accessing the tool output.", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-1-preparation", 
            "text": "The purpose of this section is to get you to log in to the server..    Go to the ip address of your GVL Galaxy server (or if you don\u2019t have one, open  Galaxy-Tut  server) in Firefox or Chrome (your choice) - Please don\u2019t use Internet Explorer or Safari.    If you have previously registered on this server just log in:   On the top menu select:  User -  Login  Enter your password  Click  Submit     If you haven\u2019t registered on this server, you\u2019ll need to now.   On the top menu select:  User -  Register  Enter your email, choose a password, repeat it and add a (all lower case) one word name  Click  Submit", 
            "title": "Section 1: Preparation."
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-2-getting-data-into-galaxy", 
            "text": "There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then work use those 3 files for the rest of the workshop.    Start a new history for this workshop. To do this:   Click on the history menu button (the   icon) at the top of the Histories panel.  Select  Create New     It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc.  We will tell Galaxy what type of file each one is as we upload it.", 
            "title": "Section 2: Getting data into Galaxy"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-1-upload-a-file-from-your-own-computer", 
            "text": "With this method you can get most of the files on your own computer into Galaxy. (there is a size limit)    Download the following file to your computer:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz   From the Galaxy tool panel, click on  Get Data -  Upload File  Click the  Choose File  button  Find and select the  Contig_stats.txt.gz  file you downloaded and click  Open  Set the \"file format\" to  tabular  Click the  Start  button  Once the progress bar reaches 100%, click the  Close  button     The file will now upload to your current history.", 
            "title": "Method 1: Upload a file from your own computer"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-2-upload-a-file-from-a-url", 
            "text": "If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy.    From the tool panel, click on  Get Data -  Upload File   Click on the  Paste/Fetch Data  button  Copy and paste the following web address into the URL/Text box:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz  Set the file format to  fastqsanger  (not fastqcsanger)  Click  Start  Once the progress bar has reached 100%, click  Close     Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.", 
            "title": "Method 2: Upload a file from a URL"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-2-again-get-data-from-a-public-url", 
            "text": "Now we are going to upload another file from the remote data source.  Repeat the above for:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna  Note that this file is a  fasta  file and not a  fastqsanger  file.    Reveal detailed instructions     From the tool panel, click on  Get Data -  Upload File   Click on the  Paste/Fetch Data  button  Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna  Set the file format to  fasta .  Click  Start  Once the progress bar has reached 100%, click  Close        (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)  \n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n      The DNA sequence of  Staphlococcus aureus MRSA252  will be loaded into your history as a fasta file.  Your history should now look like this.", 
            "title": "Method 2 (again): Get data from a public URL"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#the-data", 
            "text": "Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one.    Contigs_stats.txt   this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. )  The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly.          bacterial_std_err_1.fastq.gz   This file contains sequence reads as they would come off an Illunina sequencing machine. They are in  fastq  format.          MRSA0252.fna   This file contains the genome sequence of  Staphylococcus aureus MRSA252 . It is in  fasta  format.", 
            "title": "The data"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-3-play-with-the-tools", 
            "text": "The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools.  Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So:   Click on the   icon (edit) next to the file in the history called:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq  In the \"Name\" text box, give it a new name. Call it:  Typical Fastq File  Click the  Save  button.   Repeat the process for the MRSA252 fasta file. Rename it to  MRSA252.fna  Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date.  Ok, back to the tools..", 
            "title": "Section 3: Play with the tools"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-1-histogram-and-summary-statistics", 
            "text": "The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools.  Click on the   icon of the  Contig_stats.txt  file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this:  1. Cut out column 1 and column 6.   From the tool panel, click on  Text Manipulation -  Cut  and set the following:  Set \"Cut Columns\" to:  c1,c6  \"Delimited by\":  Tab  \"Cut from\":  Contig_stats.txt  Click  Execute   Examine the new file by clicking on it\u2019s   icon. We now have 2 columns instead of the 18 in the original file.  2. Remove the Header lines of the new file.   From the tool panel, click on  Text Manipulation -  Remove beginning  and set the following:  \"Remove First\":  1  \"from\":  Cut on data1  click  Execute   Note the the new file is the same as the previous one without the header line.  3. Make a histogram.   From the tool panel, click on  Graph/Display Data -  Histogram  and set the following:  \"Dataset\":  Remove beginning on Data X  \"Numerical column for X axis\":  c2  \"Number of breaks\":  25  \"Plot title\":  Histogram of Contig Coverage  \"Label for X axis\":  Coverage depth  Click  Execute   Click on the   icon of the histogram to have a look at it. Note there are a few peaks.. Maybe these correspond to single, double and triple copy number of these contigs.  4. Calculate summary statistics for contig coverage depth.   From the tool panel, click on  Statistics -  Summary Statisitics  and set the following:  \"Summary statistics on\":  Remove beginning on Data X  \"Column or expression\":  c2  Click  Execute   You\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug   symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator.", 
            "title": "Example 1: Histogram and summary statistics"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-2-convert-fastq-to-fasta", 
            "text": "This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data.  Converter tool   From the tool panel, click on  Convert Formats -  FASTQ to FASTA  and set the following:  \"FASTQ file to convert\":  Typical Fastq File  Click  Execute   This will have created a new Fasta file called FASTQ to FASTA on data 2.", 
            "title": "Example 2: Convert Fastq to Fasta"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-3-find-ribosomal-rna-features-in-a-dna-sequence", 
            "text": "This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence.  1. Find all of the ribosomal RNA's in a sequence   From the tool panel, click on  Annotation -  barrnap  and set the following:  \"Fasta file\": MRSA252.fna  Click  Execute   A new file called  barrnap on data 3  will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change it\u2019s name to something more appropriate (click on the   icon.) There is also a STDERR output file from this tool - just ignore this one.  Now lets say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool.  2. Filter the annotations to get the 23S RNAs   From the tool panel, click on  Filter and Sort -  Select  and set the following:  \"Select lines from\":  (whatever you called the barrnap gff3 output)  \"the pattern\":  23S      (this will look for all the lines in the file that contain \u201c23S\u201d)  Click  Execute   Now you have a gff3 file with just the 23S annotations!", 
            "title": "Example 3: Find Ribosomal RNA Features in a DNA Sequence"
        }, 
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#what-now", 
            "text": "Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button   at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server.  That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see  http://genome.edu.au/learn", 
            "title": "What now?"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/", 
            "text": ".image-header-gvl {\n    -webkit-column-count: 3;\n    -moz-column-count: 3;\n    column-count: 3;\n\n    -webkit-column-gap: 140px;\n    -moz-column-gap: 140px;\n    column-gap: 140px;\n  }\n\n\n\n\n\n    \n\n    \n\n    \n\n\n\n\n\nGalaxy Workflows\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nBackground\n\n\nThis workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics:\n\n\n\n\nLogging in to the server\n\n\nHow to construct and use a workflow by various methods\n\n\nHow to share a workflow\n\n\n\n\n\n\nSection 1: Preparation.\n\n\nThe purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: \nLaunch a GVL Galaxy Instance\n or you can use the public \nGalaxy-mel\n\n\nGo to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.)\n\n\n\n\nIf you have previously registered on this server just log in:\n\n\nOn the top menu select: \nUser -\n Login\n\n\nEnter your password\n\n\nClick the \nSubmit\n button\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you haven\u2019t registered on this server, you\u2019ll need to now.\n\n\nOn the top menu select: \nUser -\n Register\n\n\nEnter your email, choose a password, repeat it and add a (all lower case) one word name\n\n\nClick on the \nSubmit\n button.\n\n\n\n\n\n\n\n\n\n\nSection 2: Create and run a workflow.\n\n\nThis section will show you two different methods to create a workflow and then how to run one.\n\n\nImport the workflow history\n\n\nIn this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it \nhere\n. Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\"\n\n\n\n\nFrom the menu at the top of the Galaxy window, click \nShared Data -\n Histories\n\n\nFind the history called \"\nworkflow_finished\n\" and click on it.\n\n\nThen click on \nImport history\n at the top right of the screen.\n\n\nChange the name if you wish and then click \nImport\n\n\n\n\nThis history should now be in your history pane on the right.\n\n\n\n\n\nWorkflow creation: Method 1\n\n\nWe will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow.\n\n\nMake sure your current history is the one you imported in Section 2 - Step 1 above (\nimported: workflow_finished.\n) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history.\n\n\nNow we will create the workflow.\n\n\n\n\nClick on the histories menu button \n at the top of the history pane.\n\n\nClick \nExtract Workflow\n\n\n\n\nYou will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and:\n\n\n\n\nChange the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d\n\n\nClick \nCreate Workflow\n\n\n\n\nThe workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.\n\n\nSome discussion\n\n\nHave a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence.\n\n\nMore interesting though is to:\n\n\n\n\nClick on the  \nWorkflows\n link in the top menu\n\n\nClick on the down arrow on your workflow\u2019s button.\n\n\nClick \nEdit\n\n\n\n\nA visualisation of your workflow will appear. Note the connections and the steps.\n\n\nNext we\u2019ll go through how to create this workflow using the editor..\n\n\nWorkflow Creation: Method 2\n\n\nWe will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.\n\n\nStep 1: Create a workflow name and edit space.\n\n\n\n\nClick on \nWorkflow\n in Galaxy\u2019s menu.\n\n\nClick on the \nCreate New Workflow\n button.\n\n\nIn the \"Workflow Name\" text box type: \nVariants from scratch\n\n\nClick the \nCreate\n button.\n\n\n\n\nYou should now be presented with a blank workflow editing grid.\n\n\nStep 2: Open the editor and place component tools\n\n\nAdd three input datafiles.\n\n\n\n\nIn the Workflow control section of the tool pane, click on \nInputs -\n Input dataset\n three times.\n\n\nSpread them out towards the left hand side of the workflow grid by clicking and dragging them around.\n\n\nFor each one, change their name.\n\n\nClick on each input box in turn\n\n\nIn the right hand pane (where the history usually is), change the name to:\n\n\nReference data\n\n\nReads 1\n\n\nReads 2 - respectively.\n\n\n\n\n\n\nIf you click on \nInput dataset\n in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press \nEnter\n to make the change.\n\n\n\n\n\n\n\n\nAdd in the BWA mapping step.\n\n\n\n\nClick on \nNGS: Mapping -\n Map with BWA\n in the tool pane. BWA will be added to the workflow grid.\n\n\nIn the right hand pane (where the history usually is), change the following parameters.\n\n\nChange \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to \nUse a genome from history.\n\n\nNote that the BWA box on the grid changes to match these settings.\n\n\n\n\n\n\n\n\nConnect the tools together.\n\n\n\n\nClick and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.)\n\n\n\"Reference data\" output to reference to \"Use the following dataset as the reference sequence\"\n\n\n\"Reads 1\" output to \"Select first set of reads\"\n\n\n\"Reads 2\" output to \"Select second set of reads\"\n\n\n\n\n\n\n\n\nAdd in the Freebayes (Variant Calling step.)\n\n\n\n\nClick on \nNGS: Variant Calling -\n Freebayes\n\n\nIn the right hand pane, change the following:\n\n\n\"Choose the source for the reference list:\"  to \nHistory\n\n\nConnect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.\n\n\nConnect the \"Reference data\" output to \"Freebayes\u2019\" \nUse the following dataset as the reference sequence\n input.\n\n\n\n\n\n\n\n\nIf you\u2019re keen - \nNote: this is optional.\n Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes.\n\n\n\n\n\"Choose parameter selection level\": \nComplete list of all options\n\n\n\"Population model options\": \nSet population model options\n\n\n\"Set ploidy for the analysis\": \n1\n\n\n\"Input filters\": \nSet input filters\n\n\n\"Exclude alignments from analysis if they have a mapping quality less than\": \n20\n\n\n\"Exclude alleles from analysis if their supporting base quality is less than\": \n20\n\n\n\"Require at least this fraction of observations \u2026 to evaluate the position\": \n0.9\n\n\n\"Require at least this count of observations .. to evaluate the position\": \n10\n\n\n\"Population and mappability priors\": \nSet population and mappability priors\n\n\n\"Disable incorporation of prior expectations about observations\": \nYes\n\n\n\n\nAdd in the Filter step.\n\n\n\n\nClick on \nFilter and Sort - \n Filter\n\n\nConnect \"Freebayes\u2019\" output_vcf to the \"filter\" input.\n\n\nIn the right hand pane, change the following:\n\n\n\"With the following condition\": \nc6 \n 500\n\n\n\"Number of header lines to skip\": \n56\n\n\n\n\n\n\n\n\nPhew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore:\n\n\nSelect workflow outputs.\n\n\n\n\nClick on the star next to \"Map with BWA\u2019s\" bam file output.\n\n\nClick on the star next to \"Filter\u2019s\" output vcf.\n\n\n\n\nStep 3: Save it!\n\n\nClick on the \n at the top of the workflow grid and select \nSave\n.\n\n\nCongratulations. You\u2019ve just created a Galaxy workflow.\n\n\nNow to run it!\n\n\nRunning the workflow\n\n\nWe will now make a new history called \"Test\" and run the workflow on it\u2019s data.\n\n\nCreate the new history\n\n\n\n\nFrom the Histories Menu, select \nCopy Datasets\n\n\nSelect the 2 x fastq files and the Ecoli .fna file.\n\n\nUnder destination history, enter \nTest\n into \"New History Named:\"\n\n\nClick \nCopy History Items\n button\n\n\nClick on the link to the new history in the green bar at the top of the screen\n\n\n\n\nRun the workflow\n\n\n\n\nOn the tools pane, click \nAll Workflows\n\n\nSelect the \nVariants from scratch\n workflow (or whatever you called it.)\n\n\nGive it the correct files.\n\n\nEcoli ... .fna\n for Reference data\n\n\nbacterial_std_err_1.fastq\n for Reads 1\n\n\nbacterial_std_err_2.fastq\n for Reads 2\n\n\nClick \nRun Workflow\n\n\n\n\nYour workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.\n\n\nWhat now?\n\n\nWhere to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#galaxy-workflows", 
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)", 
            "title": "Galaxy Workflows"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#background", 
            "text": "This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics:   Logging in to the server  How to construct and use a workflow by various methods  How to share a workflow", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#section-1-preparation", 
            "text": "The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in:  Launch a GVL Galaxy Instance  or you can use the public  Galaxy-mel  Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.)   If you have previously registered on this server just log in:  On the top menu select:  User -  Login  Enter your password  Click the  Submit  button       If you haven\u2019t registered on this server, you\u2019ll need to now.  On the top menu select:  User -  Register  Enter your email, choose a password, repeat it and add a (all lower case) one word name  Click on the  Submit  button.", 
            "title": "Section 1: Preparation."
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#section-2-create-and-run-a-workflow", 
            "text": "This section will show you two different methods to create a workflow and then how to run one.", 
            "title": "Section 2: Create and run a workflow."
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#import-the-workflow-history", 
            "text": "In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it  here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\"   From the menu at the top of the Galaxy window, click  Shared Data -  Histories  Find the history called \" workflow_finished \" and click on it.  Then click on  Import history  at the top right of the screen.  Change the name if you wish and then click  Import   This history should now be in your history pane on the right.", 
            "title": "Import the workflow history"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-1", 
            "text": "We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow.  Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history.  Now we will create the workflow.   Click on the histories menu button   at the top of the history pane.  Click  Extract Workflow   You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and:   Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d  Click  Create Workflow   The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.", 
            "title": "Workflow creation: Method 1"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#some-discussion", 
            "text": "Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence.  More interesting though is to:   Click on the   Workflows  link in the top menu  Click on the down arrow on your workflow\u2019s button.  Click  Edit   A visualisation of your workflow will appear. Note the connections and the steps.  Next we\u2019ll go through how to create this workflow using the editor..", 
            "title": "Some discussion"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-2", 
            "text": "We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.", 
            "title": "Workflow Creation: Method 2"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-1-create-a-workflow-name-and-edit-space", 
            "text": "Click on  Workflow  in Galaxy\u2019s menu.  Click on the  Create New Workflow  button.  In the \"Workflow Name\" text box type:  Variants from scratch  Click the  Create  button.   You should now be presented with a blank workflow editing grid.", 
            "title": "Step 1: Create a workflow name and edit space."
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-2-open-the-editor-and-place-component-tools", 
            "text": "Add three input datafiles.   In the Workflow control section of the tool pane, click on  Inputs -  Input dataset  three times.  Spread them out towards the left hand side of the workflow grid by clicking and dragging them around.  For each one, change their name.  Click on each input box in turn  In the right hand pane (where the history usually is), change the name to:  Reference data  Reads 1  Reads 2 - respectively.    If you click on  Input dataset  in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press  Enter  to make the change.     Add in the BWA mapping step.   Click on  NGS: Mapping -  Map with BWA  in the tool pane. BWA will be added to the workflow grid.  In the right hand pane (where the history usually is), change the following parameters.  Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to  Use a genome from history.  Note that the BWA box on the grid changes to match these settings.     Connect the tools together.   Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.)  \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\"  \"Reads 1\" output to \"Select first set of reads\"  \"Reads 2\" output to \"Select second set of reads\"     Add in the Freebayes (Variant Calling step.)   Click on  NGS: Variant Calling -  Freebayes  In the right hand pane, change the following:  \"Choose the source for the reference list:\"  to  History  Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.  Connect the \"Reference data\" output to \"Freebayes\u2019\"  Use the following dataset as the reference sequence  input.     If you\u2019re keen -  Note: this is optional.  Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes.   \"Choose parameter selection level\":  Complete list of all options  \"Population model options\":  Set population model options  \"Set ploidy for the analysis\":  1  \"Input filters\":  Set input filters  \"Exclude alignments from analysis if they have a mapping quality less than\":  20  \"Exclude alleles from analysis if their supporting base quality is less than\":  20  \"Require at least this fraction of observations \u2026 to evaluate the position\":  0.9  \"Require at least this count of observations .. to evaluate the position\":  10  \"Population and mappability priors\":  Set population and mappability priors  \"Disable incorporation of prior expectations about observations\":  Yes   Add in the Filter step.   Click on  Filter and Sort -   Filter  Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input.  In the right hand pane, change the following:  \"With the following condition\":  c6   500  \"Number of header lines to skip\":  56     Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore:  Select workflow outputs.   Click on the star next to \"Map with BWA\u2019s\" bam file output.  Click on the star next to \"Filter\u2019s\" output vcf.", 
            "title": "Step 2: Open the editor and place component tools"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-3-save-it", 
            "text": "Click on the   at the top of the workflow grid and select  Save .  Congratulations. You\u2019ve just created a Galaxy workflow.  Now to run it!", 
            "title": "Step 3: Save it!"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#running-the-workflow", 
            "text": "We will now make a new history called \"Test\" and run the workflow on it\u2019s data.", 
            "title": "Running the workflow"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#create-the-new-history", 
            "text": "From the Histories Menu, select  Copy Datasets  Select the 2 x fastq files and the Ecoli .fna file.  Under destination history, enter  Test  into \"New History Named:\"  Click  Copy History Items  button  Click on the link to the new history in the green bar at the top of the screen", 
            "title": "Create the new history"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#run-the-workflow", 
            "text": "On the tools pane, click  All Workflows  Select the  Variants from scratch  workflow (or whatever you called it.)  Give it the correct files.  Ecoli ... .fna  for Reference data  bacterial_std_err_1.fastq  for Reads 1  bacterial_std_err_2.fastq  for Reads 2  Click  Run Workflow   Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.", 
            "title": "Run the workflow"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#what-now", 
            "text": "Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.", 
            "title": "What now?"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/", 
            "text": "History creation instructions for Workflow tutorial\n\n\nUse this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.\n\n\nStep 1: Import the raw datafiles\n\n\n\n\nCreate a new blank history by clicking on the history menu \n, then \nCreate New\n\n\nUse the upload data tool to upload the data files from a remote repository..\n\n\nClick \nGet Data -\n Upload File\n\n\nClick \nPaste/Fetch data\n\n\nIn the box paste the following two url's (one per line): \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz\n and \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz\n\n\nChange the \nType\n to \nfastqsanger\n\n\nClick the \nPaste/Fetch data\n button again.\n\n\nPaste \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\nChange the \nType\n on this file to \nfasta\n\n\nClick the \nStart\n button.\n\n\nClick the \nClose\n button.\n\n\n\n\n\n\n\n\nAfter the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.\n\n\nStep 2: Run BWA\n\n\nNow we will run BWA on these files to map the reads to the reference.\n\n\n\n\nIn the tools menu, click \nNGS: Mapping -\n Map with BWA\n\n\nSet the following in the tool interface:\n\n\n\"Will you select a reference genome from your history or use a built-in index?\": \nUse a genome from history and build index\n\n\n\"Use the following dataset as the reference sequence\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\n\"Select first set of reads\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq\n\n\n\"Select second set of reads\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.\n\n\nStep 3: Run Freebayes\n\n\nNow we will run Freebayes to call variants in out reads compared with the reference.\n\n\n\n\nIn the tools menu, click \nNGS: Variant Analysis -\n Freebayes\n\n\nSet the following in the tool interface:\n\n\n\"Choose the source for the reference genome\": \nHistory\n\n\n\"BAM file\": \nMap with BWA on data 2, data 1, and data 3 (mapped reads in BAM format)\n\n\n\"Use the following dataset as the reference sequence\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will run Freebayes on your BAM file and will result in a variant calling format file (vcf). \n\n\nStep 4: Filter the VCF file.\n\n\nNow we will filter the vcf file to something sensible.\n\n\n\n\nIn the tools menu, click \nFilter and Sort -\n Filter\n\n\nSet the following in the tool interface:\n\n\n\"Filter\": \nFreeBayes on data 3 and data 4 (variants)\n\n\n\"With following condition\": \nc6 \n 500\n\n\n\"Number of header lines to skip\": \n56\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will filter out VCF file.\n\n\nYou should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial.\n\n\nReturn to it \nhere", 
            "title": "History Creation"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#history-creation-instructions-for-workflow-tutorial", 
            "text": "Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.", 
            "title": "History creation instructions for Workflow tutorial"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-1-import-the-raw-datafiles", 
            "text": "Create a new blank history by clicking on the history menu  , then  Create New  Use the upload data tool to upload the data files from a remote repository..  Click  Get Data -  Upload File  Click  Paste/Fetch data  In the box paste the following two url's (one per line):  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz  and  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz  Change the  Type  to  fastqsanger  Click the  Paste/Fetch data  button again.  Paste  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna  Change the  Type  on this file to  fasta  Click the  Start  button.  Click the  Close  button.     After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.", 
            "title": "Step 1: Import the raw datafiles"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-2-run-bwa", 
            "text": "Now we will run BWA on these files to map the reads to the reference.   In the tools menu, click  NGS: Mapping -  Map with BWA  Set the following in the tool interface:  \"Will you select a reference genome from your history or use a built-in index?\":  Use a genome from history and build index  \"Use the following dataset as the reference sequence\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna  \"Select first set of reads\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq  \"Select second set of reads\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq    Click  Execute   This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.", 
            "title": "Step 2: Run BWA"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-3-run-freebayes", 
            "text": "Now we will run Freebayes to call variants in out reads compared with the reference.   In the tools menu, click  NGS: Variant Analysis -  Freebayes  Set the following in the tool interface:  \"Choose the source for the reference genome\":  History  \"BAM file\":  Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format)  \"Use the following dataset as the reference sequence\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna    Click  Execute   This will run Freebayes on your BAM file and will result in a variant calling format file (vcf).", 
            "title": "Step 3: Run Freebayes"
        }, 
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-4-filter-the-vcf-file", 
            "text": "Now we will filter the vcf file to something sensible.   In the tools menu, click  Filter and Sort -  Filter  Set the following in the tool interface:  \"Filter\":  FreeBayes on data 3 and data 4 (variants)  \"With following condition\":  c6   500  \"Number of header lines to skip\":  56    Click  Execute   This will filter out VCF file.  You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial.  Return to it  here", 
            "title": "Step 4: Filter the VCF file."
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/", 
            "text": "What is GenomeSpace?\n\n\nGenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer.\n\n\nGenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next.\n\n\nThe GVL GenomeSpace can be found at \nhttps://genomespace.genome.edu.au\n.\n\n\nPrerequisites\n\n\nGenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.\n\n\nRegistering a GenomeSpace account\n\n\nTo register a GenomeSpace account you need to have a valid email address and do the following:\n\n\n\n\n\n\nGo to \nhttps://genomespace.genome.edu.au/\n and click on the \"Register new GenomeSpace user\" link:\n    \n\n\n\n\n\n\nEnter your preferred username, password and the valid email address and click the Sign up button.\n\n\nNote:\n You will receive an error if the username has already been taken.\n\n\n\nIf everything goes right you will see the following page.\n\n\n\n\n\n\n\nActivate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active.\n\n\n\n\n\n\nGo to \nhttps://genomespace.genome.edu.au\n and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace.\n\n\nIn a few seconds you will be redirected to your Home page.\n\n\n\nOn this page you can find the following items:\n\n\n\n\nYour username in the top right corner\n\n\nThe menu bar\n\n\nThe application bar\n\n\nYour home directory\n\n\nThe directory under the name \nShared to \u201cyour username\u201d\n contains any folders that have been shared to you through the GenomeSpace website.\n\n\nThe public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.\n\n\n\n\n\n\n\n\nMaking a swift container\n\n\n(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.)\n\n\nNeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably.\n\n\nIf you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: \nhttp://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/\n.\n\n\nGo to the NeCTAR dashboard at \nhttps://dashboard.rc.nectar.org.au\n.\n\n\n\n\n\n\nOn the left hand side of the dashboard click on \"Object Store\" and then \"Containers\".\n\n\n\n\n\n\nTo make a container, click \"Create Container\".\n\n\n\n\n\n\n\n\nMounting a swift container\n\n\nContainers can be found under the Object Store link in NeCTAR\u2019s dashboard.\n\n\nTo mount an available container go to Connect menu bar in GenomeSpace and select Swift Container.\n\n\n\nYou will see a new page as follows:\n\n\n\n\nTo fill out this form you need the following parameters:\n\n\n\n\nOpenStack EndPoint:\n The default value should be correct for NeCTAR: \nhttps://keystone.rc.nectar.org.au:5000/v2.0/tokens\n\n\nUser Name:\n This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below:\n  \n\n\nPassword:\n This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key:\n\n\nLogin to your NeCTAR dashboard account.\n\n\nOn the top right hand side of your Home click on the setting link\n    \n\n\nPress the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.)\n    \n\n\n\n\n\n\nTenancy name:\n Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. \n\n\nContainer name:\n The name of the container that you want to connect to. \n\n\n\n\nBasic file manipulation\n\n\nUnder the containers directory you can perform basic file manipulation as follows:\n\n\n\n\nCreating a directory:\n To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created.\n\n\nUploading a file into a directory:\n Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green.\n\n\nDeleting a file:\n To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds.\n\n\nPreviewing a file:\n Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file.\n\n\nDownloading a file:\n To download a file simply right click on the file and select download. Your download will be started in a few seconds.\n\n\nCreating a public link:\n Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.)\n\n\nCreating a private link:\n Right click on the file and select the \"view file\".\n\n\n\n\nAdding a Galaxy service to your account:\n\n\nPREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first.\n\n\nThe latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows:\n\n\n\n\n\n\nFrom the Menu bar go to the manage menu and select Private Tool.\n    \n\n\n\n\n\n\nFrom the opened window press the \"Add new\" button.      \n\n\n\n\n\n\n\n\nIn the new window fill out the form as follows:\n\n\n\n\nGive a name to your Galaxy\n\n\nGive a description (Optional)\n\n\nTool provider GVL (Optional)\n\n\nBase URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer\n\n\nParameter name: URL\n\n\nRequired: Ticked\n\n\nAllow multiple files: Ticked\n\n\nMultiple file Delimiter: ,\n\n\nSelect the files\u2019 types that you want your galaxy to work on\n\n\nUpload an image as an Icon for Galaxy (Optional)\n\n\nPress the save button. In a few seconds your Galaxy instance will be added to the Application bar.\n\n\n\n\nA sample page can be seen in the following image:\n\n\n\n\n\n\n\nLaunching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch.\n\n\nGalaxy will be opened in a new window.\n\n\n\n\n(Note: Your browser may block the pop-up. Allow the pop-up accordingly).\n\n\n\n\n\n\nFrom the opened Galaxy login and under your username go to the preferences options\n    \n\n\nand select the Manage OpenIDs links:\n\n\n\n\n\n\n\nFrom the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL.\n\n\n\n\n\n\n\n\nFrom now on your Galaxy can talk to GenomeSpace under your UserName.\n\n\nFile transfer to/from Galaxy\n\n\nPREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first (\nHow to\n).\n\n\n\n\n\n\nSending a file:\n\n\n\n\nFrom your Galaxy instance:\n Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy.\n\n\nFrom GenomeSpace:\n You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have \nconnected Galaxy and GenomeSpace\n).    \n\n\n\n\n\n\n\n\nReceiving a file:\n\n\nUnder each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.", 
            "title": "Introduction to GenomeSpace"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#what-is-genomespace", 
            "text": "GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer.  GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next.  The GVL GenomeSpace can be found at  https://genomespace.genome.edu.au .", 
            "title": "What is GenomeSpace?"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#prerequisites", 
            "text": "GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#registering-a-genomespace-account", 
            "text": "To register a GenomeSpace account you need to have a valid email address and do the following:    Go to  https://genomespace.genome.edu.au/  and click on the \"Register new GenomeSpace user\" link:\n        Enter your preferred username, password and the valid email address and click the Sign up button.  Note:  You will receive an error if the username has already been taken.  \nIf everything goes right you will see the following page.    Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active.    Go to  https://genomespace.genome.edu.au  and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace.  In a few seconds you will be redirected to your Home page.  \nOn this page you can find the following items:   Your username in the top right corner  The menu bar  The application bar  Your home directory  The directory under the name  Shared to \u201cyour username\u201d  contains any folders that have been shared to you through the GenomeSpace website.  The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.", 
            "title": "Registering a GenomeSpace account"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#making-a-swift-container", 
            "text": "(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.)  NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably.  If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial:  http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ .  Go to the NeCTAR dashboard at  https://dashboard.rc.nectar.org.au .    On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\".    To make a container, click \"Create Container\".", 
            "title": "Making a swift container"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#mounting-a-swift-container", 
            "text": "Containers can be found under the Object Store link in NeCTAR\u2019s dashboard.  To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container.  You will see a new page as follows:   To fill out this form you need the following parameters:   OpenStack EndPoint:  The default value should be correct for NeCTAR:  https://keystone.rc.nectar.org.au:5000/v2.0/tokens  User Name:  This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below:\n    Password:  This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key:  Login to your NeCTAR dashboard account.  On the top right hand side of your Home click on the setting link\n      Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.)\n        Tenancy name:  Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process.   Container name:  The name of the container that you want to connect to.", 
            "title": "Mounting a swift container"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#basic-file-manipulation", 
            "text": "Under the containers directory you can perform basic file manipulation as follows:   Creating a directory:  To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created.  Uploading a file into a directory:  Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green.  Deleting a file:  To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds.  Previewing a file:  Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file.  Downloading a file:  To download a file simply right click on the file and select download. Your download will be started in a few seconds.  Creating a public link:  Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.)  Creating a private link:  Right click on the file and select the \"view file\".", 
            "title": "Basic file manipulation"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#adding-a-galaxy-service-to-your-account", 
            "text": "PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first.  The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows:    From the Menu bar go to the manage menu and select Private Tool.\n        From the opened window press the \"Add new\" button.           In the new window fill out the form as follows:   Give a name to your Galaxy  Give a description (Optional)  Tool provider GVL (Optional)  Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer  Parameter name: URL  Required: Ticked  Allow multiple files: Ticked  Multiple file Delimiter: ,  Select the files\u2019 types that you want your galaxy to work on  Upload an image as an Icon for Galaxy (Optional)  Press the save button. In a few seconds your Galaxy instance will be added to the Application bar.   A sample page can be seen in the following image:    Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch.  Galaxy will be opened in a new window.   (Note: Your browser may block the pop-up. Allow the pop-up accordingly).    From the opened Galaxy login and under your username go to the preferences options\n      and select the Manage OpenIDs links:    From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL.     From now on your Galaxy can talk to GenomeSpace under your UserName.", 
            "title": "Adding a Galaxy service to your account:"
        }, 
        {
            "location": "/tutorials/genomespace/genomespace/#file-transfer-tofrom-galaxy", 
            "text": "PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ).    Sending a file:   From your Galaxy instance:  Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy.  From GenomeSpace:  You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have  connected Galaxy and GenomeSpace ).         Receiving a file:  Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.", 
            "title": "File transfer to/from Galaxy"
        }, 
        {
            "location": "/tutorials/docker/docker/", 
            "text": "Workshop Slides\n (use the arrow keys to navigate)\n\n\n\n\nPart 1: Docker and Containers\n\n\nPart 2: Running Containers\n\n\nPart 3: Making your Own Image\n\n\nPart 4: Docker on HPC", 
            "title": "Containerized Bioinformatics"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/", 
            "text": "Introduction to Variant Calling using Galaxy\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.\n\n\nNote: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with the FASTQ format and base quality scores\n\n\nBe able to align reads to generate a BAM file and subsequently generate a pileup file\n\n\nBe able to run the FreeBayes variant caller to find SNVs and indels\n\n\nBe able to visualise BAM files using the \nIntegrative Genomics Viewer (IGV)\n and identify likely SNVs and indels by eye\n\n\n\n\n\n\nBackground\n\n\nSome background reading material - \nbackground\n\n\nWhere is the data in this tutorial from?\n\n\nThe workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the \n1000 genomes\n Genomes project.\n\n\n\n\n1. Preparation\n\n\n\n\nMake sure you have an instance of Galaxy ready to go.\n\n\nIf you are not using your own Galaxy instance, you can use our \nGalaxy Tutorial server\n or \nGalaxy Melbourne server\n.\n\n\n\n\n\n\n\n\nImport data for the tutorial.\n\n\n\n\nIn this case, we are uploading a \nFASTQ\n file.\n\n\nMethod 1\n\n\nPaste/Fetch data from a URL to Galaxy.\n\n\nIn the Galaxy tools panel (left), click on \nGet Data\n and choose \nUpload File\n.\n\n\nClick \nPaste/Fetch data\n and paste the following URL into the box\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq\n\n\n\n\n\nSelect \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nOnce the upload status turns \ngreen\n, it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).\n\n\nThe dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the \nName\n box and give the file a shorter name by removing the URL. Then click \nSave\n.\n\n\n\n\n\n\n\n\nAlternatively, if you have a local file to upload (\nFor the purpose of this tutorial we can stick with the option above\n):\n\n\n\n\nMethod 2\n\n\nUpload data to Galaxy.\n\n\nIn the Galaxy tools panel (left), click on \nGet Data\n and choose \nUpload File\n.\n\n\nFrom \nChoose local file\n select the downloaded FASTQ file. Select \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nOnce the upload status turns \ngreen\n, it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).\n\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\nSo far, we have started a Galaxy instance, got hold of our data and uploaded it to\nthe Galaxy instance.\n\nNow we are ready to perform our analysis.\n\n\n\n\n2. Quality Control\n\n\nThe first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!\n\n\n1. Take a look at the FASTQ file\n\n\n\n\nClick on the eye icon to the top right of the fastq file to view the a snippet of the file.\n\n\nNote that each read is represented by 4 lines:\n\n\nread identifier\n\n\nshort read sequence\n\n\nseparator\n\n\nshort read sequence quality scores\n\n\n\n\n\n\n\n\ne.g.\nidentifier:    @61CC3AAXX100125:7:72:14903:20386/1\nread sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT\nseparator:     +\nquality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@\n6\n\n\n\n\nFor more details see \nFASTQ\n.\n\n\n2. Assessing read quality from the FASTQ files\n\n\n\n\nFrom the Galaxy tools panel, select \nNGS: QC and manipulation \n FastQC: Read Quality reports\n.\n\nThe input FASTQ file will be selected by default. Keep the other defaults and click execute.\n\n\n\n\n\n\n\n   Tip:\n   Note the batch processing interface of Galaxy:\n\n   \n\n   \n grey = waiting in queue\n\n   \n yellow = running\n\n   \n green = finished\n\n   \n red = tried to run and failed\n   \n\n\n\n\nWhen the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data).\nLook at the various quality scores. The data looks pretty good - \nhigh Per base sequence quality\n (avg. above 30).\n\n\nNote that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step.\n\n\n\n\n3. Alignment to the reference - (FASTQ to BAM)\n\n\nThe basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.\n\n\n1. Align the reads with BWA\n\n\n\n\n\n\nMap/align the reads with the \nBWA\n tool to Human reference genome 19 (hg19) \nUCSC hg19\n.\n    From the Galaxy tools panel, select\n\n\n\nNGS: Mapping \n Map with BWA-MEM [3-5mins]\n\n\n\nFrom the options:\n\nUsing reference genome: set to hg19\n\nSingle or Paired-end reads: set to Single\n\n\n\nMake sure your fastq file is the input file.\n\n\nKeep other options as default and click execute.\n\n\n\n\nNote: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings\n\n\n\n\n\n\nSort the BAM file.\n  From the Galaxy tools panel, select\n\n\n\nNGS: SAM Tools \n Sort BAM dataset\n\n\n\nFrom the options:\n\n\nBAM File: set to the output from the alignment BAM file\n\n\nSort by: Chromosomal coordinates\n\n\nKeep other options as default and click execute\n\n\n\n\n\n\n\n\n\n2. Examine the alignment\n\n\n\n\n\n\nTo examine the output sorted BAM file, we need to first convert it into readable \nSAM\n format.\n  From the Galaxy tools panel, select\n\n\n\nNGS: SAM Tools \n BAM-to-SAM\n\n\n\nFrom the options:\n\n\nBAM File to Convert: set to the output to the sorted BAM file\n\n\nKeep other options as default and click execute\n\n\n\n\n\n\n\n\n\nExamine the generated Sequence Alignment Map (SAM) file.\n\n\n\n\nClick the eye icon next to the newly generated file\n\n\nFamiliarise yourself with the \nSAM\n format\n\n\nNote that some reads have mapped to non-chr22 chromosomes (see column 3).\n\n\n\n\nThis is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant?\n\n\n\n\n\nTip:\nGalaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs.\n\n\nThis can be done as follows:\n\n\nClick on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.\n\n\n\n\n\n\n\n\n3. Assess the alignment data\n\n\nWe can generate some mapping statistics from the BAM file to assess the quality of our alignment.\n\n\n\n\n\n\nRun IdxStats. From the Galaxy tools panel, select\n    \n\n    \nNGS: SAM Tools \n IdxStats\n\n    \n\n    Select the sorted BAM file as input. Keep other options as default and click execute.\n    \n\n    \n\n\nIdxStats generates a tab-delimited output with four columns.\nEach line consists of a reference sequence name (e.g. a chromosome),\nreference sequence length, number of mapped reads and number of placed but\nunmapped reads.\n\n\nWe can see that most of the reads are aligning to chromosome 22 as expected.\n\n\n\n\n\n\nRun Flagstat. From the Galaxy tools panel, select\n    \n\n    \nNGS: Sam Tools \n Flagstat\n\n    \n\n    From the options:\n    \n\n    The BAM: select the sorted BAM file\n    \n\n    Keep other options as default and click execute\n    \n\n    \n\n\nNote that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.\n\n\n\n\n\n\n\n\n4. Visualise the BAM file.\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on the sorted BAM file dataset in the History panel.\n\n\nClick on \"Display with IGV \nweb current\n\". This should download a .jnlp\n    Java Web Start file to your computer. Open this file to run IGV.\n    (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \"\nlocal\n\" instead of \"web current\", and this will open the BAM file in your current IGV session.\n\n\nOnce IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded.\n\n\nOur reads for this tutorial are from chromosome 22, so select \nchr22\n from the second\n    drop box under the toolbar. Zoom in to view alignments of reads to the reference genome.\n\n\n\n\n\n\nTry looking at region \nchr22:36,006,744-36,007,406\n\n\nCan you see a few variants?  \n\n\n\n\nDon't close IGV yet as we'll be using it later.\n\n\n\n\n5. Generate a pileup file\n\n\nA pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view.\n\n\nThe current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.\n\n\n\n\n\n\nGenerate a pileup file:\n\n\nFrom the Galaxy tools panel, select\n\n\n\nNGS: SAMtools \n Generate Pileup\n\n\n\nFrom the options:\n\nCall consensus according to MAQ model = Yes\n\nThis generates a called 'consensus base' for each chromosomal position.\n\n\n\nKeep other options as default and click execute\n\n\n\n\nFor each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to \npileup\n.\n\n\n\n\nFirst, rename the output to something more meaningful by clicking on the pencil icon\n\n\nTo change the datatype, click on the Datatype link from the top tab while you're editing attributes.\n\n\nFor downstream processing we want to tell Galaxy that this is a \npileup\n file. From the drop-down, select Pileup and click save.\n\n\n\n\n\n\n\n Tip:\nThe pileup file we generated has 10 columns:\n\n\n 1. chromosome\n\n\n 2. position\n\n\n 3. current reference base\n\n\n 4. consensus base from the mapped reads\n\n\n 5. consensus quality\n\n\n 6. SNV quality\n\n\n 7. maximum mapping quality\n\n\n 8. coverage\n\n\n 9. bases within reads\n\n\n 10. quality values\n\n\nFurther information on (9):\n\nEach character represents one of the following (the longer this string, higher the coverage):\n  \n\n  \n . = match on forward strand for that base\n  \n , = match on reverse strand\n  \n ACGTN = mismatch on forward\n  \n acgtn = mismatch on reverse\n  \n +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next\n  \n -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next\n  \n ^ = start of read\n  \n $ = end of read\n  \n BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores\n  \n\n\n\n\n\n\n\n\nFilter the pileup file:\n\n\nIf you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads.\n  \n\n  From the Galaxy tools panel, select:\n  \n\n  \nNGS: SAM Tools \n Filter Pileup\n\n  \n\n  From the options:\n\n  which contains = Pileup with ten columns (with consensus)\n\n  Do not report positions with coverage lower than = 10\n\n\nTry this filtering step two ways:\n\n\n\n\nFirst, set the parameter \nOnly report variants?\n to \nNo\n. This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location.\n\n\nThen, repeat the step but set \nOnly report variants?\n to \nYes\n. This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores.\n\n\n\n\nExamine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.\n\n\n\n\n\n\n6. Call variants with FreeBayes\n\n\nFreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the \nAdvanced Variant Calling\n tutorial.\n\n\n\n\n\n\nCall variants with FreeBayes.\n Under \nNGS ANALYSIS\n, select the tool \nNGS: Variant Analysis -\n FreeBayes\n.\n\n\n\n\nSelect your sorted BAM file as input, and select the correct reference genome.\n\n\nUnder \nChoose parameter selection level\n, select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on \nGalaxy-specific options\n.\n\n\nExecute\n FreeBayes.\n\n\n\n\n\n\n\n\nCheck the generated list of variants\n.\n\n\n\n\nClick the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns.\n\n\nHow many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)\n\n\nWhat sort of quality scores do your variants have?\n\n\n\n\nFreeBayes, like most variant callers, produces a \nVariant Call Format (VCF)\n file.\n\n\nVCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers.\n\n\nThe data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call.\n\n\nThe columns in more detail are:\n\n\n\n\n\n\n\n\nCol\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCHROM\n\n\nChromosome name\n\n\n\n\n\n\n2\n\n\nPOS\n\n\n1-based position. For an indel, this is the position preceding the indel.\n\n\n\n\n\n\n3\n\n\nID\n\n\nVariant identifier (optional). Usually the dbSNP rsID.\n\n\n\n\n\n\n4\n\n\nREF\n\n\nReference sequence at POS involved in the variant. For a SNP, it is a single base.\n\n\n\n\n\n\n5\n\n\nALT\n\n\nComma delimited list of alternative sequence(s) seen in our reads.\n\n\n\n\n\n\n6\n\n\nQUAL\n\n\nPhred-scaled probability of all samples being homozygous reference.\n\n\n\n\n\n\n7\n\n\nFILTER\n\n\nSemicolon delimited list of filters that the variant fails to pass.\n\n\n\n\n\n\n8\n\n\nINFO\n\n\nSemicolon delimited list of variant information.\n\n\n\n\n\n\n9\n\n\nFORMAT\n\n\nColon delimited list of the format of individual genotypes in the following fields.\n\n\n\n\n\n\n10+\n\n\nSample(s)\n\n\nIndividual genotype information defined by FORMAT.\n\n\n\n\n\n\n\n\nFor even more detail on VCF files, you can look at the \nVCF format specification\n.\n\n\n\n\n\n\nVisualise the variants and compare files\n\n\n\n\nOpen the VCF file in IGV using the dataset's \ndisplay in IGV local\n link (using the \nweb current\n link will open IGV again, and using \nlocal\n should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.\n\n\nTake a look again at the same region as earlier: \nchr22:36,006,744-36,007,406\n\n\nTry comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool \nFilter and Sort \n Filter\n. Choose your previously-filtered pileup file as input, and set the filter condition to \nc1==\"chr22\" and c2 \n 36006744 and c2 \n 36007406\n.\n\n\n\n\n\n\n\n\nOptional: filter variants\n: See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the \nFilter and Sort: Filter\n tool we used above.\n\n\n\n\n\n\n7. Further steps\n\n\nWe've seen how to:\n\n\n\n\nAlign the raw data (sequence reads) to a reference genome\n\n\nGenerate variant calls from aligned reads\n\n\nInterpret the various file formats used in storing reads, alignments and variant calls\n\n\nVisualise the data using IGV\n\n\n\n\nFor real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants.\n\n\nWe look at some further steps in the \nAdvanced Variant Calling\n tutorial.", 
            "title": "Introduction to Variant Calling"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#introduction-to-variant-calling-using-galaxy", 
            "text": "", 
            "title": "Introduction to Variant Calling using Galaxy"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#tutorial-overview", 
            "text": "In this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.  Note: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants.", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#learning-objectives", 
            "text": "At the end of this tutorial you should:   Be familiar with the FASTQ format and base quality scores  Be able to align reads to generate a BAM file and subsequently generate a pileup file  Be able to run the FreeBayes variant caller to find SNVs and indels  Be able to visualise BAM files using the  Integrative Genomics Viewer (IGV)  and identify likely SNVs and indels by eye", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#background", 
            "text": "Some background reading material -  background", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#where-is-the-data-in-this-tutorial-from", 
            "text": "The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the  1000 genomes  Genomes project.", 
            "title": "Where is the data in this tutorial from?"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-preparation", 
            "text": "Make sure you have an instance of Galaxy ready to go.  If you are not using your own Galaxy instance, you can use our  Galaxy Tutorial server  or  Galaxy Melbourne server .     Import data for the tutorial.   In this case, we are uploading a  FASTQ  file.  Method 1  Paste/Fetch data from a URL to Galaxy.  In the Galaxy tools panel (left), click on  Get Data  and choose  Upload File .  Click  Paste/Fetch data  and paste the following URL into the box \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq   Select  Type  as  fastqsanger  and click  Start .  Once the upload status turns  green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).  The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the  Name  box and give the file a shorter name by removing the URL. Then click  Save .     Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ):   Method 2  Upload data to Galaxy.  In the Galaxy tools panel (left), click on  Get Data  and choose  Upload File .  From  Choose local file  select the downloaded FASTQ file. Select  Type  as  fastqsanger  and click  Start .  Once the upload status turns  green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).       Summary: \nSo far, we have started a Galaxy instance, got hold of our data and uploaded it to\nthe Galaxy instance. \nNow we are ready to perform our analysis.", 
            "title": "1. Preparation"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-quality-control", 
            "text": "The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!", 
            "title": "2. Quality Control"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-take-a-look-at-the-fastq-file", 
            "text": "Click on the eye icon to the top right of the fastq file to view the a snippet of the file.  Note that each read is represented by 4 lines:  read identifier  short read sequence  separator  short read sequence quality scores     e.g.\nidentifier:    @61CC3AAXX100125:7:72:14903:20386/1\nread sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT\nseparator:     +\nquality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@ 6  For more details see  FASTQ .", 
            "title": "1. Take a look at the FASTQ file"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-assessing-read-quality-from-the-fastq-files", 
            "text": "From the Galaxy tools panel, select  NGS: QC and manipulation   FastQC: Read Quality reports . The input FASTQ file will be selected by default. Keep the other defaults and click execute.    \n   Tip:\n   Note the batch processing interface of Galaxy: \n    \n     grey = waiting in queue \n     yellow = running \n     green = finished \n     red = tried to run and failed\n      When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data).\nLook at the various quality scores. The data looks pretty good -  high Per base sequence quality  (avg. above 30).  Note that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step.", 
            "title": "2. Assessing read quality from the FASTQ files"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-alignment-to-the-reference-fastq-to-bam", 
            "text": "The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.", 
            "title": "3. Alignment to the reference - (FASTQ to BAM)"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-align-the-reads-with-bwa", 
            "text": "Map/align the reads with the  BWA  tool to Human reference genome 19 (hg19)  UCSC hg19 .\n    From the Galaxy tools panel, select  NGS: Mapping   Map with BWA-MEM [3-5mins]  \nFrom the options: \nUsing reference genome: set to hg19 \nSingle or Paired-end reads: set to Single  \nMake sure your fastq file is the input file. \nKeep other options as default and click execute.   Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings    Sort the BAM file.\n  From the Galaxy tools panel, select  NGS: SAM Tools   Sort BAM dataset  \nFrom the options: \nBAM File: set to the output from the alignment BAM file \nSort by: Chromosomal coordinates \nKeep other options as default and click execute", 
            "title": "1. Align the reads with BWA"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-examine-the-alignment", 
            "text": "To examine the output sorted BAM file, we need to first convert it into readable  SAM  format.\n  From the Galaxy tools panel, select  NGS: SAM Tools   BAM-to-SAM  \nFrom the options: \nBAM File to Convert: set to the output to the sorted BAM file \nKeep other options as default and click execute     Examine the generated Sequence Alignment Map (SAM) file.   Click the eye icon next to the newly generated file  Familiarise yourself with the  SAM  format  Note that some reads have mapped to non-chr22 chromosomes (see column 3).   This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant?   \nTip:\nGalaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs.  This can be done as follows:  Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.", 
            "title": "2. Examine the alignment"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-assess-the-alignment-data", 
            "text": "We can generate some mapping statistics from the BAM file to assess the quality of our alignment.    Run IdxStats. From the Galaxy tools panel, select\n     \n     NGS: SAM Tools   IdxStats \n     \n    Select the sorted BAM file as input. Keep other options as default and click execute.\n     \n      IdxStats generates a tab-delimited output with four columns.\nEach line consists of a reference sequence name (e.g. a chromosome),\nreference sequence length, number of mapped reads and number of placed but\nunmapped reads.  We can see that most of the reads are aligning to chromosome 22 as expected.    Run Flagstat. From the Galaxy tools panel, select\n     \n     NGS: Sam Tools   Flagstat \n     \n    From the options:\n     \n    The BAM: select the sorted BAM file\n     \n    Keep other options as default and click execute\n     \n      Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.", 
            "title": "3. Assess the alignment data"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-visualise-the-bam-file", 
            "text": "To visualise the alignment data:   Click on the sorted BAM file dataset in the History panel.  Click on \"Display with IGV  web current \". This should download a .jnlp\n    Java Web Start file to your computer. Open this file to run IGV.\n    (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session.  Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded.  Our reads for this tutorial are from chromosome 22, so select  chr22  from the second\n    drop box under the toolbar. Zoom in to view alignments of reads to the reference genome.    Try looking at region  chr22:36,006,744-36,007,406  Can you see a few variants?     Don't close IGV yet as we'll be using it later.", 
            "title": "4. Visualise the BAM file."
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#5-generate-a-pileup-file", 
            "text": "A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view.  The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.    Generate a pileup file:  From the Galaxy tools panel, select  NGS: SAMtools   Generate Pileup  \nFrom the options: \nCall consensus according to MAQ model = Yes \nThis generates a called 'consensus base' for each chromosomal position.  \nKeep other options as default and click execute   For each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to  pileup .   First, rename the output to something more meaningful by clicking on the pencil icon  To change the datatype, click on the Datatype link from the top tab while you're editing attributes.  For downstream processing we want to tell Galaxy that this is a  pileup  file. From the drop-down, select Pileup and click save.    \n Tip:\nThe pileup file we generated has 10 columns:   1. chromosome   2. position   3. current reference base   4. consensus base from the mapped reads   5. consensus quality   6. SNV quality   7. maximum mapping quality   8. coverage   9. bases within reads   10. quality values  Further information on (9): \nEach character represents one of the following (the longer this string, higher the coverage):\n   \n    . = match on forward strand for that base\n    , = match on reverse strand\n    ACGTN = mismatch on forward\n    acgtn = mismatch on reverse\n    +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next\n    -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next\n    ^ = start of read\n    $ = end of read\n    BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores\n       Filter the pileup file:  If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads.\n   \n  From the Galaxy tools panel, select:\n   \n   NGS: SAM Tools   Filter Pileup \n   \n  From the options: \n  which contains = Pileup with ten columns (with consensus) \n  Do not report positions with coverage lower than = 10  Try this filtering step two ways:   First, set the parameter  Only report variants?  to  No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location.  Then, repeat the step but set  Only report variants?  to  Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores.   Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.", 
            "title": "5. Generate a pileup file"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#6-call-variants-with-freebayes", 
            "text": "FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the  Advanced Variant Calling  tutorial.    Call variants with FreeBayes.  Under  NGS ANALYSIS , select the tool  NGS: Variant Analysis -  FreeBayes .   Select your sorted BAM file as input, and select the correct reference genome.  Under  Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on  Galaxy-specific options .  Execute  FreeBayes.     Check the generated list of variants .   Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns.  How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)  What sort of quality scores do your variants have?   FreeBayes, like most variant callers, produces a  Variant Call Format (VCF)  file.  VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers.  The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call.  The columns in more detail are:     Col  Field  Description      1  CHROM  Chromosome name    2  POS  1-based position. For an indel, this is the position preceding the indel.    3  ID  Variant identifier (optional). Usually the dbSNP rsID.    4  REF  Reference sequence at POS involved in the variant. For a SNP, it is a single base.    5  ALT  Comma delimited list of alternative sequence(s) seen in our reads.    6  QUAL  Phred-scaled probability of all samples being homozygous reference.    7  FILTER  Semicolon delimited list of filters that the variant fails to pass.    8  INFO  Semicolon delimited list of variant information.    9  FORMAT  Colon delimited list of the format of individual genotypes in the following fields.    10+  Sample(s)  Individual genotype information defined by FORMAT.     For even more detail on VCF files, you can look at the  VCF format specification .    Visualise the variants and compare files   Open the VCF file in IGV using the dataset's  display in IGV local  link (using the  web current  link will open IGV again, and using  local  should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.  Take a look again at the same region as earlier:  chr22:36,006,744-36,007,406  Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool  Filter and Sort   Filter . Choose your previously-filtered pileup file as input, and set the filter condition to  c1==\"chr22\" and c2   36006744 and c2   36007406 .     Optional: filter variants : See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the  Filter and Sort: Filter  tool we used above.", 
            "title": "6. Call variants with FreeBayes"
        }, 
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#7-further-steps", 
            "text": "We've seen how to:   Align the raw data (sequence reads) to a reference genome  Generate variant calls from aligned reads  Interpret the various file formats used in storing reads, alignments and variant calls  Visualise the data using IGV   For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants.  We look at some further steps in the  Advanced Variant Calling  tutorial.", 
            "title": "7. Further steps"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/", 
            "text": "Variant Detection - Advanced Workshop\n\n\nTutorial Overview\n\n\nIn this tutorial, we will look further at variant calling from sequence data. We will:\n\n\n\n\nAlign NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop\n\n\nCarry out local realignment on our aligned reads\n\n\nCompare the performance of different variant calling tools\n\n\nAnnotate our called variants with reference information\n\n\n\n\nBackground\n\n\nSome background reading and reference material can be found \nhere\n.\n\n\nThe slides used in this workshop can be found \nhere\n.\n\n\nWhere is the data in this tutorial from?\n\n\nThe data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/\n\n\nPreparation\n\n\n\n\n\n\nMake sure you have an instance of Galaxy ready to go.\n\n\n\n\nIf you don't have your own - go to our \nGalaxy-Tut\n or \nGalaxy-Melbourne\n server.\n\n\nLog in so that your work will be saved.\nIf you don't already have an account on this server, select from the menu \nUser -\n Register\n and create one.\n\n\n\n\n\n\n\n\nImport data for the tutorial.\n\n\n\n\n\n\nWe will import a pair of FASTQ files containing paired-end reads, and a\n  VCF file of known human variants to use for variant evaluation.\n\n\n\n\n\n\nMethod 1: Paste/Fetch data from a URL to Galaxy.\n\n\n\n\nIn the Galaxy tools panel (left), under \nBASIC TOOLS\n, click on \nGet Data\n and choose \nUpload File\n.\n\n\nGet the FASTQ files: click \nPaste/Fetch data\n and enter these URLs into the text box.\nIf you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines.\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2\n\n\n\n\n\nSelect \nType\n as \nfastqsanger\n and click \nStart\n. Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which.\n\n\nGet the VCF file: click \nPaste/Fetch data\n again to open a new text box, and paste the following URL into the box\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf\n\n\n\n\n\nThis time, you can leave the \nType\n on Auto-detect. Click \nStart\n.\n\n\n\n\nOnce the upload status for both sets of files turns \ngreen\n, you can click \nClose\n. You should now be able to see all three files in the Galaxy history panel (right).\n\n\n\n\n\n\nMethod 2: Upload local data to Galaxy.\n (In most cases, you won't need this for the tutorial)\n\n\n\n\nUse this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial.\n\n\nIn the Galaxy tools panel (left), under \nBASIC TOOLS\n, click on \nGet Data\n and choose \nUpload File\n.\n\n\nClick \nChoose local file\n and select the downloaded  FASTQ files. Select \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nClick \nChoose local file\n again and select the downloaded VCF file. Click \nStart\n.\n\n\nOnce the upload status for all files turns \ngreen\n, you can click \nClose\n. You should now be able to see all three files in the Galaxy history panel (right).\n\n\n\n\n\n\n\n\nRename the datasets\n\n\n\n\nYou should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names.\n\n\nClick on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is \nNA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1\n. Click \nSave\n.\n\n\nSimilarly, rename the second dataset to \nNA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2\n.\n\n\nSimilarly, rename the third dataset to \ndbSNP135_excludingsitesafter129.chr20.vcf\n.\n\n\n\n\n\n\n\n\nSection 1: Quality Control\n\n\nThe aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome!\n\n\n\n\nAnalyse the quality of the reads in the FASTQ file.\n\n\nFrom the left hand tool panel in Galaxy, under \nNGS ANALYSIS\n, select \nNGS: QC and manipulation -\n FastQC\n\n\nSelect one of the FASTQ files as input and \nExecute\n the tool.\n\n\nWhen the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics.\n\n\n\n\n\n\n\n\nLook at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).\n\n\nSection 2: Alignment and depth of coverage\n\n\nIn this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome.\n\n\nSome of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects.\n\n\nWe will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment.\n\n\n\n\n\n\nMap/align the reads with Bowtie2 to the human reference genome.\n We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under \nNGS ANALYSIS\n in the tools panel, select the tool \nNGS: Mapping -\n Bowtie2\n.\n\n\n\n\nWe have paired-end reads in two FASTQ files, so select \npaired-end\n.\n\n\nSelect the two FASTQ files as inputs.\n\n\nUnder \nSelect reference genome\n select the human genome \nhg19\n.\n\n\nNext we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under \nSet read groups information?\n select \nSet read groups (SAM/BAM specification)\n. (Picard-style should also work.)\n\n\nSet the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier.\n\n\nSet the sample name to \"NA12878\"\n\n\nSet the platform to \nILLUMINA\n\n\nSet the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction.\n\n\n\n\n\n\nYou can leave other read group information blank, and use default Bowtie2 settings. \nExecute\n the tool.\n\n\nWhen the alignment has finished, you should rename the BAM file to something more convenient, such as \nNA12878.chr20_2mb.30xPE.bam\n.\n\n\nNote: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents.\n\n\n\n\n\n\n\n\nVisualise the aligned BAM file with IGV.\n The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server).\n\n\n\n\nNote: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps.\n\n\nIn the green dataset box for your BAM file in the history panel, you will see some \ndisplay with IGV\n links. Launch IGV by clicking the \nweb current\n link. If IGV is already running on your computer, instead click the \nlocal\n link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download.\n\n\nIf\n your BAM file was not automatically loaded, download and open it:\n\n\nDownload the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory.\n\n\nIn IGV, select the correct reference genome, \nhg19\n, in the top-left drop-down menu.\n\n\nIn IGV, open the BAM file using \nFile -\n Load from File\n.\n\n\n\n\n\n\nSelect chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box).\n\n\nZoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome.\n\n\nScroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...)\n\n\n\n\n\n\n\n\nRestrict the genomic region considered.\n Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region.\n\n\n\n\nUnder \nBASIC TOOLS\n, select the tool \nText manipulation -\n Create single interval\n. Enter these values:\n\n\nChromosome: chr20\n\n\nStart position: 0\n\n\nEnd position: 2000000\n\n\nName: chr20_2mb\n\n\nStrand: plus\n\n\n\n\n\n\nExecute\n this tool. This will create a small BED file specifying just one genomic region.\n\n\nWhen the file is created, rename it to \nchr20_2mb.bed\n. Have a look at the contents of this BED file.\n\n\n\n\n\n\n\n\nEvaluate the depth of coverage of the aligned region.\n Under \nNGS COMMON TOOLSETS\n, select the tool \nNGS: GATK Tools 2.8 -\n Depth of Coverage\n.\n\n\n\n\nSelect the BAM file you just generated as the input BAM file.\n\n\nMake sure the reference genome we aligned to is selected under \nUsing reference genome\n.\n\n\nSet \nOutput format\n to \ntable\n.\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nThis tool will produce a lot of files. We are most interested in the summaries. Examine the contents of:\n\n\n\u2018Depth of Coverage on data.... (output summary sample)\u2019:\n this file  will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution.\n\n\nYour mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling.\n\n\nAlso have a quick look at the \n(per locus coverage)\n file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome.\n\n\n\n\n\n\nThe other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.\n\n\n\n\n\n\n\n\nSection 3. Local realignment\n\n\nAlignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome.\n\n\nThis is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels.\n\n\nBoth GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels.\n\n\nIf you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2.\nHowever performing local realignment will improve the accuracy of our variant calls.\n\n\n\n\n\n\nGenerate the list of indel regions to be realigned.\n Under \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -\n Realigner Target Creator\n. This tool will look for regions containing potential indels.\n\n\n\n\nSelect your BAM file as input.\n\n\nSelect the correct reference genome (the genome used for alignment).\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nIf you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step.\n\n\n\n\n\n\n\n\nRealign the subsets of reads around the target indel areas.\n Under \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -\n Indel Realigner\n. This step will realign reads and generate a new BAM file.\n\n\n\n\nSelect your BAM file as input.\n\n\nSelect the correct reference genome.\n\n\nUnder \nRestrict realignment to provided intervals\n, select the intervals file generated in the previous step.\n\n\nExecute\n.\n\n\nRename the file to something easier to recognise, e.g. \nNA12878.chr20_2mb.30xPE.realigned.bam\n\n\nTo keep your history clean, you may want to delete the log files generated by GATK in the last two steps.\n\n\n\n\n\n\n\n\nCompare the realigned BAM file to original BAM around an indel.\n\n\n\n\nOpen the realigned BAM file in IGV. You can use the \nlocal\n link to open it in your already-running IGV session and compare it to the pre-realignment BAM file.\n\n\nGenerally, the new BAM should appear identical to the old BAM except in the realigned regions.\n\n\nFind some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions).\n\n\nIf you can\u2019t find anything obvious, check region chr20:1163914-1163954;\nyou should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.\n\n\n\n\n\n\n\n\nSection 4. Calling variants with FreeBayes\n\n\nFreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It  can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome.\n\n\nYou can \nread more about FreeBayes here\n.\n\n\n\n\n\n\nCall variants with FreeBayes.\n Under \nNGS ANALYSIS\n, select the tool \nNGS: Variant Analysis -\n FreeBayes\n.\n\n\n\n\nSelect your realigned BAM file as input, and select the correct reference genome.\n\n\nUnder \nChoose parameter selection level\n, select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on \nGalaxy-specific options\n.\n\n\nExecute\n FreeBayes.\n\n\nWhen it has run, rename the resulting VCF file to something shorter, such as \nNA12878.FreeBayes.chr20_2mb.vcf\n.\n\n\n\n\n\n\n\n\nCheck the generated list of variants\n.\n\n\n\n\nClick the eye icon to examine the VCF file contents.\n\n\nHow many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)\n\n\nWhat sort of quality scores do your variants have?\n\n\nOpen the VCF file in IGV using the dataset's \ndisplay in IGV local\n link (using the \nweb current\n link will open IGV again, and using \nlocal\n should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.\n\n\n\n\n\n\n\n\nSection 5. Calling variants with GATK Unified Genotyper\n\n\nFor comparison, we will call variants with a second variant caller.\n\n\nThe GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step.\n\n\nThe GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants.\n\n\nYou can \nread more about the GATK here\n.\n\n\n\n\n\n\nCall variants using Unified Genotyper.\n Under \nNGS COMMON TOOLSETS\n, select the tool \nNGS: GATK Tools 2.8 -\n Unified Genotyper\n.\n\n\n\n\nSelect your realigned BAM file as input, and select the correct reference genome.\n\n\nUnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under \ndbSNP ROD file\n, select the dataset \ndbsnp135_excludingsitesafter129_chr20.vcf\n.\n\n\nSet \nGenotype likelihoods calculation model to employ\n to \nSNP\n.\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nRename the file to something useful eg \nNA12878.GATK.chr20_2mb.vcf\n.\n\n\nThe output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files.\n\n\n\n\n\n\n\n\nCheck the generated list of variants.\n\n\n\n\nRoughly how many variants are there in your VCF file (how many lines in the dataset?)\n\n\nClick the eye icon to examine the contents of the VCF file. Notice that the \nID\n column (the third column) is populated with known SNP IDs from the VCF file we provided.\n\n\nNotice that the VCF header rows, and the corresponding information in the \nINFO\n column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the \nINFO\n column, so long as it adds header rows to describe the fields it uses.\n\n\nOpen the VCF file in IGV, using the link in the history panel.\n\n\nFind a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.\n\n\n\n\n\n\n\n\nSection 6. Evaluate variants\n\n\nHow can we evaluate our variants?\n\n\nWe've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample:\n\n\n\n\nWe expect to see true variations at the rate of about 1 per 1000bp against the reference genome\n\n\n85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP)\n\n\nA transition/transversion (Ti/Tv) rate of \n2 if the variants are high quality, even higher if the variants are in coding regions.\n\n\n\n\nYou can \nread more about SNP call set properties here\n.\n\n\nYou may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs).\n\n\nWe will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on?\n\n\n\n\n\n\nEvaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool.\n\n\n\n\nUnder \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -\n Eval Variants\n.\n\n\nUnder \nInput variant file\n, select the VCF file you generated with FreeBayes.\n\n\nClick \nInsert Variant\n to add a second input file, and under \nInput variant file\n, select the VCF file you generated with UnifiedGenotyper.\n\n\nSet the reference genome to \nhg19\n.\n\n\nProvide our list of known variants: make sure \nProvide a dbSNP Reference-Ordered Data (ROD) file\n is set to \nSet dbSNP\n and under \ndbSNP ROD file\n select the input reference variant file, \ndbSNP135_excludingsitesafter129.chr20.vcf\n.\n\n\nWe will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set \nBasic or Advanced Analysis options\n to \nAdvanced\n. Then set the following:\n\n\nEval modules to apply on the eval track(s)\n:\n\n\nCompOverlap\n\n\nTiTvVariantEvaluator\n\n\n\n\n\n\nDo not use the standard eval modules by default\n: check this option\n\n\n\n\n\n\nExecute\n.\n\n\n\n\n\n\n\n\nInterpret the dbSNP concordance section of the evaluation report.\n\n\n\n\nExamine the contents of the \u2018Eval Variants on data... (report)\u2019\n\n\nThe first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP.\n\n\nThe \nEvalRod\n column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively.\n\n\nThe \nCompRod\n column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp.\n\n\nNovelty\n: whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing \nall\n variants are the most informative summaries.\n\n\nnEvalVariants\n: number of variant sites in EvalRod (i.e. all called variants).\n\n\nnovelSites\n: number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP).\n\n\nCompOverlap\n: number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP).\n\n\ncompRate\n: percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants).\n\n\nThis metric is important, and it\u2019s what people generally refer to as \ndbSNP concordance\n.\n\n\n\n\n\n\nnConcordant\n and \nconcordantRate\n: number and percentage of overlapping variants that have the same genotype.\n\n\n\n\n\n\n\n\n\n\n\n\nInterpret the TiTv section of the evaluation report.\n\n\n\n\nThe second section of the report lists the transition/transversion ratio for different groups of variants from the callsets.\n\n\nIt generally follows the table format above. The most interesting metric is in the column labelled \ntiTvRatio\n.\n\n\nThe expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation.\n\n\nIn this table, it can be useful to compare not just the rows labelled \nall\n, but also those labelled \nknown\n and \nnovel\n. Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual?\n\n\n\n\n\n\n\n\nHow much overlap is there in the call sets?\n\n\n\n\nOne way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates.\n\n\nRemove the header lines from a VCF file: select the tool \nBASIC TOOLS -\n Filter and Sort -\nSelect\n.\n\n\nAs an input file, in \nSelect lines from\n, select the VCF file you generated using FreeBayes.\n\n\nSelect \nNOT Matching\n.\n\n\nAs the pattern, enter \"^#\". \n^\n in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line.\n\n\nExecute\n. This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have!\n\n\n\n\n\n\nRepeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper.\n\n\nCreate a Venn diagram: select the tool \nSTATISTICS AND VISUALISATION -\n Graph/Display Data -\n proportional venn\n.\n\n\nEnter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\".\n\n\nAs \nInput file 1\n, select the first of the filtered files you just generated.\n\n\nAs \nColumn index\n, enter 1. This is the second column, i.e. the column containing the position coordinate.\n\n\nUnder \nas name\n, enter a name for this input file, e.g. just \"FreeBayes\".\n\n\nAs \nInput file 2\n, select the second of the filtered VCF files.\n\n\nAgain as \nColumn index\n, enter 1.\n\n\nUnder \nas name\n, enter a name for this input file, e.g. just \"UnifiedGenotyper\".\n\n\nExecute\n.\n\n\n\n\n\n\nYou should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram:\n\n\n\"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and \nnot\n UnifiedGenotyper.\n\n\n\"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers.\n\n\n\n\n\n\nYou will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to  remove many false positives, but also a few true positives.\n\n\n\n\n\n\n\n\nSection 7. Annotation\n\n\nThe variants we have detected can be annotated with information from known reference data. This can include, for instance\n\n\n\n\nwhether the variant corresponds to a previously-observed variant in other samples, or across the population\n\n\nwhether the variant is inside or near a known gene\n\n\nwhether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc)\n\n\nwhether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated\n\n\n\n\n... and lots of other information!\n\n\nMost human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant.\n\n\nFor this workshop we will annotate our variants with the \nSnpEff\n tool, which has its own prebuilt annotation databases.\n\n\n\n\n\n\nAnnotate the detected variants.\n Select the tool \nNGS ANALYSIS -\n NGS: Annotation -\n SnpEff\n.\n\n\n\n\nChoose any of your generated lists of variants as the input VCF file in the first field.\n\n\nSelect \nVCF\n as the output format as well. This will add the annotated information to the INFO column of the VCF file.\n\n\nAs \nGenome source\n, select \nNamed on demand\n. Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed.\n\n\nMake sure \nProduce Summary Stats\n is set to \nYes\n.\n\n\nExecute\n SnpEff.\n\n\n\n\n\n\n\n\nExamine the annotated information.\n Click the eye icon on the resulting VCF file to view it.\n\n\n\n\nYou should see more information in the INFO column for each variant.\n\n\nYou should also see a few extra VCF header rows. In particular notice the new \nINFO=\nID=EFF...\n header row, listing the information added on the predicted effects of each variant.\n\n\nHave a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short \nEFF\n field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information.\n\n\nTry filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool \nBASIC TOOLS -\n Filter and Sort -\n Select\n on your annotated VCF file and filtering for lines matching the string \"missense_variant\".\n\n\n\n\n\n\n\n\nExamine the annotation summary stats.\n The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file.\n\n\n\n\nYou will see summaries of types of variants, impact of variants, functional regions etc.\n Go through these tables and graphs and see if you understand what they represent.\n\n\nWhere are most of the variants found (in exons, introns etc)? Does this match what you'd expect?", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#variant-detection-advanced-workshop", 
            "text": "", 
            "title": "Variant Detection - Advanced Workshop"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#tutorial-overview", 
            "text": "In this tutorial, we will look further at variant calling from sequence data. We will:   Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop  Carry out local realignment on our aligned reads  Compare the performance of different variant calling tools  Annotate our called variants with reference information", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#background", 
            "text": "Some background reading and reference material can be found  here .  The slides used in this workshop can be found  here .  Where is the data in this tutorial from?  The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#preparation", 
            "text": "Make sure you have an instance of Galaxy ready to go.   If you don't have your own - go to our  Galaxy-Tut  or  Galaxy-Melbourne  server.  Log in so that your work will be saved.\nIf you don't already have an account on this server, select from the menu  User -  Register  and create one.     Import data for the tutorial.    We will import a pair of FASTQ files containing paired-end reads, and a\n  VCF file of known human variants to use for variant evaluation.    Method 1: Paste/Fetch data from a URL to Galaxy.   In the Galaxy tools panel (left), under  BASIC TOOLS , click on  Get Data  and choose  Upload File .  Get the FASTQ files: click  Paste/Fetch data  and enter these URLs into the text box.\nIf you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2   Select  Type  as  fastqsanger  and click  Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which.  Get the VCF file: click  Paste/Fetch data  again to open a new text box, and paste the following URL into the box \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf   This time, you can leave the  Type  on Auto-detect. Click  Start .   Once the upload status for both sets of files turns  green , you can click  Close . You should now be able to see all three files in the Galaxy history panel (right).    Method 2: Upload local data to Galaxy.  (In most cases, you won't need this for the tutorial)   Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial.  In the Galaxy tools panel (left), under  BASIC TOOLS , click on  Get Data  and choose  Upload File .  Click  Choose local file  and select the downloaded  FASTQ files. Select  Type  as  fastqsanger  and click  Start .  Click  Choose local file  again and select the downloaded VCF file. Click  Start .  Once the upload status for all files turns  green , you can click  Close . You should now be able to see all three files in the Galaxy history panel (right).     Rename the datasets   You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names.  Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is  NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click  Save .  Similarly, rename the second dataset to  NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 .  Similarly, rename the third dataset to  dbSNP135_excludingsitesafter129.chr20.vcf .", 
            "title": "Preparation"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-1-quality-control", 
            "text": "The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome!   Analyse the quality of the reads in the FASTQ file.  From the left hand tool panel in Galaxy, under  NGS ANALYSIS , select  NGS: QC and manipulation -  FastQC  Select one of the FASTQ files as input and  Execute  the tool.  When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics.     Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).", 
            "title": "Section 1: Quality Control"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-2-alignment-and-depth-of-coverage", 
            "text": "In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome.  Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects.  We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment.    Map/align the reads with Bowtie2 to the human reference genome.  We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under  NGS ANALYSIS  in the tools panel, select the tool  NGS: Mapping -  Bowtie2 .   We have paired-end reads in two FASTQ files, so select  paired-end .  Select the two FASTQ files as inputs.  Under  Select reference genome  select the human genome  hg19 .  Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under  Set read groups information?  select  Set read groups (SAM/BAM specification) . (Picard-style should also work.)  Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier.  Set the sample name to \"NA12878\"  Set the platform to  ILLUMINA  Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction.    You can leave other read group information blank, and use default Bowtie2 settings.  Execute  the tool.  When the alignment has finished, you should rename the BAM file to something more convenient, such as  NA12878.chr20_2mb.30xPE.bam .  Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents.     Visualise the aligned BAM file with IGV.  The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server).   Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps.  In the green dataset box for your BAM file in the history panel, you will see some  display with IGV  links. Launch IGV by clicking the  web current  link. If IGV is already running on your computer, instead click the  local  link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download.  If  your BAM file was not automatically loaded, download and open it:  Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory.  In IGV, select the correct reference genome,  hg19 , in the top-left drop-down menu.  In IGV, open the BAM file using  File -  Load from File .    Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box).  Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome.  Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...)     Restrict the genomic region considered.  Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region.   Under  BASIC TOOLS , select the tool  Text manipulation -  Create single interval . Enter these values:  Chromosome: chr20  Start position: 0  End position: 2000000  Name: chr20_2mb  Strand: plus    Execute  this tool. This will create a small BED file specifying just one genomic region.  When the file is created, rename it to  chr20_2mb.bed . Have a look at the contents of this BED file.     Evaluate the depth of coverage of the aligned region.  Under  NGS COMMON TOOLSETS , select the tool  NGS: GATK Tools 2.8 -  Depth of Coverage .   Select the BAM file you just generated as the input BAM file.  Make sure the reference genome we aligned to is selected under  Using reference genome .  Set  Output format  to  table .  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of:  \u2018Depth of Coverage on data.... (output summary sample)\u2019:  this file  will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution.  Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling.  Also have a quick look at the  (per locus coverage)  file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome.    The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.", 
            "title": "Section 2: Alignment and depth of coverage"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-3-local-realignment", 
            "text": "Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome.  This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels.  Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels.  If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2.\nHowever performing local realignment will improve the accuracy of our variant calls.    Generate the list of indel regions to be realigned.  Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -  Realigner Target Creator . This tool will look for regions containing potential indels.   Select your BAM file as input.  Select the correct reference genome (the genome used for alignment).  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step.     Realign the subsets of reads around the target indel areas.  Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -  Indel Realigner . This step will realign reads and generate a new BAM file.   Select your BAM file as input.  Select the correct reference genome.  Under  Restrict realignment to provided intervals , select the intervals file generated in the previous step.  Execute .  Rename the file to something easier to recognise, e.g.  NA12878.chr20_2mb.30xPE.realigned.bam  To keep your history clean, you may want to delete the log files generated by GATK in the last two steps.     Compare the realigned BAM file to original BAM around an indel.   Open the realigned BAM file in IGV. You can use the  local  link to open it in your already-running IGV session and compare it to the pre-realignment BAM file.  Generally, the new BAM should appear identical to the old BAM except in the realigned regions.  Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions).  If you can\u2019t find anything obvious, check region chr20:1163914-1163954;\nyou should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.", 
            "title": "Section 3. Local realignment"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-4-calling-variants-with-freebayes", 
            "text": "FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It  can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome.  You can  read more about FreeBayes here .    Call variants with FreeBayes.  Under  NGS ANALYSIS , select the tool  NGS: Variant Analysis -  FreeBayes .   Select your realigned BAM file as input, and select the correct reference genome.  Under  Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on  Galaxy-specific options .  Execute  FreeBayes.  When it has run, rename the resulting VCF file to something shorter, such as  NA12878.FreeBayes.chr20_2mb.vcf .     Check the generated list of variants .   Click the eye icon to examine the VCF file contents.  How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)  What sort of quality scores do your variants have?  Open the VCF file in IGV using the dataset's  display in IGV local  link (using the  web current  link will open IGV again, and using  local  should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.", 
            "title": "Section 4. Calling variants with FreeBayes"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-5-calling-variants-with-gatk-unified-genotyper", 
            "text": "For comparison, we will call variants with a second variant caller.  The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step.  The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants.  You can  read more about the GATK here .    Call variants using Unified Genotyper.  Under  NGS COMMON TOOLSETS , select the tool  NGS: GATK Tools 2.8 -  Unified Genotyper .   Select your realigned BAM file as input, and select the correct reference genome.  UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under  dbSNP ROD file , select the dataset  dbsnp135_excludingsitesafter129_chr20.vcf .  Set  Genotype likelihoods calculation model to employ  to  SNP .  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  Rename the file to something useful eg  NA12878.GATK.chr20_2mb.vcf .  The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files.     Check the generated list of variants.   Roughly how many variants are there in your VCF file (how many lines in the dataset?)  Click the eye icon to examine the contents of the VCF file. Notice that the  ID  column (the third column) is populated with known SNP IDs from the VCF file we provided.  Notice that the VCF header rows, and the corresponding information in the  INFO  column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the  INFO  column, so long as it adds header rows to describe the fields it uses.  Open the VCF file in IGV, using the link in the history panel.  Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.", 
            "title": "Section 5. Calling variants with GATK Unified Genotyper"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-6-evaluate-variants", 
            "text": "How can we evaluate our variants?  We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample:   We expect to see true variations at the rate of about 1 per 1000bp against the reference genome  85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP)  A transition/transversion (Ti/Tv) rate of  2 if the variants are high quality, even higher if the variants are in coding regions.   You can  read more about SNP call set properties here .  You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs).  We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on?    Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool.   Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -  Eval Variants .  Under  Input variant file , select the VCF file you generated with FreeBayes.  Click  Insert Variant  to add a second input file, and under  Input variant file , select the VCF file you generated with UnifiedGenotyper.  Set the reference genome to  hg19 .  Provide our list of known variants: make sure  Provide a dbSNP Reference-Ordered Data (ROD) file  is set to  Set dbSNP  and under  dbSNP ROD file  select the input reference variant file,  dbSNP135_excludingsitesafter129.chr20.vcf .  We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set  Basic or Advanced Analysis options  to  Advanced . Then set the following:  Eval modules to apply on the eval track(s) :  CompOverlap  TiTvVariantEvaluator    Do not use the standard eval modules by default : check this option    Execute .     Interpret the dbSNP concordance section of the evaluation report.   Examine the contents of the \u2018Eval Variants on data... (report)\u2019  The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP.  The  EvalRod  column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively.  The  CompRod  column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp.  Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing  all  variants are the most informative summaries.  nEvalVariants : number of variant sites in EvalRod (i.e. all called variants).  novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP).  CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP).  compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants).  This metric is important, and it\u2019s what people generally refer to as  dbSNP concordance .    nConcordant  and  concordantRate : number and percentage of overlapping variants that have the same genotype.       Interpret the TiTv section of the evaluation report.   The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets.  It generally follows the table format above. The most interesting metric is in the column labelled  tiTvRatio .  The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation.  In this table, it can be useful to compare not just the rows labelled  all , but also those labelled  known  and  novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual?     How much overlap is there in the call sets?   One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates.  Remove the header lines from a VCF file: select the tool  BASIC TOOLS -  Filter and Sort - Select .  As an input file, in  Select lines from , select the VCF file you generated using FreeBayes.  Select  NOT Matching .  As the pattern, enter \"^#\".  ^  in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line.  Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have!    Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper.  Create a Venn diagram: select the tool  STATISTICS AND VISUALISATION -  Graph/Display Data -  proportional venn .  Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\".  As  Input file 1 , select the first of the filtered files you just generated.  As  Column index , enter 1. This is the second column, i.e. the column containing the position coordinate.  Under  as name , enter a name for this input file, e.g. just \"FreeBayes\".  As  Input file 2 , select the second of the filtered VCF files.  Again as  Column index , enter 1.  Under  as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\".  Execute .    You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram:  \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and  not  UnifiedGenotyper.  \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers.    You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to  remove many false positives, but also a few true positives.", 
            "title": "Section 6. Evaluate variants"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-7-annotation", 
            "text": "The variants we have detected can be annotated with information from known reference data. This can include, for instance   whether the variant corresponds to a previously-observed variant in other samples, or across the population  whether the variant is inside or near a known gene  whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc)  whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated   ... and lots of other information!  Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant.  For this workshop we will annotate our variants with the  SnpEff  tool, which has its own prebuilt annotation databases.    Annotate the detected variants.  Select the tool  NGS ANALYSIS -  NGS: Annotation -  SnpEff .   Choose any of your generated lists of variants as the input VCF file in the first field.  Select  VCF  as the output format as well. This will add the annotated information to the INFO column of the VCF file.  As  Genome source , select  Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed.  Make sure  Produce Summary Stats  is set to  Yes .  Execute  SnpEff.     Examine the annotated information.  Click the eye icon on the resulting VCF file to view it.   You should see more information in the INFO column for each variant.  You should also see a few extra VCF header rows. In particular notice the new  INFO= ID=EFF...  header row, listing the information added on the predicted effects of each variant.  Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short  EFF  field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information.  Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool  BASIC TOOLS -  Filter and Sort -  Select  on your annotated VCF file and filtering for lines matching the string \"missense_variant\".     Examine the annotation summary stats.  The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file.   You will see summaries of types of variants, impact of variants, functional regions etc.\n Go through these tables and graphs and see if you understand what they represent.  Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?", 
            "title": "Section 7. Annotation"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/", 
            "text": "Introduction to Variant detection\n\n\nBackground\n\n\nA variant is something that is different from a standard or type.\n\n\nThe aim of variation detection is to detect how many bases out of the total are different to a reference genome.\n\n\nIn Craig Venter\u2019s genome 4.1 million DNA variants were reported.\n\n\nWhat sort of variation could we find in the DNA sequencing?\n\n\n\n\nSingle nucleotide variations (SNVs)\n\n\nSingle nucleotide polymorphisms (SNPs)\n\n\nSmall insertions and deletions (INDELs)\n\n\nLarge Chromosome rearrangements-Structural variations (SV)\n\n\nCopy number variations (CNV)\n\n\n\n\nVariant Calling vs genotyping\n\n\nVariant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms.\n\n\nVariant callers estimate the probability of a particular genotype given the observed data.\n\n\nThe question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position.\n\n\nThe result of variant calling is a list of probable variants.\n\n\nProcess of variant calling\n\n\nSample DNA -\n Sequencing -\n Read alignment -\n BAM file of aligned reads against reference genome -\n Genotyper -\n Variant list\n\n\nThe number of reads that stack up on each other is called \nread coverage\n. The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.\n\n\nHomozygous or Heterozygous mutations:\n\n\nWhat should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.\n\n\nVariant Calling Software:\n\n\nThere a number of software available for variant calling some of which are as follows:\n\n\n\n\nSAMtools (mpileup and bcftools): Li 2009 Bioinformatics\n\n\nGATK: McKenna et al. 2010 Genome Res\n\n\nFreeBayes: MIT\n\n\nDiBayes: SOLiD software http://www.lifetechnologies.com\n\n\nInGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. \nBioinformatics\n\n\nMAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. \nNature Methods\n\n\n\n\nVariant Calling using Samtools (Mpileup + bcftools)\n\n\nSamtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods.\n\n\nMpileup: Input: BAM file Output: Pileuped up reads under the reference\n\n\nbcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes\n\n\nFurther information\n\n\nVariant Calling using GATK-Unified Genotyper\n\n\nGATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form.\n\n\nGATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods.\n\n\nInputs: BAM file\n\n\nOutput: VCF file with sites and genotypes.\n\n\nThe probability of a variant genotype for a given sequence of data is calculated using the \nBayes Theorem\n as follows:\n\n\nP(Genotype | Data) =  (P(Data | Genotype) * P(Genotype)) / P(Data)\n\n\n\n\nP(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype.\n\n\nP(Data | Genotype) is the probability of the data (the reads) given the genotype\n\n\nP(Data) is the probability of seeing the reads.\n\n\nGATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well.\n\n\nFurther information\n\n\nVariant Calling using FreeBayes\n\n\nFreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files).\n\n\nFurther information\n\n\nEvaluation of detected variants using Variant Eval\n\n\nThe identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set.\n\n\nThe results will have:\n\n\n\n\nTrue Positives (TP): The variants called by the software which are also a known variant in the known variants file.\n\n\nFalse Positives (FP): The Variants called by the software which are not known to be variants in the known variants file.\n\n\nTrue Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file.\n\n\nFalse Negatives (FN): The variants not called by the software which are known as variants in the known variants file.\n\n\n\n\nQuality Matrix:\n\n\nTP | FP\n---|----\nTN | FN\n\n\n\n\nSensitivity: TP/(TP+FN)\n\n\nSpecificity: TN/(TN+FP)\n\n\nNote:\n\n\nAlthough software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity.\n\n\nDNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping.\n\n\n\n\n\n\nTransition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\\n->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions.\n\n\n\n\n\n\nTransversion: a substitute of a purine for a pyrimidine.\n\n\n\n\n\n\nAlthough there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations.\n\n\nFor more details on variant eval visit: \nhttp://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html\n\n\nNotes:\n\n\nAn important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.\n\n\nLocal realignment\n\n\nIn order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: \nhttp://samtools.sourceforge.net/mpileup.shtml\n\n\nThe Galaxy workflow platform\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nThe Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\nData Format used in the tutorial\n\n\nSequence Alignment Map format\n\n\nSAM format\n\n\nSequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format:\n\n\n\n\n11 mandatory fields (+ variable number of optional fields)\n\n\n1 QNAME: Query name of the read\n\n\n2 FLAG\n\n\n3 RNAME: Reference sequence name\n\n\n4 POS: Position of alignment in reference sequence\n\n\n5 MAPQ: Mapping quality (Phred-scaled)\n\n\n6 CIGAR: String that describes the specifics of the alignment against the reference\n\n\n7 MRNM\n\n\n8 MPOS\n\n\n9 ISIZE\n\n\n10 SEQQuery: Sequence on the same strand as the reference\n\n\n11 QUAL: Query quality (ASCII-33=Phred base quality)\n\n\n\n\nSAM example\n\n\n    SRR017937.312 16 chr20 43108717 37 76M * 0 0\n    TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG\n    ?,@A=A\\\n5=,@==A:BB@=B9(.;A@B;\\\n@ABBB@@9BB@:@5\\\nBBBB9)\\\nBBB2\\\nBBB@BBB?;;BABBBBBBB@\n\n\n\n\nFor this example:\n\n\n\n\nQNAME = SRR017937.312\n - this is the name of this read\n\n\nFLAG = 16\n - see the format description below\n\n\nRNAME = chr20\n - this read aligns to chromosome 20\n\n\nPOS = 43108717\n - this read aligns the sequence on chr20 at position 43108717\n\n\nMAPQ = 37\n - this is quite a high quality score for the alignment (b/w 0 and 90)\n\n\nCIGAR = 76M\n - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field)\n\n\nMRNM = *\n - see the format description below\n\n\nMPOS = 0\n as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*.\n\n\nISIZE = 0\n as there is no mate for this read\n\n\nSEQQuery =\n the 76bp sequence of the reference segment\n\n\nQUAL =\n per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file\n\n\n\n\nSAM\n format is described more fully \nhere\n\n\nNOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself\n\n\nSee\nhttp://www.illumina.com/technology/paired_end_sequencing_assay.ilmn\n for an overview of paired-end sequencing.\n\n\nSAM file in Galaxy\n\n\n\n\nBinary Sequence Alignment Map format\n\n\nBAM format\n\n\nSAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM.\n\n\nData in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file\n\n\nBAM file in IGV\n\n\n\n\nVCF file format\n\n\nWhat is VCF file:\n\n\nThe \nVariant Call Format\n (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations.\n\n\nVCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official \nVCF spec\n for a more rigorous description of the format.\n\n\n\n\n\n\n\n\nCol\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCHROM\n\n\nChromosome name\n\n\n\n\n\n\n2\n\n\nPOS\n\n\n1-based position. For an indel, this is the position preceding the indel.\n\n\n\n\n\n\n3\n\n\nID\n\n\nVariant identifier. Usually the dbSNP rsID.\n\n\n\n\n\n\n4\n\n\nREF\n\n\nReference sequence at POS involved in the variant. For a SNP, it is a single base.\n\n\n\n\n\n\n5\n\n\nALT\n\n\nComma delimited list of alternative sequence(s).\n\n\n\n\n\n\n6\n\n\nQUAL\n\n\nPhred-scaled probability of all samples being homozygous reference.\n\n\n\n\n\n\n7\n\n\nFILTER\n\n\nSemicolon delimited list of filters that the variant fails to pass.\n\n\n\n\n\n\n8\n\n\nINFO\n\n\nSemicolon delimited list of variant information.\n\n\n\n\n\n\n9\n\n\nFORMAT\n\n\nColon delimited list of the format of individual genotypes in the following fields.\n\n\n\n\n\n\n10+\n\n\nSample(s)\n\n\nIndividual genotype information defined by FORMAT.\n\n\n\n\n\n\n\n\nVCF format in Galaxy:\n\n\n\n\nBcf file format:\n\n\n\n\nBCF format:\n\n\nBCF, or the binary variant call format, is the binary version of VCF. It keeps the same information in VCF, while much more efficient to process especially for many samples. The relationship between BCF and VCF is similar to that between BAM and SAM.", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#introduction-to-variant-detection", 
            "text": "", 
            "title": "Introduction to Variant detection"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#background", 
            "text": "A variant is something that is different from a standard or type.  The aim of variation detection is to detect how many bases out of the total are different to a reference genome.  In Craig Venter\u2019s genome 4.1 million DNA variants were reported.  What sort of variation could we find in the DNA sequencing?   Single nucleotide variations (SNVs)  Single nucleotide polymorphisms (SNPs)  Small insertions and deletions (INDELs)  Large Chromosome rearrangements-Structural variations (SV)  Copy number variations (CNV)", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-vs-genotyping", 
            "text": "Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms.  Variant callers estimate the probability of a particular genotype given the observed data.  The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position.  The result of variant calling is a list of probable variants.", 
            "title": "Variant Calling vs genotyping"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#process-of-variant-calling", 
            "text": "Sample DNA -  Sequencing -  Read alignment -  BAM file of aligned reads against reference genome -  Genotyper -  Variant list  The number of reads that stack up on each other is called  read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.", 
            "title": "Process of variant calling"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#homozygous-or-heterozygous-mutations", 
            "text": "What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.", 
            "title": "Homozygous or Heterozygous mutations:"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-software", 
            "text": "There a number of software available for variant calling some of which are as follows:   SAMtools (mpileup and bcftools): Li 2009 Bioinformatics  GATK: McKenna et al. 2010 Genome Res  FreeBayes: MIT  DiBayes: SOLiD software http://www.lifetechnologies.com  InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009.  Bioinformatics  MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009.  Nature Methods", 
            "title": "Variant Calling Software:"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-samtools-mpileup-bcftools", 
            "text": "Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods.  Mpileup: Input: BAM file Output: Pileuped up reads under the reference  bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes  Further information", 
            "title": "Variant Calling using Samtools (Mpileup + bcftools)"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-gatk-unified-genotyper", 
            "text": "GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form.  GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods.  Inputs: BAM file  Output: VCF file with sites and genotypes.  The probability of a variant genotype for a given sequence of data is calculated using the  Bayes Theorem  as follows:  P(Genotype | Data) =  (P(Data | Genotype) * P(Genotype)) / P(Data)  P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype.  P(Data | Genotype) is the probability of the data (the reads) given the genotype  P(Data) is the probability of seeing the reads.  GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well.  Further information", 
            "title": "Variant Calling using GATK-Unified Genotyper"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-freebayes", 
            "text": "FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files).  Further information", 
            "title": "Variant Calling using FreeBayes"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#evaluation-of-detected-variants-using-variant-eval", 
            "text": "The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set.  The results will have:   True Positives (TP): The variants called by the software which are also a known variant in the known variants file.  False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file.  True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file.  False Negatives (FN): The variants not called by the software which are known as variants in the known variants file.", 
            "title": "Evaluation of detected variants using Variant Eval"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#quality-matrix", 
            "text": "TP | FP\n---|----\nTN | FN  Sensitivity: TP/(TP+FN)  Specificity: TN/(TN+FP)  Note:  Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity.  DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping.    Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\ ->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions.    Transversion: a substitute of a purine for a pyrimidine.    Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations.  For more details on variant eval visit:  http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html  Notes:  An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.", 
            "title": "Quality Matrix:"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#local-realignment", 
            "text": "In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to:  http://samtools.sourceforge.net/mpileup.shtml", 
            "title": "Local realignment"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#the-galaxy-workflow-platform", 
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here  The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.", 
            "title": "The Galaxy workflow platform"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#data-format-used-in-the-tutorial", 
            "text": "", 
            "title": "Data Format used in the tutorial"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#sequence-alignment-map-format", 
            "text": "SAM format  Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format:   11 mandatory fields (+ variable number of optional fields)  1 QNAME: Query name of the read  2 FLAG  3 RNAME: Reference sequence name  4 POS: Position of alignment in reference sequence  5 MAPQ: Mapping quality (Phred-scaled)  6 CIGAR: String that describes the specifics of the alignment against the reference  7 MRNM  8 MPOS  9 ISIZE  10 SEQQuery: Sequence on the same strand as the reference  11 QUAL: Query quality (ASCII-33=Phred base quality)   SAM example      SRR017937.312 16 chr20 43108717 37 76M * 0 0\n    TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG\n    ?,@A=A\\ 5=,@==A:BB@=B9(.;A@B;\\ @ABBB@@9BB@:@5\\ BBBB9)\\ BBB2\\ BBB@BBB?;;BABBBBBBB@  For this example:   QNAME = SRR017937.312  - this is the name of this read  FLAG = 16  - see the format description below  RNAME = chr20  - this read aligns to chromosome 20  POS = 43108717  - this read aligns the sequence on chr20 at position 43108717  MAPQ = 37  - this is quite a high quality score for the alignment (b/w 0 and 90)  CIGAR = 76M  - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field)  MRNM = *  - see the format description below  MPOS = 0  as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*.  ISIZE = 0  as there is no mate for this read  SEQQuery =  the 76bp sequence of the reference segment  QUAL =  per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file   SAM  format is described more fully  here  NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself  See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn  for an overview of paired-end sequencing.  SAM file in Galaxy", 
            "title": "Sequence Alignment Map format"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#binary-sequence-alignment-map-format", 
            "text": "BAM format  SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM.  Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file  BAM file in IGV", 
            "title": "Binary Sequence Alignment Map format"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-file-format", 
            "text": "What is VCF file:  The  Variant Call Format  (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations.  VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official  VCF spec  for a more rigorous description of the format.     Col  Field  Description      1  CHROM  Chromosome name    2  POS  1-based position. For an indel, this is the position preceding the indel.    3  ID  Variant identifier. Usually the dbSNP rsID.    4  REF  Reference sequence at POS involved in the variant. For a SNP, it is a single base.    5  ALT  Comma delimited list of alternative sequence(s).    6  QUAL  Phred-scaled probability of all samples being homozygous reference.    7  FILTER  Semicolon delimited list of filters that the variant fails to pass.    8  INFO  Semicolon delimited list of variant information.    9  FORMAT  Colon delimited list of the format of individual genotypes in the following fields.    10+  Sample(s)  Individual genotype information defined by FORMAT.", 
            "title": "VCF file format"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-format-in-galaxy", 
            "text": "", 
            "title": "VCF format in Galaxy:"
        }, 
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#bcf-file-format", 
            "text": "", 
            "title": "Bcf file format:"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/", 
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 2px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.extra {\n        color: #444444;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq - Differential Gene Expression\n\n\nAuthors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a simulated dataset from the common fruit fly, Drosophila\nmelanogaster.\n\n\nThe tutorial is designed to introduce the tools, datatypes and workflows of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\ncontain sequencing and alignment errors that make analysis more difficult.\n\n\nIn this tutorial we will:  \n\n\n\n\nintroduce the types of files typically used in RNA-seq analysis\n\n\nalign RNA-seq reads with an aligner (HISAT2)\n\n\nvisualise RNA-seq alignment data with IGV\n\n\nuse a number of different methods to find differentially expressed genes\n\n\nunderstand the importance of replicates for differential expression analysis\n\n\n\n\nThis tutorial does not cover the following steps that might do in a real\nRNA-seq DGE analysis:  \n\n\n\n\nQC (quality control) of the raw sequence data\n\n\nTrimming the reads for quality and for adaptor sequences\n\n\nQC of the RNA-seq alignment data  \n\n\n\n\nThese steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with basic workflow of alignment, quantification, and testing,\n   for RNA-seq differential expression analysis\n\n\nBe able to process raw RNA sequence data into a list of differentially\n   expressed genes\n\n\nBe aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes\n\n\n\n\n\n\nThe data\n\n\nThe sequencing data you will be working with is simulated from Drosophila\nmelanogaster. The experiment has two conditioins, WT (wildtype) and KO\n(knockout), and three samples in each condition. The sequencing data is\npaired-end, so there are two files for each of the six samples. Your aim will\nbe to find differentially expressed genes in WT vs KO.\n\n\n\n\n\n\nSection 1: Preparation\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser \n Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser \n Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data \n Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Basic_2017\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data \n Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\n\n\nUpload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the WT flies. Make sure\n    the type is specified as 'fastqsanger' when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq\n    \n\n    \n\n    These six files are three paired-end samples from the KO flies.\n    Make sure the type is specified as 'fastqsanger' when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq\n    \n\n    \n\n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf\n    \n\n    You should now have 13 files in your history.\n\n\nNote:\n If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019\n\n\n\n\n\n\n3.  View and have an understanding of the files involved in RNA-seq analysis.\n\n\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n6 files containing paired-ended reads for the WT samples:\n\n\n\nWT_01_R1.fastq\n\n\nWT_01_R2.fastq\n\n\nWT_02_R1.fastq\n\n\nWT_02_R2.fastq\n\n\nWT_03_R1.fastq\n\n\nWT_03_R2.fastq\n\n\n\n\n6 files containing paired-ended reads for the KO samples:\n\n\n\nKO_01_R1.fastq\n\n\nKO_01_R2.fastq\n\n\nKO_02_R1.fastq\n\n\nKO_02_R2.fastq\n\n\nKO_03_R1.fastq\n\n\nKO_03_R2.fastq\n\n\n\n\nAnd 1 gene annotation file:\n\n\n\nensembl_dm3.chr4.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\n\n\n\n\nThese 12 sequencing files are in FASTQ format and have the file\n    extension: .fastq. If you are not familiar with the FASTQ format, \nclick\n    here for an overview\n.  \n\n\nEach condition has three samples, and each sample has two files (an R1\nfile containing forward reads and an R2 file containing reverse reads).\n\n\nClick on the \neye icon\n to the top right of any FASTQ file to view the\nfirst part of the file.\n\n\nNote:\n Since the reads in this dataset are synthetic, they do not have\nreal quality scores.\n\n\nNote:\n The reads are paired-end, i.e. WT_01_R1.fastq and\nWT_01_R2.fastq are paired reads from one sequencing run. If you're\nunfamiliar with paired-end sequencing, you can read about it\n\nhere\n.\n\n\n\n\n\n\nThe gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file\n    describes where the genes are located in the D. melanogaster reference\n    genome, filtered for genes on chromosome 4.\n    Each feature is defined by a chromosomal start and end point, feature type\n    (CDS, gene, exon etc), and parent gene and transcript.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n    More information on the GTF format can be found\n    \nhere\n.\n\n\n\n\n\n\n\n\nSection 2: Alignment with HISAT2\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. We will use HISAT to\nperform our alignment. HISAT2 is a fast, splice-aware, alignment program that\nis a successor to TopHat2. More information on\nHISAT2 can be found \nhere\n.\n\n\n1.  Align the RNA-seq reads to a reference genome.\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis \n HISAT2\n and set the parameters as follows:  \n\n\n\n\nInput data format\n FASTQ\n\n\nSingle end or paired reads?\n Individual paired reads\n\n\n\n\nForward reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)\n\n\n\n\nWT_01_R1.fastq\n\n\nWT_02_R1.fastq\n\n\nWT_03_R1.fastq\n\n\nKO_01_R1.fastq\n\n\nKO_02_R1.fastq\n\n\nKO_03_R1.fastq\n\n\n\n\n\n\n\n\nReverse reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the reverse\nFASTQ files ending in *2.fastq.)  \n\n\n\n\nWT_01_R2.fastq\n\n\nWT_02_R2.fastq\n\n\nWT_03_R2.fastq\n\n\nKO_01_R2.fastq\n\n\nKO_02_R2.fastq\n\n\nKO_03_R2.fastq\n\n\n\n\n\n\nSource for the reference genome to align against:\n Use\nbuilt-in genome\n\n\nSelect a reference genome:\n D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Examine the alignment stats\n\n\nHISAT2 outputs one bam file for each set of paired-end read files. Rename the 6\nfiles into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam')\nby using the \npen icon\n next to the file.\n\n\nThese files are BAM files (short for\n\nBinary Alignment Map\n)\nand like the name suggests, is a binary file. This means we can't use the\neye icon to view the data in Galaxy; we need to use software that can read the\nfile or convert it into it's plain-text equivalent (SAM) to view it as text.\nIn section 3, we'll use a genome viewer to view our alignments.\n\n\nHISAT2 also outputs some information to stderr which we can preview by\nclicking on the dataset name. To view the raw file, click the \"info\" button\n(view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\"\nrow under \"Job Information\" in the table. Click the \"stderr\" link to view\nthe alignment summary output.\n\n\n16046 reads; of these:\n  16046 (100.00%) were paired; of these:\n    104 (0.65%) aligned concordantly 0 times\n    13558 (84.49%) aligned concordantly exactly 1 time\n    2384 (14.86%) aligned concordantly \n1 times\n    ----\n    104 pairs aligned concordantly 0 times; of these:\n      1 (0.96%) aligned discordantly 1 time\n    ----\n    103 pairs aligned 0 times concordantly or discordantly; of these:\n      206 mates make up the pairs; of these:\n        106 (51.46%) aligned 0 times\n        91 (44.17%) aligned exactly 1 time\n        9 (4.37%) aligned \n1 times\n99.67% overall alignment rate\n\n\n\n\nHere we see we have a very high alignment rate, which is expected since the\ndata we have is simulated and has no contamination.\n\n\n\n\nSection 3: Visualise the aligned reads\n\n\nThe purpose of this step is to :\n\n\n\n\nvisualise the quantitative, exon-based nature of RNA-seq data\n\n\nvisualise the expression differences between samples represented by the\n  quantity of reads, and\n\n\nbecome familiar with the \nIntegrative Genomics Viewer\n  (IGV)\n-- an interactive\n  visualisation tool by the Broad Institute.  \n\n\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on one of the BAM files, for example 'WT_01.bam'.\n\n\nClick on Display with IGV \n'webcurrent'\n (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)\n\n\nOnce IGV opens, it will show you the BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)\n\n\nSelect \nchr4\n from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)\n\n\n\n\nView differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (WT) and condition 2 (KO).\n\n\nSelect 'KO_02.bam' and click on 'display with IGV local'. This time we are\nusing the \n'local'\n link, as we already have an IGV window up and running\nlocally from the last step. Once the file has loaded, try to find some\ngenes that look differentially expressed.\n\n\n\n\nIf you can't find any, try changing the location to\n\nchr4:816349-830862\n using the field on the top toolbar.\nThe 'Sox102F' gene in this area looks like it has many more reads\nmapped in WT than in KO. Hover over the coverage track to view the read\ndepth of the area. But, of course, it may be that there are many more\nreads in the library for WT than KO. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.\n\n\n\n\n\n\n[Optional]\n Visualise the aligned reads in Trackster\n\nIf you have trouble getting IGV to work, you can also use the inbuilt\nGalaxy genome browser, Trackster, to visualise alignments. Trackster has fewer\nfeatures than IGV, but sometimes it may be more convenient to use as it only\nrequires the browser.\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization \n New Track Browser\n.\n\n\nName your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select WT_01.bam and\n    KO_01.bam by using the checkboxes on the left.\n\n\nSelect chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.\n\n\nNext to the drop down list, click on the chromosomal position number\n    display and specify the location \nchr4:816349-830862\n.  \n\n\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 4. Quantification\n\n\nHTSeq-count counts the number of the reads from each bam file that map to the\ngenomic features in the provided annotation file. For each feature (a\ngene for example) we will obtain a numerical value associated with the\nexpression of that feature in our sample (i.e. the number of reads that\nwere aligned to that gene).\n\n\n1.  Examine the GTF file\n\n\nClick on the \neye icon\n to display the ensembl_dm3.chr4.gtf file in Galaxy.\n\n\nThis GTF file is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found\n\nhere\n.\n\n\nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster.\n\n\n2.  Run HTSeq-count\n\n\n\n\n\n\nUse HTSeq-count to count the number of reads for each feature.\n\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis \n htseq-count\n and set the parameters as follows:  \n\n\n\n\nAligned SAM/BAM File:\n\n  (Select 'Multiple datasets', then select all six bam files using the shift key.)\n\n\nWT_01.bam\n\n\nWT_02.bam\n\n\nWT_03.bam\n\n\nKO_01.bam\n\n\nKO_02.bam\n\n\nKO_03.bam\n\n\n\n\n\n\nGFF File:\n ensembl_dm3.chr4.gtf\n\n\nStranded:\n No\n\n\nID Attribute:\n gene_name\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nIn the previous step, each input BAM file outputted two files. The first\n    file contains the counts for each of our genes. The second file\n    (ending with \"(no feature)\") contains the stats for the reads that weren't\n    able to be uniquely aligned to a gene. We don't need the \"(no feature)\"\n    files so we can remove then with the delete \"X\" button on the top right.\n\n\n\n\n\n\nRename the remaining six files from htseq-count to meaningful names,\n    such as WT_01, WT_02, etc.\n\n\n\n\n\n\n3.  Generate a count matrix\n\n\n\n\nGenerate a combined count matrix by combining our six files.\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis \n Generate count matrix\n and set the parameters as follows:  \n\n\nCount files from your history:\n\n    (Select all six count files using the shift key.)\n\n\nWT_01\n\n\nWT_02\n\n\nWT_03\n\n\nKO_01\n\n\nKO_02\n\n\nKO_03\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nExamine the outputted matrix by using the \neye icon\n.\n\nEach column corresponds to a sample and each row corresponds to a gene. By\nsight, see if you can find a gene you think is differentially expressed\njust by looking at the counts.\n\n\nWe now have a count matrix which we will now use to find differentially\nexpressed genes between WT samples and KO samples.\n\n\n\n\nSection 5. Degust\n\n\nDegust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at \ndegust.erc.monash.edu/\n.\n\n\n\n\n1. Load count data into Degust\n\n\n\n\nIn Galaxy, download the count matrix you generated in the last section\n    using the \ndisk icon\n.\n\n\nGo to \ndegust.erc.monash.edu/\n\n    and click on \"Upload your counts file\".\n\n\nClick \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.\n\n\n\n\n2. Configure your uploaded data\n\n\n\n\nGive your visualisation a name.\n\n\nFor the Info column, select \"gene_id\".\n\n\nAdd two conditions: WT and KO. For each condition, select the three\n    samples which correspond with the condition.\n\n\nSet min gene CPM to 1 in at least 3 samples.\n\n\nClick \nSave changes\n and view your data.\n\n\n\n\n\n\nRead through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.\n\n\n\n\nOn the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.\n\n\nOn the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.\n\n\n4. Explore the demo data\n\n\nDegust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.\n\n\n\n\nSection 5. DESeq2\n\n\nIn this section we'll use the \"DESeq2\" tool in Galaxy to do our differential\ngene analysis. This tool uses the separate HTSeq files we generated in section\n4.\n\n\nSimilar to Voom/Limma or edgeR that was used in Degust to statistically test\nour data, DESeq2 will:\n\n\n\n\nstatistically test for expression differences in normalised read counts for\n    each gene, taking into account the variance observed between samples,  \n\n\nfor each gene, calculate the p-value of the gene being differentially\n    expressed-- this is the probability of seeing the data or something more\n    extreme given the null hypothesis (that the gene is not differentially\n    expressed between the two conditions),\n\n\nfor each gene, estimate the fold change in expression between the two\n    conditions.\n\n\n\n\n\n\n\n\n\nUse DESeq2 to find differentially expressed features from the count data.\n\n    In the left tool panel menu, under NGS Analysis, select \nNGS: RNA Analysis\n    \n DESeq2\n and set the parameters as follows:\n\n\n1: Factor\n\n\nSpecify a factor name:\n condition\n\n\n1: Factor level:\n\n\nSpecify a factor level:\n WT\n\n(Select the three WT htseq-count files.)\n\n\nWT_01\n\n\nWT_02\n\n\nWT_03\n\n\n\n\n\n\n\n\n\n\n2: Factor level:\n\n\nSpecify a factor level:\n KO\n\n(Select the three KO htseq-count files.)\n\n\nKO_01\n\n\nKO_02\n\n\nKO_03\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHave a look at the outputs of DESeq2. We will now filter significant\n    (adjusted p-value \n 0.05) genes from the DESeq2 result file.\n\n    Under Basic Tools, click on \nFilter and Sort \n Filter\n:\n\n\nFilter:\n \"DESeq2 result file on data ...\"\n\n\nWith following condition:\n c7 \n 0.05\n\n\nExecute\n\n\n\n\n\n\n\n\nHow many differentially expressed genes with adjusted p-value \n 0.05 are there?\n\n\n\n\nSection 6. The importance of replicates\n\n\n\n\n\n\nRepeat the previous differential expression analysis with two samples in\n    each group instead of three. How do you expect your results to differ when\n    using fewer samples?\n\n\n\n\n\n\nFilter genes with adjusted-p-value \n 0.05. How many genes are significant?\n\n\n\n\n\n\nRun DESeq2 again, using only one sample from each group. How many genes are\n    now significant?\n\n\n\n\n\n\nCan you find genes that were identified as differentially expressed when\n    using three samples in each condition that were not identified as\n    differentially expressed when using two samples? What do you expect these\n    gene's counts or logFC values to look like compared to genes that remained\n    statistically significance? Have a look at the counts or the logFC values\n    of these genes.\n\n\n\n\n\n\nThe identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.\n\n\nMyoglianin is an example of a gene that showed up as differentially expressed\nwhen we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion.\nIf we say that genes like Myoglianin was \ntruly\n differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.\n\n\nIt is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.\n\n\n\n\nOptional extension\n\n\nHave a go at doing another differential expression analysis with the following\nSaccharomyces cerevisiae data from chromosome I. This time, the two conditions\nare called 'batch' and 'chem', and like before, there are three samples per\ncondition.\n\n\nBatch sequence data:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n\n\n\n\nChem sequence data:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n\n\n\n\nGene annotation:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#rna-seq-differential-gene-expression", 
            "text": "Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie", 
            "title": "RNA-Seq - Differential Gene Expression"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#tutorial-overview", 
            "text": "In this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a simulated dataset from the common fruit fly, Drosophila\nmelanogaster.  The tutorial is designed to introduce the tools, datatypes and workflows of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\ncontain sequencing and alignment errors that make analysis more difficult.  In this tutorial we will:     introduce the types of files typically used in RNA-seq analysis  align RNA-seq reads with an aligner (HISAT2)  visualise RNA-seq alignment data with IGV  use a number of different methods to find differentially expressed genes  understand the importance of replicates for differential expression analysis   This tutorial does not cover the following steps that might do in a real\nRNA-seq DGE analysis:     QC (quality control) of the raw sequence data  Trimming the reads for quality and for adaptor sequences  QC of the RNA-seq alignment data     These steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#learning-objectives", 
            "text": "At the end of this tutorial you should:   Be familiar with basic workflow of alignment, quantification, and testing,\n   for RNA-seq differential expression analysis  Be able to process raw RNA sequence data into a list of differentially\n   expressed genes  Be aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#the-data", 
            "text": "The sequencing data you will be working with is simulated from Drosophila\nmelanogaster. The experiment has two conditioins, WT (wildtype) and KO\n(knockout), and three samples in each condition. The sequencing data is\npaired-end, so there are two files for each of the six samples. Your aim will\nbe to find differentially expressed genes in WT vs KO.", 
            "title": "The data"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-1-preparation", 
            "text": "1.  Register as a new user in Galaxy if you don\u2019t already have an account   Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User   Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User   Login .   2.  Import the RNA-seq data for the workshop.  If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data   Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Basic_2017 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data   Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.   Upload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the WT flies. Make sure\n    the type is specified as 'fastqsanger' when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq\n     \n     \n    These six files are three paired-end samples from the KO flies.\n    Make sure the type is specified as 'fastqsanger' when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq\n     \n     \n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf\n     \n    You should now have 13 files in your history.  Note:  If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019    3.  View and have an understanding of the files involved in RNA-seq analysis.    You should now have the following files in your Galaxy history:  6 files containing paired-ended reads for the WT samples:  WT_01_R1.fastq  WT_01_R2.fastq  WT_02_R1.fastq  WT_02_R2.fastq  WT_03_R1.fastq  WT_03_R2.fastq   6 files containing paired-ended reads for the KO samples:  KO_01_R1.fastq  KO_01_R2.fastq  KO_02_R1.fastq  KO_02_R2.fastq  KO_03_R1.fastq  KO_03_R2.fastq   And 1 gene annotation file:  ensembl_dm3.chr4.gtf   These files can be renamed by clicking the  pen icon  if you wish.    These 12 sequencing files are in FASTQ format and have the file\n    extension: .fastq. If you are not familiar with the FASTQ format,  click\n    here for an overview .    Each condition has three samples, and each sample has two files (an R1\nfile containing forward reads and an R2 file containing reverse reads).  Click on the  eye icon  to the top right of any FASTQ file to view the\nfirst part of the file.  Note:  Since the reads in this dataset are synthetic, they do not have\nreal quality scores.  Note:  The reads are paired-end, i.e. WT_01_R1.fastq and\nWT_01_R2.fastq are paired reads from one sequencing run. If you're\nunfamiliar with paired-end sequencing, you can read about it here .    The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file\n    describes where the genes are located in the D. melanogaster reference\n    genome, filtered for genes on chromosome 4.\n    Each feature is defined by a chromosomal start and end point, feature type\n    (CDS, gene, exon etc), and parent gene and transcript.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n    More information on the GTF format can be found\n     here .", 
            "title": "Section 1: Preparation"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-2-alignment-with-hisat2", 
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. We will use HISAT to\nperform our alignment. HISAT2 is a fast, splice-aware, alignment program that\nis a successor to TopHat2. More information on\nHISAT2 can be found  here .  1.  Align the RNA-seq reads to a reference genome.  In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis   HISAT2  and set the parameters as follows:     Input data format  FASTQ  Single end or paired reads?  Individual paired reads   Forward reads: \n(Click on the  multiple datasets icon  and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)   WT_01_R1.fastq  WT_02_R1.fastq  WT_03_R1.fastq  KO_01_R1.fastq  KO_02_R1.fastq  KO_03_R1.fastq     Reverse reads: \n(Click on the  multiple datasets icon  and select all six of the reverse\nFASTQ files ending in *2.fastq.)     WT_01_R2.fastq  WT_02_R2.fastq  WT_03_R2.fastq  KO_01_R2.fastq  KO_02_R2.fastq  KO_03_R2.fastq    Source for the reference genome to align against:  Use\nbuilt-in genome  Select a reference genome:  D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)  Use defaults for the other fields  Execute    Note: This may take a few minutes, depending on how busy the server is.  2.  Examine the alignment stats  HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6\nfiles into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam')\nby using the  pen icon  next to the file.  These files are BAM files (short for Binary Alignment Map )\nand like the name suggests, is a binary file. This means we can't use the\neye icon to view the data in Galaxy; we need to use software that can read the\nfile or convert it into it's plain-text equivalent (SAM) to view it as text.\nIn section 3, we'll use a genome viewer to view our alignments.  HISAT2 also outputs some information to stderr which we can preview by\nclicking on the dataset name. To view the raw file, click the \"info\" button\n(view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\"\nrow under \"Job Information\" in the table. Click the \"stderr\" link to view\nthe alignment summary output.  16046 reads; of these:\n  16046 (100.00%) were paired; of these:\n    104 (0.65%) aligned concordantly 0 times\n    13558 (84.49%) aligned concordantly exactly 1 time\n    2384 (14.86%) aligned concordantly  1 times\n    ----\n    104 pairs aligned concordantly 0 times; of these:\n      1 (0.96%) aligned discordantly 1 time\n    ----\n    103 pairs aligned 0 times concordantly or discordantly; of these:\n      206 mates make up the pairs; of these:\n        106 (51.46%) aligned 0 times\n        91 (44.17%) aligned exactly 1 time\n        9 (4.37%) aligned  1 times\n99.67% overall alignment rate  Here we see we have a very high alignment rate, which is expected since the\ndata we have is simulated and has no contamination.", 
            "title": "Section 2: Alignment with HISAT2"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-3-visualise-the-aligned-reads", 
            "text": "The purpose of this step is to :   visualise the quantitative, exon-based nature of RNA-seq data  visualise the expression differences between samples represented by the\n  quantity of reads, and  become familiar with the  Integrative Genomics Viewer\n  (IGV) -- an interactive\n  visualisation tool by the Broad Institute.     To visualise the alignment data:   Click on one of the BAM files, for example 'WT_01.bam'.  Click on Display with IGV  'webcurrent'  (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)  Once IGV opens, it will show you the BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)  Select  chr4  from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)   View differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (WT) and condition 2 (KO).  Select 'KO_02.bam' and click on 'display with IGV local'. This time we are\nusing the  'local'  link, as we already have an IGV window up and running\nlocally from the last step. Once the file has loaded, try to find some\ngenes that look differentially expressed.   If you can't find any, try changing the location to chr4:816349-830862  using the field on the top toolbar.\nThe 'Sox102F' gene in this area looks like it has many more reads\nmapped in WT than in KO. Hover over the coverage track to view the read\ndepth of the area. But, of course, it may be that there are many more\nreads in the library for WT than KO. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.    [Optional]  Visualise the aligned reads in Trackster \nIf you have trouble getting IGV to work, you can also use the inbuilt\nGalaxy genome browser, Trackster, to visualise alignments. Trackster has fewer\nfeatures than IGV, but sometimes it may be more convenient to use as it only\nrequires the browser.   On the top bar of Galaxy, select  Visualization   New Track Browser .  Name your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select WT_01.bam and\n    KO_01.bam by using the checkboxes on the left.  Select chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.  Next to the drop down list, click on the chromosomal position number\n    display and specify the location  chr4:816349-830862 .     Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.", 
            "title": "Section 3: Visualise the aligned reads"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-4-quantification", 
            "text": "HTSeq-count counts the number of the reads from each bam file that map to the\ngenomic features in the provided annotation file. For each feature (a\ngene for example) we will obtain a numerical value associated with the\nexpression of that feature in our sample (i.e. the number of reads that\nwere aligned to that gene).  1.  Examine the GTF file  Click on the  eye icon  to display the ensembl_dm3.chr4.gtf file in Galaxy.  This GTF file is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found here . \nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster.  2.  Run HTSeq-count    Use HTSeq-count to count the number of reads for each feature. \n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis   htseq-count  and set the parameters as follows:     Aligned SAM/BAM File: \n  (Select 'Multiple datasets', then select all six bam files using the shift key.)  WT_01.bam  WT_02.bam  WT_03.bam  KO_01.bam  KO_02.bam  KO_03.bam    GFF File:  ensembl_dm3.chr4.gtf  Stranded:  No  ID Attribute:  gene_name  Use defaults for the other fields  Execute     In the previous step, each input BAM file outputted two files. The first\n    file contains the counts for each of our genes. The second file\n    (ending with \"(no feature)\") contains the stats for the reads that weren't\n    able to be uniquely aligned to a gene. We don't need the \"(no feature)\"\n    files so we can remove then with the delete \"X\" button on the top right.    Rename the remaining six files from htseq-count to meaningful names,\n    such as WT_01, WT_02, etc.    3.  Generate a count matrix   Generate a combined count matrix by combining our six files.\n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis   Generate count matrix  and set the parameters as follows:    Count files from your history: \n    (Select all six count files using the shift key.)  WT_01  WT_02  WT_03  KO_01  KO_02  KO_03    Use defaults for the other fields  Execute     Examine the outputted matrix by using the  eye icon . \nEach column corresponds to a sample and each row corresponds to a gene. By\nsight, see if you can find a gene you think is differentially expressed\njust by looking at the counts.  We now have a count matrix which we will now use to find differentially\nexpressed genes between WT samples and KO samples.", 
            "title": "Section 4. Quantification"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-degust", 
            "text": "Degust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at  degust.erc.monash.edu/ .   1. Load count data into Degust   In Galaxy, download the count matrix you generated in the last section\n    using the  disk icon .  Go to  degust.erc.monash.edu/ \n    and click on \"Upload your counts file\".  Click \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.   2. Configure your uploaded data   Give your visualisation a name.  For the Info column, select \"gene_id\".  Add two conditions: WT and KO. For each condition, select the three\n    samples which correspond with the condition.  Set min gene CPM to 1 in at least 3 samples.  Click  Save changes  and view your data.    Read through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.   On the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.  On the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.  4. Explore the demo data  Degust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.", 
            "title": "Section 5. Degust"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-deseq2", 
            "text": "In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential\ngene analysis. This tool uses the separate HTSeq files we generated in section\n4.  Similar to Voom/Limma or edgeR that was used in Degust to statistically test\nour data, DESeq2 will:   statistically test for expression differences in normalised read counts for\n    each gene, taking into account the variance observed between samples,    for each gene, calculate the p-value of the gene being differentially\n    expressed-- this is the probability of seeing the data or something more\n    extreme given the null hypothesis (that the gene is not differentially\n    expressed between the two conditions),  for each gene, estimate the fold change in expression between the two\n    conditions.     Use DESeq2 to find differentially expressed features from the count data. \n    In the left tool panel menu, under NGS Analysis, select  NGS: RNA Analysis\n      DESeq2  and set the parameters as follows:  1: Factor  Specify a factor name:  condition  1: Factor level:  Specify a factor level:  WT \n(Select the three WT htseq-count files.)  WT_01  WT_02  WT_03      2: Factor level:  Specify a factor level:  KO \n(Select the three KO htseq-count files.)  KO_01  KO_02  KO_03      Use defaults for the other fields  Execute         Have a look at the outputs of DESeq2. We will now filter significant\n    (adjusted p-value   0.05) genes from the DESeq2 result file. \n    Under Basic Tools, click on  Filter and Sort   Filter :  Filter:  \"DESeq2 result file on data ...\"  With following condition:  c7   0.05  Execute     How many differentially expressed genes with adjusted p-value   0.05 are there?", 
            "title": "Section 5. DESeq2"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-6-the-importance-of-replicates", 
            "text": "Repeat the previous differential expression analysis with two samples in\n    each group instead of three. How do you expect your results to differ when\n    using fewer samples?    Filter genes with adjusted-p-value   0.05. How many genes are significant?    Run DESeq2 again, using only one sample from each group. How many genes are\n    now significant?    Can you find genes that were identified as differentially expressed when\n    using three samples in each condition that were not identified as\n    differentially expressed when using two samples? What do you expect these\n    gene's counts or logFC values to look like compared to genes that remained\n    statistically significance? Have a look at the counts or the logFC values\n    of these genes.    The identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.  Myoglianin is an example of a gene that showed up as differentially expressed\nwhen we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion.\nIf we say that genes like Myoglianin was  truly  differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.  It is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.", 
            "title": "Section 6. The importance of replicates"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#optional-extension", 
            "text": "Have a go at doing another differential expression analysis with the following\nSaccharomyces cerevisiae data from chromosome I. This time, the two conditions\nare called 'batch' and 'chem', and like before, there are three samples per\ncondition.  Batch sequence data: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq  \nChem sequence data: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq  \nGene annotation: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf", 
            "title": "Optional extension"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/", 
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq Differential Gene Expression: Basic Tutorial\n\n\nAuthors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a small synthetic dataset from the model organism,\nDrosophila melanogaster.\n\n\nThe tutorial is designed to introduce the tools, datatypes and workflow of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\nwould contain sequencing and alignment errors that make analysis more difficult.\n\n\nOur input data for this tutorial will be raw RNA-seq reads from two\nexperimental conditions and we will output a list of differentially\nexpressed genes identified to be statistically significant.\n\n\nIn this tutorial we will:  \n\n\n\n\nintroduce the types of files typically used in RNA-seq analysis\n\n\nalign RNA-seq reads with Tophat\n\n\nvisualise RNA-seq alignment data with IGV\n\n\nfind differentially expressed genes with Cuffdiff\n\n\nunderstand the importance of replicates for differential expression analysis\n\n\n\n\nThis tutorial does not cover the following steps that you would do in a real\nRNA-seq DGE analysis:  \n\n\n\n\nQC (quality control) of the raw sequence data\n\n\nTrimming the reads for quality and for adaptor sequences\n\n\nQC of the RNA-seq alignment data  \n\n\n\n\nThese steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with the Tuxedo Protocol workflow for RNA-seq differential\n   expression analysis\n\n\nBe able to process raw RNA sequence data into a list of differentially\n   expressed genes\n\n\nBe aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes\n\n\n\n\n\n\nBackground\n\n\nWhere does the data in this tutorial come from?\n\n\nThe data for this tutorial is from an RNA-seq experiment looking for\ndifferentially expressed genes in D. melanogaster (fruit fly) between two\nexperimental conditions. The experiment and analysis protocol we will follow\nis derived from a paper in Nature Protocols by the research group responsible\nfor one of the most widely used set of RNA-seq analysis tools: \n\"Differential\ngene and transcript expression analysis of RNA-seq experiments with TopHat and\nCufflinks\"\n (Trapnell et al 2012).\n\n\nThe sequence datasets are single-end Illumina synthetic short reads,\nfiltered to only include chromosome 4 to facilitate faster mapping (which\nwould otherwise take hours). We\u2019ll use data from three biological replicates\nfrom each of the two experimental conditions.\n\n\nThe Tuxedo Protocol\n\n\nThe workflow this tutorial is based on is the Tuxedo Protocol. Reads are first\nmapped with TopHat and a transcriptome is then assembled using Cufflinks.\nCuffdiff then quantifies the expression in each condition, and tests for\ndifferential expression.\n\n\nIn this tutorial we use a simpler protocol as the D. melanogaster\ntranscriptome is already very well characterised.\n\n\nMore information about the Tuxedo protocol can be found \nhere\n.\n\n\n\n\nSection 1: Preparation [15 min]\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account (\nwhat is Galaxy?\n)\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser \n Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser \n Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data \n Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Basic_Sec_1\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data \n Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\nUpload the sequence data by pasting the following links into the text\n    input area:\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq\n    \n\n    \n\n    Select the type as 'fastqsanger' and press \nstart\n to upload the\n    files to Galaxy.\n\n\nUpload the annotated gene list reference by pasting the following link\n    into the text input area:\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf\n    \n\n    You don't need to specify the type for this file as Galaxy will\n    auto-detect the file as a GTF file.\n\n\n\n\n3.  View and have an understanding of the files involved in RNA-seq analysis.\n\n\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n6 files containing single-ended reads:\n\n\n\nC1_R1.chr4.fq\n\nC1_R2.chr4.fq\n\nC1_R3.chr4.fq\n\nC2_R1.chr4.fq\n\nC2_R2.chr4.fq\n\nC2_R3.chr4.fq\n\n\n\n\nAnd 1 gene annotation file:\n\n\n\nensembl_dm3.chr4.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\n\n\n\n\nThese 6 sequencing files are in FASTQ format and have the file\n    extension: .fq. If you are not familiar with the FASTQ format, \nclick\n    here for an overview\n.  \n\n\nClick on the \neye icon\n to the top right of each FASTQ file to view the\nfirst part of the file.\nThe first 3 files are from the first condition (C1) and has 3\nreplicates labelled R1, R2, and R3. The next 3 FASTQ files are from\nthe second condition (C2) and has 3 replicates labelled R1, R2, and R3.\n\n\nIn this tutorial, we aim to find genes which are differentially\nexpressed between condition C1 and condition C2.\n\n\n\n\n\n\nThe gene annotation file is in GTF format. This file describes where\n    the genes are located in the Drosophila reference genome.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n\n\n\n\n\n\nNOTE:\n Since the reads in this dataset are synthetic, they do not have\nreal quality scores.\n\n\nNOTE:\n If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019\n\n\n\n\nSection 2: Align reads with Tophat [30 mins]\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found \nhere\n.\n\n\n1.  Align the RNA-seq short reads to a reference genome.\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis \n Tophat\n and set the parameters as follows:  \n\n\n\n\nIs this single-end or paired-end data?\n Single-end  \n\n\nRNA-Seq FASTQ file:\n\n  (Click on the multiple datasets icon and select all six of the FASTQ\n  files. This can be done by holding down the shift key to select a range\n  of files, or holding down the ctrl key (Windows) or command key (OSX) and\n  clicking to select multiple files.)\n\n\nC1_R1.chr4.fq\n\n\nC1_R2.chr4.fq\n\n\nC1_R3.chr4.fq\n\n\nC2_R1.chr4.fq\n\n\nC2_R2.chr4.fq\n\n\nC2_R3.chr4.fq  \n\n\n\n\n\n\nUse a built in reference genome or own from your history:\n Use\n  built-in genome\n\n\nSelect a reference genome:\n D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Examine the output files\n\n\nYou should have 5 output files for each of the FASTQ input files:\n\n\n\n\nTophat on data 1: accepted_hits:\n This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.\n\n\nTophat on data 1: splice junctions:\n This file lists all the places\n  where Tophat had to split a read into two pieces to span an exon\n  junction.\n\n\nTophat on data 1: deletions\n and \nTophat on data 1: insertions:\n\n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.\n\n\nTophat on data 1: align_summary:\n This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.\n\n\n\n\nYou should have a total of 30 Tophat output files in your history.\n\n\n3.  Visualise the aligned reads with IGV\n\n\nThe purpose of this step is to :\n\n\n\n\nvisualise the quantitative, exon-based nature of RNA-seq data\n\n\nvisualise the expression differences between samples represented by the\n  quantity of reads, and\n\n\nbecome familiar with the \nIntegrative Genomics Viewer\n  (IGV)\n-- an interactive\n  visualisation tool by the Broad Institute.  \n\n\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on one of the Tophat accepted hits files, for example 'Tophat on\n    data 1: accepted_hits'.\n\n\nClick on Display with IGV \n'webcurrent'\n (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)\n\n\nOnce IGV opens, it will show you the accepted_hits BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)\n\n\nSelect \nchr4\n from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)\n\n\nView one of the splice function files such as 'TopHat on data 1: splice\n    junctions'. You will need to save this file to your local disk\n    using the \ndisk icon\n under the details of the file. Then open\n    the saved .bed file directly in IGV using the \nFile \n Load From File\n\n    option from IGV. This is because IGV doesn\u2019t automatically stream BED\n    files from Galaxy.\n    \n\n    The junctions file is loaded at the bottom of the\n    IGV window and splicing events are represented as coloured arcs. The\n    height and thickness of the arcs are proportional to the read depth.\n\n\n\n\nView differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (C1) and condition 2 (C2).\n\n\nSelect 'TopHat on data 4: accepted_hits' (this is the accepted hits\nalignment file from first replicate of condition C2) and click on\n'display with IGV local'. This time we are using the \n'local'\n link, as\nwe already have an IGV window up and running locally from the last\nstep. One the file has loaded, change the location to\n\nchr4:325197-341887\n using the field on the top toolbar.\n\n\nThe middle gene in this area clearly looks like it has many more reads\nmapped in condition 2 than condition 1, whereas for the surrounding\ngenes the reads look about the same. The middle gene looks like it is\ndifferentially expressed. But, of course, it may be that there are\nmany more reads in the readsets for C1 and C2, and the other genes\nare underexpressed in condition 2. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.\n\n\n\n\n\n\n4.  \n[Optional]\n Visualise the aligned reads in Trackster\n\n\nWe can also use the inbuilt Galaxy genome browser, Trackster, to visualise\nalignments. Trackster has fewer features than IGV, but sometimes it may be\nmore convenient to use as it only requires the browser.\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization \n New Track Browser\n.\n\n\nName your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select Tophat on\n    data 1: accepted_hits and Tophat on data 4: accepted_hits by using the\n    checkboxes on the left.\n\n\nSelect chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.\n\n\nNext to the drop down list, click on the chromosomal position number\n    display and specify the location \nchr4:325197-341887\n.  \n\n\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 3: Test differential expression with Cuffdiff [45 min]\n\n\nThe aim in this section is to:\n\n\n\n\ngenerate tables of normalised read counts per gene per condition based on the\n  annotated reference transcriptome,\n\n\nstatistically test for expression differences in normalised read counts for\n  each gene, taking into account the variance observed between samples,  \n\n\nfor each gene, calculate the p-value of the gene being differentially\n  expressed-- this is the probability of seeing the data or something more\n  extreme given the null hypothesis (that the gene is not differentially\n  expressed between the two conditions),\n\n\nfor each gene, estimate the fold change in expression between the two\n  conditions.\n\n\n\n\nAll these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff\nis part of the Cufflinks software suite which takes the aligned reads from\nTophat and generates normalised read counts and a list of differentially\nexpressed genes based on a reference transcriptome - in this case, the curated\nEnsembl list of D. melanogaster genes from chromosome 4 that we supply as a\nGTF (Gene Transfer Format) file.\nA more detailed explanation of Cufflinks DGE testing can be found\n\nhere\n.\n\n\n1.  Examine the reference transcriptome\n\n\nClick on the \neye icon\n to display the ensembl_dm3.chr4.gtf reference\ntranscriptome file in Galaxy.\n\n\nThe reference transcriptome is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found\n\nhere\n.\n\n\nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff\nuses the reference transcriptome to aggregate read counts per gene,\ntranscript, transcription start site and coding sequence (CDS). For this\ntutorial, we\u2019ll only consider differential gene testing, but it is also\npossible to test for differential expression of transcripts or\ntranscription start sites.\n\n\n2.  Run Cuffdiff to identify differentially expressed genes and transcripts\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis \n Cuffdiff\n and set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n\n\nTophat on data 1: accepted_hits\n\n\nTophat on data 2: accepted_hits\n\n\nTophat on data 3: accepted_hits\n\n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key for Windows or the command key for OSX.)\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n\n\nTophat on data 4: accepted_hits\n\n\nTophat on data 5: accepted_hits\n\n\nTophat on data 6: accepted_hits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n3.  Explore the Cuffdiff output files\n\n\nThere should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 37, data 32, and others\".\n\n\nFPKM tracking files:\n  \n\n\n\n\ntranscript FPKM tracking\n\n\ngene FPKM tracking\n\n\nTSS groups FPKM tracking\n\n\nCDS FPKM tracking  \n\n\n\n\nThese 4 files contain the FPKM (a unit of normalised expression taking\ninto account the transcript length for each transcript\nand the library size of the sample) for each of the two conditions.\n\n\nDifferential expression testing files:\n  \n\n\n\n\ngene differential expression testing\n\n\ntranscript differential expression testing\n\n\nTSS groups differential expression testing\n\n\nCDS FPKM differential expression testing\n\n\nCDS overloading diffential expression testing\n\n\npromoters differential expression testing\n\n\nsplicing differential expression testing\n\n\n\n\nThese 7 files contain the statistical\nresults from testing the level of expression between condition C1 and condition C2.\n\n\n\n\n\n\nExamine the tables of normalised gene counts\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM\n    tracking\" by clicking on the \neye icon\n. The file consists of one row\n    for each gene from the reference transcriptome, with columns containing the\n    normalised read counts for each of the two conditions.\n    Note:  \n\n\n\n\nCuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates\n  to a gene. Multiple transcription start sites are aggregated under a\n  single tracking_id.\n\n\nA gene encompasses a chromosomal locus which covers all the features\n  that make up that gene (exons, introns, 5\u2019 UTR, etc).\n\n\n\n\n\n\n\n\nInspect the gene differential expression testing file\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene\n    differential expression testing\" by clicking on the \neye icon\n. The\n    columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10),\n    p_value (c12), q_value (c13) and significant (c14).\n\n\n\n\n\n\nFilter based on column 14 (\u2018significant\u2019) - a binary assessment of\n    q_value \n 0.05, where q_value is p_value adjusted for multiple testing.\n    Under Basic Tools, click on \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nThis will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed.\n\nWe can rename this file (screenshot) by clicking on the \npencil icon\n of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Significant_DE_Genes\".\n\n\n\n\n\n\nExamine the sorted list of differentially expressed genes.\n    Click on the \neye icon\n next to \"Significant_DE_Genes\" to view the data.\n\n\n\n\n\n\n\n    \nHow many genes are in the Significant_DE_Genes file? What are their names?\n\n    \n\n    Two genes have been identified as differentially expressed between\n    conditions C1 and C2:\n    \n\n        \nAnk\n located at chr4:137014-150378, and\n        \nCG2177\n located at chr4:331557-334534\n    \n\n    Both genes have q-values of 0.00175.\n\n\n\n\n\"CG2177\" located at chr4:331557-334534 was the gene that we intuitively (with IGV)\nsaw to be differentially expressed in the previous section, in the broader\nregion of chr4:325197-341887.\n\n\n\n\nSection 4. Repeat without replicates [20 min]\n\n\nIn this section, we will run Cuffdiff with fewer replicates.\n\n\n\n    \nStop and think:\n\n    Why do we need replicates for an RNA-seq differential gene expression\n    experiment? What do you expect to happen if we only use one sample from\n    each condition for our analysis?\n\n\n\n\n\n\n\n\nRepeat the differential gene expression testing from section 2, but this\n    time only use one replicate from each condition group (C1 and C2).\n\n    From the Galaxy tool panel, select \nNGS: RNA Analysis \n Cuffdiff\n and\n    set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n Tophat on data 1: accepted_hits\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n Tophat on data 4: accepted_hits\n\n\n\n\n\n\n\n\n\n\nLibrary normalization method:\n classic-fpkm\n\n\nDispersion estimation method:\n blind\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_C1_R1_vs_C2_R1\"\n\n\n\n\n\n\nClick on the \neye icon\n of Significant_DE_Genes_C1_R1_vs_C2_R1.\n\n    You should get \nno\n differentially expressed genes at statistical\n    significance of 0.05. The \"Ank\" gene and the \"CG1277\", which were found to\n    be significantly differentially expressed in our first analysis, are not\n    identified as differentially expressed when we only use one sample for each\n    condition.\n\n\n\n\n\n\nRepeat this no-replicates analysis, but this time specify a different set\n    of samples.\n    From the Galaxy tool panel, select \nNGS: RNA Analysis \n Cuffdiff\n and\n    set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n Tophat on data 1: accepted_hits\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n Tophat on data 5: accepted_hits\n\n\n\n\n\n\n\n\n\n\nLibrary normalization method:\n classic-fpkm\n\n\nDispersion estimation method:\n blind\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_C1_R1_vs_C2_R2\"\n\n\n\n\n\n\nClick on the \neye icon\n of Significant_DE_Genes_C1_R1_vs_C2_R2.\n\n    We now see \"CG2177\" appear again in the list as significantly\n    differentially expressed, but not \"Ank\".\n\n\n\n\n\n\n\n    \nHow can we interpret the difference in results from using different\n    replicates? \n\n    \n\n    There is a larger absolute difference in CG1277 expression between\n    samples 1 (C1_R1) and 5 (C2_R2) than samples 1 (C1_R1) and 4 (C2_R1), hence\n    Cuffdiff identifies CG1277 as differentially expressed between C1_R1 and\n    C2_R2, but not between C1_R1 and C2_R1.\n    \n\n    On the other hand, differences in level of expression of Ank is much smaller\n    between samples, so we need to see it consistently across multiple replicates\n    for Cuffdiff to be confident it actually exists. One replicate is not enough.\n\n\n\n\nThe identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.\n\n\nIf we say that genes Ank and CG2177 are \ntruly\n differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.\n\n\nIt is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.\n\n\n[Optional step]\n\nRepeat this analysis, specifying groups of two replicates each. What do you\nget? How many replicates do we need to identify Ank as differentially\nexpressed?\n\n\n\n\nSection 5. Optional Extension [20 min]\n\n\nExtension on the Tuxedo Protocol\n\n\nThe full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge,\nand CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by\nidentifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow\nyou to identify new genes and transcripts, and then analyse them for\ndifferential expression. This is critical for organisms in which the\ntranscriptome is not well characterised. CummeRbund helps visualise the data\nproduced from the Cuffdiff using the R statistical programming language.\n\n\nRead more on the full Tuxedo protocol \nhere\n.\n\n\nIf the organism we were working on did not have a well characterized reference\ntranscriptome, we would run Cufflinks and Cuffmerge to create a transcriptome.\n\n\n\n\n\n\nSuppose we didn't have our Drosophila GTF file containing the location of\n    known genes. We can use Cufflinks to assemble transcripts from the\n    alignment data to create GTF files.\n    From the Galaxy tool panel, select \nNGS: RNA Analysis \n Cufflinks\n and\n    set the parameters as follows:  \n\n\n\n\nSAM or BAM file of aligned RNA-Seq reads:\n\n  Click on the multiple datasets icon and select all 6 BAM files from\n  Tophat\n\n\nMax Intron Length:\n 50000  \n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nNext, we want to merge the assemblies outputted by Cufflinks by selecting\n    \nNGS: RNA Analysis \n Cuffmerge\n and setting the parameters as follows:  \n\n\n\n\nGTF file(s) produced by Cufflinks:\n Select the 6 GTF files ending\n  with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or\n  command key to select multiple files.\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nNote: In cases where you have a reference GTF, but also want to identify\nnovel transcripts with Cufflinks, you would add the reference GTF to the\ncuffmerge inputs with the \nAdditional GTF Inputs (Lists)\n parameter.\n\n\n\n\n\n\nView the Cuffmerge GTF file by clicking the \neye icon\n\n\n\n\n\n\nRun Cuffdiff using the new GTF file\n    In the Galaxy tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis \n Cuffdiff\n and set the parameters as follows:  \n\n\n\n\nTranscripts:\n Cuffmerge on data x, data x, and others: merged\n  transcripts\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n\n\nTophat on data 1: accepted_hits\n\n\nTophat on data 2: accepted_hits\n\n\nTophat on data 3: accepted_hits\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n\n\nTophat on data 4: accepted_hits\n\n\nTophat on data 5: accepted_hits\n\n\nTophat on data 6: accepted_hits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_using_Cufflinks_Assembly\"\n\n\n\n\n\n\nViewing the significant genes, we see that there are two genes that are\nidentified as differentially expressed by Cuffdiff using the GTF file produced\nfrom Cufflinks and Cuffmerge. The locations of these two genes correspond to\nthe previous result from section 3 (genes Ank and CG2177).\n\n\nTranscript-level differential expression\n\n\nOne can think of a scenario in an experiment aiming to investigate the\ndifferences between two experimental conditions, where a gene had the same\nnumber of read counts in the two conditions but these read counts were derived\nfrom different transcripts; this gene would not be identified in a differential\ngene expression test, but would be in a differential transcript expression test.\nThe choice of what \"unit of aggregation\" to use in differential expression\ntesting is one that should be made by the biological investigator, and will\naffect the bioinformatics analysis done (and probably the data generation too).\n\n\nTake a look at the different differential expression files produced by\nCuffdiff from section 3 which use different units of aggregation.\n\n\nReferences\n\n\nTrapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. \nNature Protocols\n [serial online]. March 1, 2012;7(3):562-578.", 
            "title": "Tuxedo Protocol Tutorial"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#rna-seq-differential-gene-expression-basic-tutorial", 
            "text": "Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung", 
            "title": "RNA-Seq Differential Gene Expression: Basic Tutorial"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#tutorial-overview", 
            "text": "In this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a small synthetic dataset from the model organism,\nDrosophila melanogaster.  The tutorial is designed to introduce the tools, datatypes and workflow of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\nwould contain sequencing and alignment errors that make analysis more difficult.  Our input data for this tutorial will be raw RNA-seq reads from two\nexperimental conditions and we will output a list of differentially\nexpressed genes identified to be statistically significant.  In this tutorial we will:     introduce the types of files typically used in RNA-seq analysis  align RNA-seq reads with Tophat  visualise RNA-seq alignment data with IGV  find differentially expressed genes with Cuffdiff  understand the importance of replicates for differential expression analysis   This tutorial does not cover the following steps that you would do in a real\nRNA-seq DGE analysis:     QC (quality control) of the raw sequence data  Trimming the reads for quality and for adaptor sequences  QC of the RNA-seq alignment data     These steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#learning-objectives", 
            "text": "At the end of this tutorial you should:   Be familiar with the Tuxedo Protocol workflow for RNA-seq differential\n   expression analysis  Be able to process raw RNA sequence data into a list of differentially\n   expressed genes  Be aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#background", 
            "text": "", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#where-does-the-data-in-this-tutorial-come-from", 
            "text": "The data for this tutorial is from an RNA-seq experiment looking for\ndifferentially expressed genes in D. melanogaster (fruit fly) between two\nexperimental conditions. The experiment and analysis protocol we will follow\nis derived from a paper in Nature Protocols by the research group responsible\nfor one of the most widely used set of RNA-seq analysis tools:  \"Differential\ngene and transcript expression analysis of RNA-seq experiments with TopHat and\nCufflinks\"  (Trapnell et al 2012).  The sequence datasets are single-end Illumina synthetic short reads,\nfiltered to only include chromosome 4 to facilitate faster mapping (which\nwould otherwise take hours). We\u2019ll use data from three biological replicates\nfrom each of the two experimental conditions.", 
            "title": "Where does the data in this tutorial come from?"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#the-tuxedo-protocol", 
            "text": "The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first\nmapped with TopHat and a transcriptome is then assembled using Cufflinks.\nCuffdiff then quantifies the expression in each condition, and tests for\ndifferential expression.  In this tutorial we use a simpler protocol as the D. melanogaster\ntranscriptome is already very well characterised.  More information about the Tuxedo protocol can be found  here .", 
            "title": "The Tuxedo Protocol"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-1-preparation-15-min", 
            "text": "", 
            "title": "Section 1: Preparation [15 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account-what-is-galaxy", 
            "text": "Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User   Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User   Login .", 
            "title": "1.  Register as a new user in Galaxy if you don\u2019t already have an account (what is Galaxy?)"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-import-the-rna-seq-data-for-the-workshop", 
            "text": "If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data   Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data   Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.  Upload the sequence data by pasting the following links into the text\n    input area:\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq\n     \n     \n    Select the type as 'fastqsanger' and press  start  to upload the\n    files to Galaxy.  Upload the annotated gene list reference by pasting the following link\n    into the text input area:\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf\n     \n    You don't need to specify the type for this file as Galaxy will\n    auto-detect the file as a GTF file.", 
            "title": "2.  Import the RNA-seq data for the workshop."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-view-and-have-an-understanding-of-the-files-involved-in-rna-seq-analysis", 
            "text": "You should now have the following files in your Galaxy history:  6 files containing single-ended reads:  C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq   And 1 gene annotation file:  ensembl_dm3.chr4.gtf   These files can be renamed by clicking the  pen icon  if you wish.    These 6 sequencing files are in FASTQ format and have the file\n    extension: .fq. If you are not familiar with the FASTQ format,  click\n    here for an overview .    Click on the  eye icon  to the top right of each FASTQ file to view the\nfirst part of the file.\nThe first 3 files are from the first condition (C1) and has 3\nreplicates labelled R1, R2, and R3. The next 3 FASTQ files are from\nthe second condition (C2) and has 3 replicates labelled R1, R2, and R3.  In this tutorial, we aim to find genes which are differentially\nexpressed between condition C1 and condition C2.    The gene annotation file is in GTF format. This file describes where\n    the genes are located in the Drosophila reference genome.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.    NOTE:  Since the reads in this dataset are synthetic, they do not have\nreal quality scores.  NOTE:  If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019", 
            "title": "3.  View and have an understanding of the files involved in RNA-seq analysis."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-2-align-reads-with-tophat-30-mins", 
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found  here .", 
            "title": "Section 2: Align reads with Tophat [30 mins]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-align-the-rna-seq-short-reads-to-a-reference-genome", 
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis   Tophat  and set the parameters as follows:     Is this single-end or paired-end data?  Single-end    RNA-Seq FASTQ file: \n  (Click on the multiple datasets icon and select all six of the FASTQ\n  files. This can be done by holding down the shift key to select a range\n  of files, or holding down the ctrl key (Windows) or command key (OSX) and\n  clicking to select multiple files.)  C1_R1.chr4.fq  C1_R2.chr4.fq  C1_R3.chr4.fq  C2_R1.chr4.fq  C2_R2.chr4.fq  C2_R3.chr4.fq      Use a built in reference genome or own from your history:  Use\n  built-in genome  Select a reference genome:  D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)  Use defaults for the other fields  Execute     Show screenshot       (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)  \n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n      Note: This may take a few minutes, depending on how busy the server is.", 
            "title": "1.  Align the RNA-seq short reads to a reference genome."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-examine-the-output-files", 
            "text": "You should have 5 output files for each of the FASTQ input files:   Tophat on data 1: accepted_hits:  This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.  Tophat on data 1: splice junctions:  This file lists all the places\n  where Tophat had to split a read into two pieces to span an exon\n  junction.  Tophat on data 1: deletions  and  Tophat on data 1: insertions: \n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.  Tophat on data 1: align_summary:  This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.   You should have a total of 30 Tophat output files in your history.", 
            "title": "2.  Examine the output files"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-visualise-the-aligned-reads-with-igv", 
            "text": "The purpose of this step is to :   visualise the quantitative, exon-based nature of RNA-seq data  visualise the expression differences between samples represented by the\n  quantity of reads, and  become familiar with the  Integrative Genomics Viewer\n  (IGV) -- an interactive\n  visualisation tool by the Broad Institute.     To visualise the alignment data:   Click on one of the Tophat accepted hits files, for example 'Tophat on\n    data 1: accepted_hits'.  Click on Display with IGV  'webcurrent'  (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)  Once IGV opens, it will show you the accepted_hits BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)  Select  chr4  from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)  View one of the splice function files such as 'TopHat on data 1: splice\n    junctions'. You will need to save this file to your local disk\n    using the  disk icon  under the details of the file. Then open\n    the saved .bed file directly in IGV using the  File   Load From File \n    option from IGV. This is because IGV doesn\u2019t automatically stream BED\n    files from Galaxy.\n     \n    The junctions file is loaded at the bottom of the\n    IGV window and splicing events are represented as coloured arcs. The\n    height and thickness of the arcs are proportional to the read depth.   View differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (C1) and condition 2 (C2).  Select 'TopHat on data 4: accepted_hits' (this is the accepted hits\nalignment file from first replicate of condition C2) and click on\n'display with IGV local'. This time we are using the  'local'  link, as\nwe already have an IGV window up and running locally from the last\nstep. One the file has loaded, change the location to chr4:325197-341887  using the field on the top toolbar.  The middle gene in this area clearly looks like it has many more reads\nmapped in condition 2 than condition 1, whereas for the surrounding\ngenes the reads look about the same. The middle gene looks like it is\ndifferentially expressed. But, of course, it may be that there are\nmany more reads in the readsets for C1 and C2, and the other genes\nare underexpressed in condition 2. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.", 
            "title": "3.  Visualise the aligned reads with IGV"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#4-optional-visualise-the-aligned-reads-in-trackster", 
            "text": "We can also use the inbuilt Galaxy genome browser, Trackster, to visualise\nalignments. Trackster has fewer features than IGV, but sometimes it may be\nmore convenient to use as it only requires the browser.   On the top bar of Galaxy, select  Visualization   New Track Browser .  Name your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select Tophat on\n    data 1: accepted_hits and Tophat on data 4: accepted_hits by using the\n    checkboxes on the left.  Select chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.  Next to the drop down list, click on the chromosomal position number\n    display and specify the location  chr4:325197-341887 .     Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.", 
            "title": "4.  [Optional] Visualise the aligned reads in Trackster"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-3-test-differential-expression-with-cuffdiff-45-min", 
            "text": "The aim in this section is to:   generate tables of normalised read counts per gene per condition based on the\n  annotated reference transcriptome,  statistically test for expression differences in normalised read counts for\n  each gene, taking into account the variance observed between samples,    for each gene, calculate the p-value of the gene being differentially\n  expressed-- this is the probability of seeing the data or something more\n  extreme given the null hypothesis (that the gene is not differentially\n  expressed between the two conditions),  for each gene, estimate the fold change in expression between the two\n  conditions.   All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff\nis part of the Cufflinks software suite which takes the aligned reads from\nTophat and generates normalised read counts and a list of differentially\nexpressed genes based on a reference transcriptome - in this case, the curated\nEnsembl list of D. melanogaster genes from chromosome 4 that we supply as a\nGTF (Gene Transfer Format) file.\nA more detailed explanation of Cufflinks DGE testing can be found here .", 
            "title": "Section 3: Test differential expression with Cuffdiff [45 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-examine-the-reference-transcriptome", 
            "text": "Click on the  eye icon  to display the ensembl_dm3.chr4.gtf reference\ntranscriptome file in Galaxy.  The reference transcriptome is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found here . \nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff\nuses the reference transcriptome to aggregate read counts per gene,\ntranscript, transcription start site and coding sequence (CDS). For this\ntutorial, we\u2019ll only consider differential gene testing, but it is also\npossible to test for differential expression of transcripts or\ntranscription start sites.", 
            "title": "1.  Examine the reference transcriptome"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts", 
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis   Cuffdiff  and set the parameters as follows:     Transcripts:  ensembl_dm3.chr4.gtf  Condition:     1: Condition  name:  C1  Replicates:  Tophat on data 1: accepted_hits  Tophat on data 2: accepted_hits  Tophat on data 3: accepted_hits \n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key for Windows or the command key for OSX.)      2: Condition  name:  C2  Replicates:  Tophat on data 4: accepted_hits  Tophat on data 5: accepted_hits  Tophat on data 6: accepted_hits        Use defaults for the other fields  Execute     Show screenshot       \n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "2.  Run Cuffdiff to identify differentially expressed genes and transcripts"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-explore-the-cuffdiff-output-files", 
            "text": "There should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 37, data 32, and others\".  FPKM tracking files:      transcript FPKM tracking  gene FPKM tracking  TSS groups FPKM tracking  CDS FPKM tracking     These 4 files contain the FPKM (a unit of normalised expression taking\ninto account the transcript length for each transcript\nand the library size of the sample) for each of the two conditions.  Differential expression testing files:      gene differential expression testing  transcript differential expression testing  TSS groups differential expression testing  CDS FPKM differential expression testing  CDS overloading diffential expression testing  promoters differential expression testing  splicing differential expression testing   These 7 files contain the statistical\nresults from testing the level of expression between condition C1 and condition C2.    Examine the tables of normalised gene counts\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM\n    tracking\" by clicking on the  eye icon . The file consists of one row\n    for each gene from the reference transcriptome, with columns containing the\n    normalised read counts for each of the two conditions.\n    Note:     Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates\n  to a gene. Multiple transcription start sites are aggregated under a\n  single tracking_id.  A gene encompasses a chromosomal locus which covers all the features\n  that make up that gene (exons, introns, 5\u2019 UTR, etc).     Inspect the gene differential expression testing file\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene\n    differential expression testing\" by clicking on the  eye icon . The\n    columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10),\n    p_value (c12), q_value (c13) and significant (c14).    Filter based on column 14 (\u2018significant\u2019) - a binary assessment of\n    q_value   0.05, where q_value is p_value adjusted for multiple testing.\n    Under Basic Tools, click on  Filter and Sort   Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   This will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. \nWe can rename this file (screenshot) by clicking on the  pencil icon  of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Significant_DE_Genes\".    Examine the sorted list of differentially expressed genes.\n    Click on the  eye icon  next to \"Significant_DE_Genes\" to view the data.", 
            "title": "3.  Explore the Cuffdiff output files"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-4-repeat-without-replicates-20-min", 
            "text": "In this section, we will run Cuffdiff with fewer replicates.", 
            "title": "Section 4. Repeat without replicates [20 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-5-optional-extension-20-min", 
            "text": "", 
            "title": "Section 5. Optional Extension [20 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#extension-on-the-tuxedo-protocol", 
            "text": "The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge,\nand CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by\nidentifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow\nyou to identify new genes and transcripts, and then analyse them for\ndifferential expression. This is critical for organisms in which the\ntranscriptome is not well characterised. CummeRbund helps visualise the data\nproduced from the Cuffdiff using the R statistical programming language.  Read more on the full Tuxedo protocol  here .  If the organism we were working on did not have a well characterized reference\ntranscriptome, we would run Cufflinks and Cuffmerge to create a transcriptome.    Suppose we didn't have our Drosophila GTF file containing the location of\n    known genes. We can use Cufflinks to assemble transcripts from the\n    alignment data to create GTF files.\n    From the Galaxy tool panel, select  NGS: RNA Analysis   Cufflinks  and\n    set the parameters as follows:     SAM or BAM file of aligned RNA-Seq reads: \n  Click on the multiple datasets icon and select all 6 BAM files from\n  Tophat  Max Intron Length:  50000    Use defaults for the other fields  Execute     Next, we want to merge the assemblies outputted by Cufflinks by selecting\n     NGS: RNA Analysis   Cuffmerge  and setting the parameters as follows:     GTF file(s) produced by Cufflinks:  Select the 6 GTF files ending\n  with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or\n  command key to select multiple files.  Use defaults for the other fields  Execute   Note: In cases where you have a reference GTF, but also want to identify\nnovel transcripts with Cufflinks, you would add the reference GTF to the\ncuffmerge inputs with the  Additional GTF Inputs (Lists)  parameter.    View the Cuffmerge GTF file by clicking the  eye icon    Run Cuffdiff using the new GTF file\n    In the Galaxy tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis   Cuffdiff  and set the parameters as follows:     Transcripts:  Cuffmerge on data x, data x, and others: merged\n  transcripts  Condition:     1: Condition  name:  C1  Replicates:  Tophat on data 1: accepted_hits  Tophat on data 2: accepted_hits  Tophat on data 3: accepted_hits      2: Condition  name:  C2  Replicates:  Tophat on data 4: accepted_hits  Tophat on data 5: accepted_hits  Tophat on data 6: accepted_hits        Use defaults for the other fields  Execute     Filter the recently generated gene set for significantly differentially\n    expressed genes by going to  Filter and Sort   Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   Rename the output file to something meaningful like\n\"Significant_DE_Genes_using_Cufflinks_Assembly\"    Viewing the significant genes, we see that there are two genes that are\nidentified as differentially expressed by Cuffdiff using the GTF file produced\nfrom Cufflinks and Cuffmerge. The locations of these two genes correspond to\nthe previous result from section 3 (genes Ank and CG2177).", 
            "title": "Extension on the Tuxedo Protocol"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#transcript-level-differential-expression", 
            "text": "One can think of a scenario in an experiment aiming to investigate the\ndifferences between two experimental conditions, where a gene had the same\nnumber of read counts in the two conditions but these read counts were derived\nfrom different transcripts; this gene would not be identified in a differential\ngene expression test, but would be in a differential transcript expression test.\nThe choice of what \"unit of aggregation\" to use in differential expression\ntesting is one that should be made by the biological investigator, and will\naffect the bioinformatics analysis done (and probably the data generation too).  Take a look at the different differential expression files produced by\nCuffdiff from section 3 which use different units of aggregation.", 
            "title": "Transcript-level differential expression"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#references", 
            "text": "Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks.  Nature Protocols  [serial online]. March 1, 2012;7(3):562-578.", 
            "title": "References"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/", 
            "text": "Background\n\n\nIntroduction to RNA-seq\n\n\nRNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample.\n\n\nDifferential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples.\n\n\nIn eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites\n\n\nThe Galaxy workflow platform\n\n\nWhat is Galaxy?\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nFigure 1: The Galaxy interface\n\n\nTools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\nDifferential gene expression analysis using Tophat and Cufflinks\n\n\nTwo protocols are described in the paper inspiring this tutorial (Trapnell et al 2012):\n\n\n\n\n\n\nThe \nTuxedo protocol\n: a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes\n\n\n\n\n\n\nThe \nAlternate protocol\n: a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed\n\n\n\n\n\n\nAssembling a transcriptome is advised if no well characterised transcriptome exists, but as \nD. melanogaster\n is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on \nD. melanogaster\n, so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler.\n\n\nIf we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.\n\n\nThe Alternate protocol\n\n\nThe overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.\n\n\nFigure 2: General workflow for testing expression differences between two experimental conditions\n\n\n\n\nTophat\n\n\nReads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.\n\n\n\n\nTopHat input: Fasta or Fastq files\n\n\nTopHat output: BAM file (Compressed SAM file)\n\n\n\n\nCuffdiff\n\n\nThe reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.\n\n\n\n\nCuffdiff input: Reference transcriptome as GTF file\n\n\nCuffdiff output:\n\n\nGene and transcript expression levels as tables of normalised read counts\n\n\nDifferential analysis testing on:\n\n\nGenes\n\n\nTranscripts\n\n\nTranscription Start Site (TSS) groups\n\n\nSplicing: files reporting on splicing\n\n\nPromoter: differentially spliced genes via promoter switching\n\n\nCDS: CoDing Sequences\n\n\n\n\n\n\n\n\n\n\n\n\nThe full Tuxedo Protocol\n\n\nFigure 3: Full Tuxedo protocol workflow\n\n\n\n\n\n\n\n\nTophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.\n\n\n\n\nTopHat input: Fasta or Fastq files\n\n\nTopHat output: BAM file (Compressed SAM file)\n\n\n\n\n\n\n\n\nCufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones\n\n\n\n\nCufflinks input:\n\n\nMapped reads (SAM or BAM), use accepted_hits.bam from Tophat\n\n\nGenome annotation: GTF file\n\n\n\n\n\n\nCufflinks output:\n\n\nassembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular)\n\n\ntranscript_expression (tabular): table of expression levels for each transcript\n\n\ngene_expression (tabular): table of total expression levels for each gene.\n\n\n\n\n\n\n\n\n\n\n\n\nCuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy.\n\n\n\n\nCuffcompare input:\n\n\nAssembled_transcripts for each sample\n\n\nReference Annotation\n\n\n\n\n\n\nCuffcompare output: combined_transcripts.gtf\n\n\n\n\n\n\n\n\nCuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.\n\n\n\n\nCuffdiff input:\n\n\nReference transcriptome as GTF file\n\n\nBAM files of mapped reads from Tophat for all samples\n\n\n\n\n\n\nCuffdiff output:\n\n\nGene and transcript expression levels as tables of normalised read counts\n\n\nDifferential analysis testing on:\n\n\nGenes\n\n\nTranscripts\n\n\nTranscription Start Site (TSS) groups\n\n\nSplicing: files reporting on splicing\n\n\nPromoter: differentially spliced genes via promoter switching\n\n\nCDS: CoDing Sequences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.\n\n\n\n\n\n\nProtocols recommendations\n\n\n\n\nCreate a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended.\n\n\nDo paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads.\n\n\nSequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads.\n\n\nIdentify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.\n\n\n\n\nLimitations of the protocols\n\n\n\n\nBoth Tophat and Cufflinks require a reference genome.\n\n\nThe protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.\n\n\n\n\nReferences\n\n\n\n\nTrapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. \nNature Protocols\n [serial online]. March 1, 2012;7(3):562-578.\n\n\nJames T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov.\n \nIntegrative Genomics Viewer\n. Nature Biotechnology 29, 24\u201326 (2011)\n,\n\n\n\n\n[^1]: \nThe published protocol has been designed for running on the\n    command line in Linux. This tutorial has been adapted to use on the\n    web-based Galaxy platform.\n\n\n[^2]: \nNote: Cuffmerge is a meta-assembler. It treats the assemblies\n    created by Cufflinks the same way Cufflinks treats reads from\n    Tophat. That is, it produces a parsimonious transcript assembly of\n    the assemblies. The difference between Cuffmerge and Cuffcompare is\n    that if we have two transfrags A and B, Cuffcompare only combines\n    the two where either A or B is \u2018contained\u2019 in the other transfrag or\n    in other words one of them is redundant; whereas Cuffmerge assembles\n    them if they overlap with each other and agree on splicing.\n\n\n[^3]: \nSince CummRbund is currently not installed on Galaxy, the\n    underlying steps are not included in this tutorial; instead we use\n    IGV (Robinson et al 2011) and Trackster for visualizing the output\n    which are accessible from Galaxy.", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#background", 
            "text": "", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#introduction-to-rna-seq", 
            "text": "RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample.  Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples.  In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites", 
            "title": "Introduction to RNA-seq"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-galaxy-workflow-platform", 
            "text": "", 
            "title": "The Galaxy workflow platform"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#what-is-galaxy", 
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here", 
            "title": "What is Galaxy?"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-1-the-galaxy-interface", 
            "text": "Tools on the left, data in the middle, analysis workflow on the right.", 
            "title": "Figure 1: The Galaxy interface"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#differential-gene-expression-analysis-using-tophat-and-cufflinks", 
            "text": "Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012):    The  Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes    The  Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed    Assembling a transcriptome is advised if no well characterised transcriptome exists, but as  D. melanogaster  is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on  D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler.  If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.", 
            "title": "Differential gene expression analysis using Tophat and Cufflinks"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-alternate-protocol", 
            "text": "The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.", 
            "title": "The Alternate protocol"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-2-general-workflow-for-testing-expression-differences-between-two-experimental-conditions", 
            "text": "", 
            "title": "Figure 2: General workflow for testing expression differences between two experimental conditions"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#tophat", 
            "text": "Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.   TopHat input: Fasta or Fastq files  TopHat output: BAM file (Compressed SAM file)", 
            "title": "Tophat"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#cuffdiff", 
            "text": "The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.   Cuffdiff input: Reference transcriptome as GTF file  Cuffdiff output:  Gene and transcript expression levels as tables of normalised read counts  Differential analysis testing on:  Genes  Transcripts  Transcription Start Site (TSS) groups  Splicing: files reporting on splicing  Promoter: differentially spliced genes via promoter switching  CDS: CoDing Sequences", 
            "title": "Cuffdiff"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-full-tuxedo-protocol", 
            "text": "", 
            "title": "The full Tuxedo Protocol"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-3-full-tuxedo-protocol-workflow", 
            "text": "Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.   TopHat input: Fasta or Fastq files  TopHat output: BAM file (Compressed SAM file)     Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones   Cufflinks input:  Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat  Genome annotation: GTF file    Cufflinks output:  assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular)  transcript_expression (tabular): table of expression levels for each transcript  gene_expression (tabular): table of total expression levels for each gene.       Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy.   Cuffcompare input:  Assembled_transcripts for each sample  Reference Annotation    Cuffcompare output: combined_transcripts.gtf     Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.   Cuffdiff input:  Reference transcriptome as GTF file  BAM files of mapped reads from Tophat for all samples    Cuffdiff output:  Gene and transcript expression levels as tables of normalised read counts  Differential analysis testing on:  Genes  Transcripts  Transcription Start Site (TSS) groups  Splicing: files reporting on splicing  Promoter: differentially spliced genes via promoter switching  CDS: CoDing Sequences         CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.", 
            "title": "Figure 3: Full Tuxedo protocol workflow"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#protocols-recommendations", 
            "text": "Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended.  Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads.  Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads.  Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.", 
            "title": "Protocols recommendations"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#limitations-of-the-protocols", 
            "text": "Both Tophat and Cufflinks require a reference genome.  The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.", 
            "title": "Limitations of the protocols"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#references", 
            "text": "Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks.  Nature Protocols  [serial online]. March 1, 2012;7(3):562-578.  James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov.   Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) ,   [^1]:  The published protocol has been designed for running on the\n    command line in Linux. This tutorial has been adapted to use on the\n    web-based Galaxy platform.  [^2]:  Note: Cuffmerge is a meta-assembler. It treats the assemblies\n    created by Cufflinks the same way Cufflinks treats reads from\n    Tophat. That is, it produces a parsimonious transcript assembly of\n    the assemblies. The difference between Cuffmerge and Cuffcompare is\n    that if we have two transfrags A and B, Cuffcompare only combines\n    the two where either A or B is \u2018contained\u2019 in the other transfrag or\n    in other words one of them is redundant; whereas Cuffmerge assembles\n    them if they overlap with each other and agree on splicing.  [^3]:  Since CummRbund is currently not installed on Galaxy, the\n    underlying steps are not included in this tutorial; instead we use\n    IGV (Robinson et al 2011) and Trackster for visualizing the output\n    which are accessible from Galaxy.", 
            "title": "References"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/", 
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq Differential Gene Expression: Advanced Tutorial\n\n\nAuthors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we compare the performance of three statistically-based\nexpression analysis tools:  \n\n\n\n\nCuffDiff\n\n\nEdgeR\n\n\nDESeq2\n\n\n\n\nThis tutorial builds on top of the \nbasic RNA-seq DGE tutorial\n.\nIt is recommended to have some familiarity of RNA-seq before beginning this\ntutorial.\n\n\n\n\nBackground [15 min]\n\n\nWhere does the data in this tutorial come from?\n\n\nThe data for this tutorial is from the paper, \nA comprehensive comparison of\nRNA-Seq-based transcriptome analysis from reads to differential gene expression\nand cross-comparison with microarrays: a case study in Saccharomyces\ncerevisiae\n by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK\n113-7D (yeast) under two different metabolic conditions: glucose-excess (batch)\nor glucose-limited (chemostat).\n\n\nThe RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with\naccession SRS307298. There are 6 samples in total-- two treatments with\nthree biological replicates each. The data is paired-end.  \n\n\nWe have extracted chromosome I reads from the samples to make the\ntutorial a suitable length. This has implications, as discussed in section 8.\n\n\n\n\nSection 1: Preparation [15 min]\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser \n Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser \n Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data \n Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Adv_Sec_1\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data \n Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\nUpload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the batch condition\n    (glucose-excess). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n    \n\n    \n\n    These six files are three paired-end samples from the chem condition\n    (glucose-limited). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n    \n\n    \n\n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf\n    \n\n\n\n\nYou should now have these 13 files in your history:\n\n\n\n\nbatch1_chrI_1.fastq\n\n\nbatch1_chrI_2.fastq\n\n\nbatch2_chrI_1.fastq\n\n\nbatch2_chrI_2.fastq\n\n\nbatch3_chrI_1.fastq\n\n\nbatch3_chrI_2.fastq\n\n\nchem1_chrI_1.fastq\n\n\nchem1_chrI_2.fastq\n\n\nchem2_chrI_1.fastq\n\n\nchem2_chrI_2.fastq\n\n\nchem3_chrI_1.fastq\n\n\nchem3_chrI_2.fastq\n\n\ngenes.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\nNote:\n The reads are paired end; for example batch1_chrI_1.fastq and\nbatch1_chrI_2.fastq are paired reads from one sequencing run. Low quality\nreads have already been trimmed.\n\n\n\n\n\n\n\n\nSection 2: Alignment [30 mins]\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found \nhere\n.\n\n\n1.  Map/align the reads with Tophat to the S. cerevisiae reference\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis \n Tophat\n and set the parameters as follows:  \n\n\n\n\nIs this single-end or paired-end data?\n Paired-end (as individual datasets)  \n\n\n\n\nRNA-Seq FASTQ file, forward reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)\n\n\n\n\nbatch1_chrI_1.fastq\n\n\nbatch2_chrI_1.fastq\n\n\nbatch3_chrI_1.fastq\n\n\nchem1_chrI_1.fastq\n\n\nchem2_chrI_1.fastq\n\n\nchem3_chrI_1.fastq\n\n\n\n\n\n\n\n\nRNA-Seq FASTQ file, reverse reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the reverse\nFASTQ files ending in *2.fastq.)  \n\n\n\n\nbatch1_chrI_2.fastq\n\n\nbatch2_chrI_2.fastq\n\n\nbatch3_chrI_2.fastq\n\n\nchem1_chrI_2.fastq\n\n\nchem2_chrI_2.fastq\n\n\nchem3_chrI_2.fastq\n\n\n\n\n\n\nUse a built in reference genome or own from your history:\n Use\nbuilt-in genome\n\n\nSelect a reference genome:\n S. cerevisiae June 2008 (SGD/SacCer2)\n(sacCer2)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Rename the output files\n\n\nYou should have 5 output files for each of the FASTQ input files:\n\n\n\n\nTophat on data 2 and data 1: accepted_hits:\n This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.\n\n\nTophat on data 2 and data 1: splice junctions:\n This file lists all the places\n  where TopHat had to split a read into two pieces to span an exon\n  junction.\n\n\nTophat on data 2 and data 1 deletions\n and \nTophat on data 2 and data 1: insertions:\n\n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.\n\n\nTophat on data 2 and data 1: align_summary:\n This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.\n\n\n\n\nYou should have a total of 30 Tophat output files in your history.\n\n\nRename the 6 accepted_hits files into a more meaningful name (e.g.\n'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam')\nby using the \npen icon\n next to the file.\n\n\n3.  Visualise the aligned reads with Trackster\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization \n New Track Browser\n.\n\n\nName your new visualization and select S. cerevisiae (sacCer2) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select\n    'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the\n    checkboxes on the left.\n\n\nSelect chrI from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar.\n\n\nYou can also add more tracks using the \nAdd Tracks icon\n located on the\n    top right. Load one of the splice junction files such as 'Tophat on data 2\n    and data 1: splice junctions'.\n\n\nExplore the data and try to find a splice junction. Next to the\n    drop down list, click on the chromosomal position number\n    display and specify the location \nchrI:86985-87795\n to view an\n    intron junction.  \n\n\n\n\nIdeally we would add a gene model to the visualisation; but the\ngenes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser)\nhas a slightly different naming convention for one of the chromosomes\nthan the reference genome used by Galaxy, which will cause an error to\nbe thrown by Trackster if you try to add it. This is very typical of\ngenomics currently! If you are interested, you can fiddle with the\ngenes.gtf file to rename the chromosome '2-micron' to '2micron', which\nwill fix the problem.\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 3. Cuffdiff [40 min]\n\n\nThe aim in this section is to statistically test for differential expression\nusing Cuffdiff and obtain a list of significant genes.\n\n\n1.  Run Cuffdiff to identify differentially expressed genes and transcripts\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis \n Cuffdiff\n and set the parameters as follows:\n\n\n\n\nTranscripts:\n genes.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname\n batch\n\n\nReplicates:\n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key (Windows) or the command key (OSX).)\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname\n chem\n\n\nReplicates:\n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nNote: This step may take a while, depending on how busy the server is.\n\n\n2.  Explore the Cuffdiff output files\n\n\nThere should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 43, data 38, and others\". We'll\nmostly be interested in the file ending with 'gene differential expression\ntesting' which contains the statistical results from testing the level of\ngene expression between the batch condition and chem condition.\n\n\nFilter based on column 14 (\u2018significant\u2019) - a binary assessment of\nq_value \n 0.05, where q_value is p_value adjusted for multiple testing.\nUnder Basic Tools, click on \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nThis will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. There should be 53 differentially\nexpressed genes in this list.\n\n\nWe can rename this file by clicking on the \npencil icon\n of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Cuffdiff_Significant_DE_Genes\".\n\n\n\n\nSection 4. Count reads in features [30 min]\n\n\nHTSeq-count creates a count matrix using the number of the reads from each bam\nfile that map to the genomic features in the genes.gtf. For each feature (a\ngene for example) a count matrix shows how many reads were mapped to this\nfeature.\n\n\n\n\n\n\nUse HTSeq-count to count the number of reads for each feature.\n\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis \n SAM/BAM to count matrix\n and set the parameters as follows:  \n\n\n\n\nGene model (GFF) file to count reads over from your current history:\n genes.gtf\n\n\nbam/sam file from your history:\n\n  (Select all six bam files using the shift key.)\n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nExamine the outputted matrix by using the \neye icon\n.\n\n    Each column corresponds to a sample and each row corresponds to a gene. By\n    sight, see if you can find a gene you think is differentially expressed\n    from looking at the counts.\n\n\n\n\n\n\nWe now have a count matrix, with a count against each corresponding sample. We\nwill use this matrix in later sections to calculate the differentially\nexpressed genes.\n\n\n\n\nSection 5: edgeR  [30 min]\n\n\nedgeR\n\nis an R package, that is used for analysing differential expression of\nRNA-Seq data and can either use exact statistical methods or generalised\nlinear models.\n\n\n1.  Generate a list of differentially expressed genes using edgeR\n\n\nIn the Galaxy tool panel, under NGS Analysis, select\n\nNGS: RNA \n Differential_Count\n and set the parameters as follows:\n\n\n\n\nSelect an input matrix - rows are contigs, columns are counts for each\n  sample:\n bams to DGE count matrix_htseqsams2mx.xls\n\n\nTitle for job outputs:\n Differential_Counts_edgeR\n\n\nTreatment Name:\n Batch\n\n\nSelect columns containing treatment:\n  \n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\n\n\n\n\nControl Name:\n Chem\n\n\nSelect columns containing control:\n  \n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nRun this model using edgeR:\n Run edgeR\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n2.  Examine the outputs from the previous step\n\n\n\n\nExamine the Differential_Counts_edgeR_topTable_edgeR.xls file by\n    clicking on the \neye icon\n.\n    This file is a list of genes sorted by p-value from using EdgeR to\n    perform differential expression analysis.\n\n\nExamine the Differential_Counts_edgeR.html file. This file has some\n    output logs and plots from running edgeR. If you are familiar with R,\n    you can examine the R code used for analysis by scrolling to the bottom\n    of the file, and clicking Differential_Counts.Rscript to download the\n    Rscript file.\n\n    If you are curious about the statistical methods edgeR uses, you can\n    read the \nedgeR user's guide at\n    Bioconductor\n.\n\n\n\n\n3.  Extract the significant differentially expressed genes.\n\n\nUnder Basic Tools, click on \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Differential_Counts_edgeR_topTable_edgeR.xls\"\n\n\nWith following condition:\n c6 \n= 0.05\n\n\nExecute\n\n\n\n\nThis will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 55 genes in this file.\nRename this file by clicking on the \npencil icon\n of and change the name\nfrom \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".\n\n\n\n\nSection 6. DESeq2 [30 min]\n\n\nDESeq2\n is an\nR package that uses a negative binomial statistical model to find differentially\nexpressed genes. It can work without replicates (unlike edgeR) but the author\nstrongly advises against this for reasons of statistical validity.\n\n\n1.  Generate a list of differentially expressed genes using DESeq2\n\n\nIn the Galaxy tool panel, under NGS Analysis, select\n\nNGS: RNA Analysis \n Differential_Count\n and set the parameters as follows:  \n\n\n\n\nSelect an input matrix - rows are contigs, columns are counts for each\n  sample:\n bams to DGE count matrix_htseqsams2mx.xls\n\n\nTitle for job outputs:\n Differential_Counts_DESeq2\n\n\nTreatment Name:\n Batch\n\n\nSelect columns containing treatment:\n  \n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\n\n\n\n\nControl Name:\n Chem\n\n\nSelect columns containing control:\n  \n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nRun this model using edgeR:\n Do not run edgeR\n\n\nRun the same model with DESeq2 and compare findings:\n Run DESeq2\n\n\n\n\n2.  Examine the outputs the previous step\n\n\n\n\nExamine the Differential_Counts_DESeq2_topTable_DESeq2.xls file.\n    This file is a list of genes sorted by p-value from using DESeq2 to\n    perform differential expression analysis.\n\n\nExamine the Differential_Counts_DESeq2.html file. This file has some\n    output logs and plots from running DESeq2. Take a look at the PCA plot.\n\n\n\n\n\n\n\n\nMore info on PCA plots\n\n\n\n\nPCA plots are useful for exploratory data analysis. Samples which are more\nsimilar to each other are expected to cluster together. A count matrix often\nhas thousands of dimensions (one for each feature) and our PCA plot generated in\nthe previous step transforms the data so the most variability is represented in principal\ncomponents 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively).\n\n\n\n\nTake note of the scales on the x-axis and the y-axis. The x-axis representing\nthe first principal component accounts for 96% of the variance and ranges from\napproximately -6 to +6, while the y-axis ranges from approximately -1 to +1.\n\n\nFor both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition.\n\n\nAdditionally, within conditions, the lower glucose (chem) condition shows more\nvariability between replicates than the higher glucose (batch) condition.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n3.  Filter out the significant differentially expressed genes.\n\n\nUnder Basic Tools, click on \nFilter and Sort \n Filter\n:\n\n\n\n\nFilter:\n \"Differential_Counts_DESeq2_topTable_DESeq2.xls\"\n\n\nWith following condition:\n c7 \n= 0.05\n\n\nExecute\n\n\n\n\nThis will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 53 genes in this file.\nRename this file by clicking on the \npencil icon\n of and change the name\nfrom \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see\nthe first few differentially expressed genes are similar to the ones\nidentified by EdgeR.\n\n\n\n\nSection 7: How much concordance is there between methods?\n\n\nWe are interested in how similar the identified genes are between the different\nstatistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a\nVenn diagram to visualise the amount of overlap.\n\n\n\n\n\n\nGenerate a Venn diagram of the output of the 3 differential expression tools.\n\n    Note that column index 2 (or c3) contains the gene name in the CuffDiff output.\n    Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names.\n\n    In the Galaxy tool panel, under Statistics and Visualisation, select\n    \nGraph/Display Data \n proportional venn\n and set the parameters as follows:  \n\n\n\n\ntitle:\n Common genes\n\n\ninput file 1:\n Cuffdiff_Significant_DE_Genes\n\n\ncolumn index:\n 2\n\n\nas name:\n Cuffdiff\n\n\ninput file 2:\n edgeR_Significant_DE_Genes\n\n\ncolumn index file 2:\n 0\n\n\nas name file 2:\n edgeR\n\n\ntwo or three:\n three\n\n\ninput file 3:\n DESeq2_Significant_DE_Genes\n\n\ncolumn index file 3:\n 0\n\n\nas name file 3:\n DESeq2\n\n\nExecute\n\n\n\n\n\n\n\n\nView the generated Venn diagram.\n    Agreement between the tools is good: there are 49 differentially expressed\n    genes that all three tools agree upon, and only a handful that are\n    exclusive to each tool.\n\n\n\n\n\n\n\n\nGenerate the common list of significantly expressed genes identified by the\n    three mentioned tools by extracting the respective gene list columns and\n    intersecting:\n\n\n\n\n\n\nUnder Basic Tools in the Galaxy tool panel, select\n    \nText Manipulation \n cut\n\n\n\n\nCut columns:\n c3\n\n\nDelimited by:\n Tab\n\n\nFrom:\n Cuffdiff_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_gene_list'\n\n\n\n\n\n\nSelect \nText Manipulation \n cut\n\n\n\n\nCut columns:\n c1\n\n\nDelimited by:\n Tab\n\n\nFrom:\n edgeR_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'edgeR_gene_list'\n\n\n\n\n\n\nSelect \nText Manipulation \n cut\n\n\n\n\nCut columns:\n c1\n\n\nDelimited by:\n Tab\n\n\nFrom:\n DESeq2_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'DESeq2_gene_list'\n\n\n\n\n\n\nUnder Basic Tools in the Galaxy tool panel, select\n    \nJoin, Subtract and Group \n Compare two Datasets\n\n\n\n\nCompare:\n Cuffdiff_gene_list\n\n\nagainst:\n edgeR_gene_list\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_edgeR_common_gene_list'\n\n\n\n\n\n\nSelect \nJoin, Subtract and Group \n Compare two Datasets\n\n\n\n\nCompare:\n Cuffdiff_edgeR_common_gene_list\n\n\nagainst:\n DESeq2_gene_list\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list'\n\n\n\n\n\n\n\n\n\n\nWe now have a list of 49 genes that have been identified as significantly\ndifferentially expressed by all three tools.\n\n\n\n\nSection 8: Gene set enrichment analysis\n\n\nThe biological question being asked in the original paper is essentially:\n\n\n\"What is the global response of the yeast transcriptome in the shift from\ngrowth at glucose excess conditions (batch) to glucose-limited conditions\n(chemostat)?\"\n  \n\n\nWe can address this question by attempting to interpret our differentially\nexpressed gene list at a higher level, perhaps by examining the categories of\ngene and protein networks that change in response to glucose.\n\n\nFor example, we can input our list of differentially expressed genes to a Gene\nOntology (GO) enrichment analysis tool such as GOrilla to find out the GO\nenriched terms.\n\n\nNOTE: Because of time-constraints in this tutorial, the analysis was confined to\na single chromosome (chromosome I) and as a consequence we don\u2019t have\nsufficient information to look for groups of differentially expressed genes\n(simply because we don\u2019t have enough genes identified from the one chromosome to\nlook for statistically convincing over-representation of any particular gene\ngroup).\n\n\n\n\n\n\nDownload the list of genes \nhere in a plain-text file\n\n    to your local computer by right clicking on the link and selecting Save Link As...\n\n\nNote that there are ~2500 significantly differentially expressed genes\nidentified in the full analysis. Also note that the genes are ranked in\norder of statistical significance. This is critical for the next step.\n\n\n\n\n\n\nExplore the data using gene set enrichment analysis (GSEA) using the online\n    tool GOrilla\n\n\n\n\nGo to \ncbl-gorilla.cs.technion.ac.il\n\n\nChoose Organism:\n Saccharomyces cerevisiae\n\n\nChoose running mode:\n Single ranked list of genes\n\n\nOpen the gene list you downloaded in the previous step in a text editor.\n  Select the full list, then copy and paste the list into the text box.\n\n\nChoose an Ontology:\n Process\n\n\nSearch Enriched GO terms\n\n\n\n\n\n\n\n\nOnce the analysis has finished running, you will be redirected to a\n    page depicting the GO enriched biological processes and its significance\n    (indicated by colour), based on the genes you listed.\n\n\n\nScroll down to view a table of GO terms and their significance scores.\nIn the last column, you can toggle the \n[+] Show genes\n to see the\nlist of associated genes.\n\n\n\n\n\n\nExperiment with different ontology categories (Function, Component) in GOrilla.\n\n\n\n\n\n\n\n\n\n\nAt this stage you are interpreting the experiment in different ways, potentially\ndiscovering information that will lead you to further lab experiments. This\nis driven by your biological knowledge of the problem space. There are an\nunlimited number of methods for further interpretation of which GSEA is just one.\n\n\n\n\nOptional extension: Degust\n\n\nDegust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at \nvicbioinformatics.com/degust/\n.\n\n\n\n\n1. Load count data into Degust\n\n\n\n\nIn Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\"\n    generated in Section 4 using the \ndisk icon\n.\n\n\nGo to \nvicbioinformatics.com/degust/\n\n    and click on \"Upload your counts file\".\n\n\nClick \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.\n\n\n\n\n2. Configure your uploaded data\n\n\n\n\nGive your visualisation a name.\n\n\nFor the Info column, select Contig.\n\n\nAdd two conditions: batch and chem. For each condition, select the three\n    samples which correspond with the condition.\n\n\nClick \nSave changes\n and view your data.\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n3. Explore your data\n\n\nRead through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.\n\n\n\n\nOn the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.\n\n\nOn the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.\n\n\n4. Explore the demo data\n\n\nDegust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.\n\n\n\n\nReferences\n\n\n[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10", 
            "title": "RNA-seq DGE Advanced"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#rna-seq-differential-gene-expression-advanced-tutorial", 
            "text": "Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung", 
            "title": "RNA-Seq Differential Gene Expression: Advanced Tutorial"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#tutorial-overview", 
            "text": "In this tutorial we compare the performance of three statistically-based\nexpression analysis tools:     CuffDiff  EdgeR  DESeq2   This tutorial builds on top of the  basic RNA-seq DGE tutorial .\nIt is recommended to have some familiarity of RNA-seq before beginning this\ntutorial.", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#background-15-min", 
            "text": "", 
            "title": "Background [15 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#where-does-the-data-in-this-tutorial-come-from", 
            "text": "The data for this tutorial is from the paper,  A comprehensive comparison of\nRNA-Seq-based transcriptome analysis from reads to differential gene expression\nand cross-comparison with microarrays: a case study in Saccharomyces\ncerevisiae  by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK\n113-7D (yeast) under two different metabolic conditions: glucose-excess (batch)\nor glucose-limited (chemostat).  The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with\naccession SRS307298. There are 6 samples in total-- two treatments with\nthree biological replicates each. The data is paired-end.    We have extracted chromosome I reads from the samples to make the\ntutorial a suitable length. This has implications, as discussed in section 8.", 
            "title": "Where does the data in this tutorial come from?"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-1-preparation-15-min", 
            "text": "", 
            "title": "Section 1: Preparation [15 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account", 
            "text": "Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User   Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User   Login .", 
            "title": "1.  Register as a new user in Galaxy if you don\u2019t already have an account"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-import-the-rna-seq-data-for-the-workshop", 
            "text": "If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data   Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Adv_Sec_1 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data   Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.  Upload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the batch condition\n    (glucose-excess). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n     \n     \n    These six files are three paired-end samples from the chem condition\n    (glucose-limited). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n     \n     \n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf\n       You should now have these 13 files in your history:   batch1_chrI_1.fastq  batch1_chrI_2.fastq  batch2_chrI_1.fastq  batch2_chrI_2.fastq  batch3_chrI_1.fastq  batch3_chrI_2.fastq  chem1_chrI_1.fastq  chem1_chrI_2.fastq  chem2_chrI_1.fastq  chem2_chrI_2.fastq  chem3_chrI_1.fastq  chem3_chrI_2.fastq  genes.gtf   These files can be renamed by clicking the  pen icon  if you wish.  Note:  The reads are paired end; for example batch1_chrI_1.fastq and\nbatch1_chrI_2.fastq are paired reads from one sequencing run. Low quality\nreads have already been trimmed.", 
            "title": "2.  Import the RNA-seq data for the workshop."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-2-alignment-30-mins", 
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found  here .", 
            "title": "Section 2: Alignment [30 mins]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-mapalign-the-reads-with-tophat-to-the-s-cerevisiae-reference", 
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis   Tophat  and set the parameters as follows:     Is this single-end or paired-end data?  Paired-end (as individual datasets)     RNA-Seq FASTQ file, forward reads: \n(Click on the  multiple datasets icon  and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)   batch1_chrI_1.fastq  batch2_chrI_1.fastq  batch3_chrI_1.fastq  chem1_chrI_1.fastq  chem2_chrI_1.fastq  chem3_chrI_1.fastq     RNA-Seq FASTQ file, reverse reads: \n(Click on the  multiple datasets icon  and select all six of the reverse\nFASTQ files ending in *2.fastq.)     batch1_chrI_2.fastq  batch2_chrI_2.fastq  batch3_chrI_2.fastq  chem1_chrI_2.fastq  chem2_chrI_2.fastq  chem3_chrI_2.fastq    Use a built in reference genome or own from your history:  Use\nbuilt-in genome  Select a reference genome:  S. cerevisiae June 2008 (SGD/SacCer2)\n(sacCer2)  Use defaults for the other fields  Execute     Show screenshot       (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)  \n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    });\n      Note: This may take a few minutes, depending on how busy the server is.", 
            "title": "1.  Map/align the reads with Tophat to the S. cerevisiae reference"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-rename-the-output-files", 
            "text": "You should have 5 output files for each of the FASTQ input files:   Tophat on data 2 and data 1: accepted_hits:  This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.  Tophat on data 2 and data 1: splice junctions:  This file lists all the places\n  where TopHat had to split a read into two pieces to span an exon\n  junction.  Tophat on data 2 and data 1 deletions  and  Tophat on data 2 and data 1: insertions: \n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.  Tophat on data 2 and data 1: align_summary:  This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.   You should have a total of 30 Tophat output files in your history.  Rename the 6 accepted_hits files into a more meaningful name (e.g.\n'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam')\nby using the  pen icon  next to the file.", 
            "title": "2.  Rename the output files"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-visualise-the-aligned-reads-with-trackster", 
            "text": "On the top bar of Galaxy, select  Visualization   New Track Browser .  Name your new visualization and select S. cerevisiae (sacCer2) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select\n    'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the\n    checkboxes on the left.  Select chrI from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar.  You can also add more tracks using the  Add Tracks icon  located on the\n    top right. Load one of the splice junction files such as 'Tophat on data 2\n    and data 1: splice junctions'.  Explore the data and try to find a splice junction. Next to the\n    drop down list, click on the chromosomal position number\n    display and specify the location  chrI:86985-87795  to view an\n    intron junction.     Ideally we would add a gene model to the visualisation; but the\ngenes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser)\nhas a slightly different naming convention for one of the chromosomes\nthan the reference genome used by Galaxy, which will cause an error to\nbe thrown by Trackster if you try to add it. This is very typical of\ngenomics currently! If you are interested, you can fiddle with the\ngenes.gtf file to rename the chromosome '2-micron' to '2micron', which\nwill fix the problem.  Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.", 
            "title": "3.  Visualise the aligned reads with Trackster"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-3-cuffdiff-40-min", 
            "text": "The aim in this section is to statistically test for differential expression\nusing Cuffdiff and obtain a list of significant genes.", 
            "title": "Section 3. Cuffdiff [40 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts", 
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis   Cuffdiff  and set the parameters as follows:   Transcripts:  genes.gtf  Condition:     1: Condition  name  batch  Replicates:  batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam \n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key (Windows) or the command key (OSX).)      2: Condition  name  chem  Replicates:  chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam          Use defaults for the other fields  Execute     Show screenshot       \n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n      Note: This step may take a while, depending on how busy the server is.", 
            "title": "1.  Run Cuffdiff to identify differentially expressed genes and transcripts"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-explore-the-cuffdiff-output-files", 
            "text": "There should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 43, data 38, and others\". We'll\nmostly be interested in the file ending with 'gene differential expression\ntesting' which contains the statistical results from testing the level of\ngene expression between the batch condition and chem condition.  Filter based on column 14 (\u2018significant\u2019) - a binary assessment of\nq_value   0.05, where q_value is p_value adjusted for multiple testing.\nUnder Basic Tools, click on  Filter and Sort   Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   This will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. There should be 53 differentially\nexpressed genes in this list.  We can rename this file by clicking on the  pencil icon  of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Cuffdiff_Significant_DE_Genes\".", 
            "title": "2.  Explore the Cuffdiff output files"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-4-count-reads-in-features-30-min", 
            "text": "HTSeq-count creates a count matrix using the number of the reads from each bam\nfile that map to the genomic features in the genes.gtf. For each feature (a\ngene for example) a count matrix shows how many reads were mapped to this\nfeature.    Use HTSeq-count to count the number of reads for each feature. \n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis   SAM/BAM to count matrix  and set the parameters as follows:     Gene model (GFF) file to count reads over from your current history:  genes.gtf  bam/sam file from your history: \n  (Select all six bam files using the shift key.)  batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam  chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Use defaults for the other fields  Execute     Examine the outputted matrix by using the  eye icon . \n    Each column corresponds to a sample and each row corresponds to a gene. By\n    sight, see if you can find a gene you think is differentially expressed\n    from looking at the counts.    We now have a count matrix, with a count against each corresponding sample. We\nwill use this matrix in later sections to calculate the differentially\nexpressed genes.", 
            "title": "Section 4. Count reads in features [30 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-5-edger-30-min", 
            "text": "edgeR \nis an R package, that is used for analysing differential expression of\nRNA-Seq data and can either use exact statistical methods or generalised\nlinear models.", 
            "title": "Section 5: edgeR  [30 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-edger", 
            "text": "In the Galaxy tool panel, under NGS Analysis, select NGS: RNA   Differential_Count  and set the parameters as follows:   Select an input matrix - rows are contigs, columns are counts for each\n  sample:  bams to DGE count matrix_htseqsams2mx.xls  Title for job outputs:  Differential_Counts_edgeR  Treatment Name:  Batch  Select columns containing treatment:     batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam    Control Name:  Chem  Select columns containing control:     chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Run this model using edgeR:  Run edgeR  Use defaults for the other fields  Execute", 
            "title": "1.  Generate a list of differentially expressed genes using edgeR"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-from-the-previous-step", 
            "text": "Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by\n    clicking on the  eye icon .\n    This file is a list of genes sorted by p-value from using EdgeR to\n    perform differential expression analysis.  Examine the Differential_Counts_edgeR.html file. This file has some\n    output logs and plots from running edgeR. If you are familiar with R,\n    you can examine the R code used for analysis by scrolling to the bottom\n    of the file, and clicking Differential_Counts.Rscript to download the\n    Rscript file. \n    If you are curious about the statistical methods edgeR uses, you can\n    read the  edgeR user's guide at\n    Bioconductor .", 
            "title": "2.  Examine the outputs from the previous step"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-extract-the-significant-differentially-expressed-genes", 
            "text": "Under Basic Tools, click on  Filter and Sort   Filter :   Filter:  \"Differential_Counts_edgeR_topTable_edgeR.xls\"  With following condition:  c6  = 0.05  Execute   This will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 55 genes in this file.\nRename this file by clicking on the  pencil icon  of and change the name\nfrom \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".", 
            "title": "3.  Extract the significant differentially expressed genes."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-6-deseq2-30-min", 
            "text": "DESeq2  is an\nR package that uses a negative binomial statistical model to find differentially\nexpressed genes. It can work without replicates (unlike edgeR) but the author\nstrongly advises against this for reasons of statistical validity.", 
            "title": "Section 6. DESeq2 [30 min]"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-deseq2", 
            "text": "In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis   Differential_Count  and set the parameters as follows:     Select an input matrix - rows are contigs, columns are counts for each\n  sample:  bams to DGE count matrix_htseqsams2mx.xls  Title for job outputs:  Differential_Counts_DESeq2  Treatment Name:  Batch  Select columns containing treatment:     batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam    Control Name:  Chem  Select columns containing control:     chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Run this model using edgeR:  Do not run edgeR  Run the same model with DESeq2 and compare findings:  Run DESeq2", 
            "title": "1.  Generate a list of differentially expressed genes using DESeq2"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-the-previous-step", 
            "text": "Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file.\n    This file is a list of genes sorted by p-value from using DESeq2 to\n    perform differential expression analysis.  Examine the Differential_Counts_DESeq2.html file. This file has some\n    output logs and plots from running DESeq2. Take a look at the PCA plot.     More info on PCA plots   PCA plots are useful for exploratory data analysis. Samples which are more\nsimilar to each other are expected to cluster together. A count matrix often\nhas thousands of dimensions (one for each feature) and our PCA plot generated in\nthe previous step transforms the data so the most variability is represented in principal\ncomponents 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively).   Take note of the scales on the x-axis and the y-axis. The x-axis representing\nthe first principal component accounts for 96% of the variance and ranges from\napproximately -6 to +6, while the y-axis ranges from approximately -1 to +1.  For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition.  Additionally, within conditions, the lower glucose (chem) condition shows more\nvariability between replicates than the higher glucose (batch) condition.     \n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "2.  Examine the outputs the previous step"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-filter-out-the-significant-differentially-expressed-genes", 
            "text": "Under Basic Tools, click on  Filter and Sort   Filter :   Filter:  \"Differential_Counts_DESeq2_topTable_DESeq2.xls\"  With following condition:  c7  = 0.05  Execute   This will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 53 genes in this file.\nRename this file by clicking on the  pencil icon  of and change the name\nfrom \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see\nthe first few differentially expressed genes are similar to the ones\nidentified by EdgeR.", 
            "title": "3.  Filter out the significant differentially expressed genes."
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-7-how-much-concordance-is-there-between-methods", 
            "text": "We are interested in how similar the identified genes are between the different\nstatistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a\nVenn diagram to visualise the amount of overlap.    Generate a Venn diagram of the output of the 3 differential expression tools. \n    Note that column index 2 (or c3) contains the gene name in the CuffDiff output.\n    Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. \n    In the Galaxy tool panel, under Statistics and Visualisation, select\n     Graph/Display Data   proportional venn  and set the parameters as follows:     title:  Common genes  input file 1:  Cuffdiff_Significant_DE_Genes  column index:  2  as name:  Cuffdiff  input file 2:  edgeR_Significant_DE_Genes  column index file 2:  0  as name file 2:  edgeR  two or three:  three  input file 3:  DESeq2_Significant_DE_Genes  column index file 3:  0  as name file 3:  DESeq2  Execute     View the generated Venn diagram.\n    Agreement between the tools is good: there are 49 differentially expressed\n    genes that all three tools agree upon, and only a handful that are\n    exclusive to each tool.     Generate the common list of significantly expressed genes identified by the\n    three mentioned tools by extracting the respective gene list columns and\n    intersecting:    Under Basic Tools in the Galaxy tool panel, select\n     Text Manipulation   cut   Cut columns:  c3  Delimited by:  Tab  From:  Cuffdiff_Significant_DE_Genes  Execute   Rename the output to something like 'Cuffdiff_gene_list'    Select  Text Manipulation   cut   Cut columns:  c1  Delimited by:  Tab  From:  edgeR_Significant_DE_Genes  Execute   Rename the output to something like 'edgeR_gene_list'    Select  Text Manipulation   cut   Cut columns:  c1  Delimited by:  Tab  From:  DESeq2_Significant_DE_Genes  Execute   Rename the output to something like 'DESeq2_gene_list'    Under Basic Tools in the Galaxy tool panel, select\n     Join, Subtract and Group   Compare two Datasets   Compare:  Cuffdiff_gene_list  against:  edgeR_gene_list  Use defaults for the other fields  Execute   Rename the output to something like 'Cuffdiff_edgeR_common_gene_list'    Select  Join, Subtract and Group   Compare two Datasets   Compare:  Cuffdiff_edgeR_common_gene_list  against:  DESeq2_gene_list  Use defaults for the other fields  Execute   Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list'      We now have a list of 49 genes that have been identified as significantly\ndifferentially expressed by all three tools.", 
            "title": "Section 7: How much concordance is there between methods?"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-8-gene-set-enrichment-analysis", 
            "text": "The biological question being asked in the original paper is essentially:  \"What is the global response of the yeast transcriptome in the shift from\ngrowth at glucose excess conditions (batch) to glucose-limited conditions\n(chemostat)?\"     We can address this question by attempting to interpret our differentially\nexpressed gene list at a higher level, perhaps by examining the categories of\ngene and protein networks that change in response to glucose.  For example, we can input our list of differentially expressed genes to a Gene\nOntology (GO) enrichment analysis tool such as GOrilla to find out the GO\nenriched terms.  NOTE: Because of time-constraints in this tutorial, the analysis was confined to\na single chromosome (chromosome I) and as a consequence we don\u2019t have\nsufficient information to look for groups of differentially expressed genes\n(simply because we don\u2019t have enough genes identified from the one chromosome to\nlook for statistically convincing over-representation of any particular gene\ngroup).    Download the list of genes  here in a plain-text file \n    to your local computer by right clicking on the link and selecting Save Link As...  Note that there are ~2500 significantly differentially expressed genes\nidentified in the full analysis. Also note that the genes are ranked in\norder of statistical significance. This is critical for the next step.    Explore the data using gene set enrichment analysis (GSEA) using the online\n    tool GOrilla   Go to  cbl-gorilla.cs.technion.ac.il  Choose Organism:  Saccharomyces cerevisiae  Choose running mode:  Single ranked list of genes  Open the gene list you downloaded in the previous step in a text editor.\n  Select the full list, then copy and paste the list into the text box.  Choose an Ontology:  Process  Search Enriched GO terms     Once the analysis has finished running, you will be redirected to a\n    page depicting the GO enriched biological processes and its significance\n    (indicated by colour), based on the genes you listed.  Scroll down to view a table of GO terms and their significance scores.\nIn the last column, you can toggle the  [+] Show genes  to see the\nlist of associated genes.    Experiment with different ontology categories (Function, Component) in GOrilla.      At this stage you are interpreting the experiment in different ways, potentially\ndiscovering information that will lead you to further lab experiments. This\nis driven by your biological knowledge of the problem space. There are an\nunlimited number of methods for further interpretation of which GSEA is just one.", 
            "title": "Section 8: Gene set enrichment analysis"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#optional-extension-degust", 
            "text": "Degust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at  vicbioinformatics.com/degust/ .", 
            "title": "Optional extension: Degust"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-load-count-data-into-degust", 
            "text": "In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\"\n    generated in Section 4 using the  disk icon .  Go to  vicbioinformatics.com/degust/ \n    and click on \"Upload your counts file\".  Click \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.", 
            "title": "1. Load count data into Degust"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-configure-your-uploaded-data", 
            "text": "Give your visualisation a name.  For the Info column, select Contig.  Add two conditions: batch and chem. For each condition, select the three\n    samples which correspond with the condition.  Click  Save changes  and view your data.     Show screenshot       \n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "2. Configure your uploaded data"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-explore-your-data", 
            "text": "Read through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.   On the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.  On the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.", 
            "title": "3. Explore your data"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#4-explore-the-demo-data", 
            "text": "Degust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.", 
            "title": "4. Explore the demo data"
        }, 
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#references", 
            "text": "[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10", 
            "title": "References"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/", 
            "text": "RNA-Seq Experimental Design\n\n\nWhat is RNA-seq?\n\n\nRNA-seq is a method of measuring gene expression using shotgun sequencing. The\nprocess involves reverse transcribing RNA into cDNA, then sequencing fragments\non a high-throughput platform such as Illumina to obtain a large number of\nshort reads. For each sample, the reads are then aligned to a genome, and the\nnumber of reads aligned to each gene or feature is recorded.\n\n\nA typical RNA-seq experiment aims to find differentially expressed genes\nbetween two conditions (e.g. up and down-regulated genes in knock-out mice\ncompared to wild-type mice). RNA-seq can also be used to discover new\ntranscripts, splice variants, and fusion genes.\n\n\nWhy is a good experimental design vital?\n\n\nAn RNA-seq experiment produces high dimensional data. This means we get a huge\nnumber of observations for a small number of samples. For example, the\nexpression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3\nwild-type). A frequently used approach to analyse RNA-seq data is to fit each\ngene to a linear model where for each of the 20,000 genes, parameters need to\nbe estimated using a small number of observations. To complicate matters, each\nmeasurement of gene expression is comprised of a mix of biological signal and\nunwanted noise. Thus, in order to perform a robust statistical analysis, the\nmethodology must be carefully designed.\n\n\nBefore you begin any RNA-seq experiment, some questions you should ask\nyourself are:\n\n\n\n\nWhy do you expect to find differentially expressed genes in the particular\n   tissue?\n\n\nWhat types of genes do you expect to find differentially expressed?\n\n\nWhat are the sources of variability from your samples?\n\n\nWhere do you expect most of your variation to come from?\n\n\n\n\nA coherent experimental design is the groundwork of a successful experiment.\nYou should invest time and thought in designing a robust experiment as failing\nto think this step through can lead to unusable data and wasted time, money,\nand effort.\n\n\nIt is also useful to think about the statistical methods you will use to\nanalyse the data. If you're planning to bring a data analyst or\nbioinformatician onboard for data analysis, you should include him or her in\nthe experimental design stage.\n\n\nTerminology\n\n\nBefore progressing, it may be useful to define some terms which are commonly\nused in RNA-seq.\n\n\n\n  \n\n    \nVariability:\n    \nA measure of how much the data is spread around. Variance is\n    mathematically defined as the average of the squared difference between\n    observations and the expected value. Simply put, a larger variance means\n    it is harder to identify differentially expressed genes.\n  \n\n    \nFeature:\n    \nA defined genomic region. Usually a gene in RNA-seq, but can also\n    refer to any region such as an exon or an isoform. In RNA-seq, an estimate\n    of abundance is obtained for each feature.\n  \n\n    \nBiological replicates:\n    \nSamples that have been obtained from biologically separate samples.\n    This can mean different individual organisms (e.g. tissue samples from\n    different mice), different samplings of the same tumour, or different\n    population of cells grown separately from each other but originating from\n    the same cell-line. For example, the samples obtained from three different\n    knock-out mice could be considered biological replicates in a knock-out\n    versus wild-type experiment. A biological replicate combines both technical\n    and biological variability as it is also an independent case of all the\n    technical steps.\n  \n\n    \nTechnical replicates:\n    \nSamples in which the starting biological sample is the same, but the\n    replicates are processed separately. For example, if a biological sample\n    is divided and two different library preps are processed and sequenced,\n    those two samples would be considered technical replicates.\n  \n\n    \nCovariate:\n    \nThe term 'covariate' is often used interchangeably with 'factor' or\n    'variable' in RNA-seq. The term refers to a property of the sample which\n    may have some influence on gene expression and should be represented in\n    the RNA-seq model. Covariates in RNA-seq are often categorical (e.g.\n    treatment condition, sex, batch), but continuous factors are also possible\n    (e.g. time points, age). A linear model will contain terms to represent\n    the relationships between covariates and each sample. Each possible value\n    a factor can take is called a level (e.g. 'male' and 'female' are two\n    levels in the factor 'sex'). Factors can either be directly of interest\n    to the experiment (e.g. treatment condition) or not of interest (also\n    known as nuisance variables) (e.g. sex, batch). The purpose of covariates\n    is to explain the variance seen in samples.\n  \n\n    \nConfounding variable:\n    \nA confounding variable is a nuisance variable that is associated with\n    the factor of interest. Possible confounding factors should be controlled\n    for so they don't interfere with analysis. For example, if all knock-out\n    mice samples were harvested in the morning and all wild-type mice samples\n    were harvested in the afternoon, the time of sample collection would be a\n    confounding factor as the effects from sample collection time and from the\n    knock-out cannot be separated.\n  \n\n    \nStatistical power:\n    \nThe ability to identify differentially expressed genes when there\n    really is a difference. This is partly dependent on variance and therefore\n    is affected by the number of replicates available and sequencing depth.\n\n\n\n\nThe importance of replicates to estimate variance\n\n\nWhen performing a differential gene expression analysis, we look at the\nexpression values of each gene and try to determine if the expression is\nsignificantly different in the different conditions (e.g. knock-out and\nwild-type). The ability to distinguish whether a gene is differentially\nexpressed is partly determined by the estimates of variability obtained by\nusing multiple observations in each condition.\n\n\nVariability is present in two forms: technical variability and biological\nvariability.\n\n\nCombined biological and technical variability is measured using biological \nreplicates. Biological variability is the main source of variability and is \ndue to natural variation in the population and within cells. This includes \ndifferent individuals having different levels of a particular gene and the \nstochastic nature of expression levels in different cells.\n\n\nTechnical variability is measured using technical replicates. Technical\nvariability is often very small compared to biological variability.  Usually\nthe question is whether an observed difference is greater than the total \nvariability (i.e. significant).  As combined variability is measured by \nbiological replicates technical replicates are only important if you need to\nknow the degree of biological variability or technical variability.  An\nexample of wanting technical variability would be method development. The\nmain source of technical variation comes from RNA processing and from \nlibrary prep. Variability from sequencing in different flow cells or different\nlanes is usually minimal. Generally, creating technical replicates from multiple\nlibrary preps is unnecessary for RNA-seq experiments.\n\n\nThe amount of variance between your biological replicates will affect the\noutcome of your analysis. Ideally, you aim to have minimal variability between\nsamples so you only measure the effect of the condition of interest. Too much\nvariability between samples can drown out the signal of truly differentially\nexpressed genes. Controlling for possible confounding factors between\nconditions is also important to prevent falsely attributing differential\nexpression to the condition of interest.\n\n\nStrategies to minimise variation between samples and to control confounding\nvariables include:\n\n\n\n\nchoosing organisms from the same litter,\n\n\nchoosing organisms of the same sex if possible,\n\n\nusing a constant sample collection time,\n\n\nhaving the same laboratory technician perform each library prep,\n\n\nrandomising samples to prevent a confounding batch effect if all samples\n   can't be processed at one time.\n\n\n\n\nIf variation between samples can not be removed it should be balanced between\nconditions of interest as much as possible, and carefully recorded to allow\nits effect to be measured and potentially removed during analysis.\n\n\nHow many replicates and how many reads do I need?\n\n\nTwo very common question asked are:\n\n\n\n\nhow many biological replicates do I need, and\n\n\nwhat sequencing depth is needed for each sample\n\n\n\n\nin order to have enough statistical power for my RNA-seq experiment?\n\n\nThese questions cannot be precisely answered without a pilot study. A small\namount of data (minimum of two biological replicates for each condition with\nat least 10M reads) can estimate the amount of biological variation, which\ndetermines how many biological replicates are required. Performing a pilot\nstudy is highly recommended to estimate statistical power and identify\npossible problems before investing more time and money into the project.\n\n\nScotty\n is a web-based tool\nthat uses data generated from a pilot study to optimize a design for\nstatistical power. With a limited budget, one must balance sequence coverage\nand number of biological replicates. Scotty also has a cost estimate feature\nwhich returns the most powerful design within budget constraints.\n\n\nAs a general rule, the number of biological replicates should never be below 3.\nFor a basic RNA-seq differential expression experiment, 10M to 20M reads per\nsample is usually enough.  If similar data exists it can be helpful to check\nthe read counts for key genes of interest to estimate the required depth.\n\n\nBiological variability is usually the largest effect limiting the power of \nRNA-seq analysis.  The most improvement in an experiment will usually be \nachieved by increasing the biological replication to improve estimation of \nthe biological variation.\n\n\nIt is often possible to design experiments where the analysis is done \nincrementally such that a pilot study is added to with an additional block of\nsamples or a pool of libraries is sequenced to additional depth. In these cases\ncare must be taken to balance the design in a manner that each stage is a\nvalid experiment in its own right.  This can allow a focused question to be \nanswered in the first stage, with an ability to either address issues or \nprogress to a second stage with additional questions.\n\n\nSequencing options to consider\n\n\nHow much total RNA is needed:\n\nMany sequencing centres such as\n\nAGRF\n\nrecommend at least 250ng of total RNA for RNA sequencing. It is possible to go\nas low as 100ng of total RNA, but results are not guaranteed. The quality of\nRNA is also important when making libraries. A RNA Integrity Number (RIN) is a\nnumber from 1 (poor) to 10 (good) and can indicate how much degradation there\nis in the sample. A poor score can lead to over representation at the 3' end\nof the transcript and low yield. Samples with low RIN scores (below 8) are\nnot recommended for sequencing.  Care should also be taken to ensure RIN is\nconsistent between conditions to avoid confounding this technical effect with\nthe biological question.\n\n\nChoosing an enrichment method:\n\nRibosomal RNA makes up \n95% of total cellular RNA, so a preparation for\nRNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA.\nPoly-A enrichment is recommended for most standard RNA-seq experiments, but\nwill not provide information about microRNAs and other non-coding RNA species.\nIn general, ribo-depleted RNA-seq data will contain more noise, however, the\nprotocol is recommended if you have poor or variable quality of RNA as the 3\u2019\nbias of poly-A enrichment will be more pronounced with increased RNA\ndegradation. The amount of RNA needed for each method differs. For Poly-A\nenrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of\n200ng is recommended.\n\n\nChoosing read type:\n\nFor basic differential expression analysis RNA-seq experiments, single-end\nsequencing is recommended to obtain gene transcript counts. In more advanced\nexperiments, paired-ends are useful for determining transcript structure and\ndiscovering splice variants.\n\n\nChoosing strandedness:\n  \n\n\nWith a non-directional (unstranded) protocol, there is no way to identify\nwhether a read originated from the coding strand or its reverse complement.\nNon-directional protocols allow mapping of a read to a genomic location, but\nnot the direction in which the RNA was transcribed. They are therefore used to\ncount transcripts for known genes, and are recommended for basic RNA-seq\nexperiments. Directional protocols (stranded) preserve strand information and\nare useful for novel transcript discovery.\n\n\nMultiplexing:\n\nMultiplexing is an approach to sequence multiple samples in the same\nsequencing lane. By sequencing all samples in the same lane, multiplexing can\nalso minimise bias from lane effects.\n\n\nSpike-in controls:\n\nRNA-seq spike-in controls are a set of synthetic RNAs of known concentration\nwhich act as negative or positive controls. These controls have been used for\nnormalisation and quality control, but recent work has shown that the amount\nof technical variability in their use dramatically reduces their utility.\n\n\nSummary\n\n\n\n\nA good experimental design is vital for a successful experiment. If you're\n   planning to work with a data analyst or bioinformatician, include them in\n   the design stage.\n\n\nAim to minimise variability by identifying possible sources of variance in\n   your samples.\n\n\nBiological replicates are important. The number of biological replicates\n   you should have should never be below 3. Technical replicates are often\n   unnecessary.\n\n\nPilot studies are highly recommended for identifying how many replicates\n   and how many reads you should have for enough statistical power in your\n   experiment.\n\n\nFor basic RNA-seq experiments, poly-A enriched, single-ended, unstranded\n   sequencing at depths of 10M to 20M is probably what you want.", 
            "title": "RNA-seq DGE Experimental Design"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#rna-seq-experimental-design", 
            "text": "", 
            "title": "RNA-Seq Experimental Design"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#what-is-rna-seq", 
            "text": "RNA-seq is a method of measuring gene expression using shotgun sequencing. The\nprocess involves reverse transcribing RNA into cDNA, then sequencing fragments\non a high-throughput platform such as Illumina to obtain a large number of\nshort reads. For each sample, the reads are then aligned to a genome, and the\nnumber of reads aligned to each gene or feature is recorded.  A typical RNA-seq experiment aims to find differentially expressed genes\nbetween two conditions (e.g. up and down-regulated genes in knock-out mice\ncompared to wild-type mice). RNA-seq can also be used to discover new\ntranscripts, splice variants, and fusion genes.", 
            "title": "What is RNA-seq?"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#why-is-a-good-experimental-design-vital", 
            "text": "An RNA-seq experiment produces high dimensional data. This means we get a huge\nnumber of observations for a small number of samples. For example, the\nexpression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3\nwild-type). A frequently used approach to analyse RNA-seq data is to fit each\ngene to a linear model where for each of the 20,000 genes, parameters need to\nbe estimated using a small number of observations. To complicate matters, each\nmeasurement of gene expression is comprised of a mix of biological signal and\nunwanted noise. Thus, in order to perform a robust statistical analysis, the\nmethodology must be carefully designed.  Before you begin any RNA-seq experiment, some questions you should ask\nyourself are:   Why do you expect to find differentially expressed genes in the particular\n   tissue?  What types of genes do you expect to find differentially expressed?  What are the sources of variability from your samples?  Where do you expect most of your variation to come from?   A coherent experimental design is the groundwork of a successful experiment.\nYou should invest time and thought in designing a robust experiment as failing\nto think this step through can lead to unusable data and wasted time, money,\nand effort.  It is also useful to think about the statistical methods you will use to\nanalyse the data. If you're planning to bring a data analyst or\nbioinformatician onboard for data analysis, you should include him or her in\nthe experimental design stage.", 
            "title": "Why is a good experimental design vital?"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#terminology", 
            "text": "Before progressing, it may be useful to define some terms which are commonly\nused in RNA-seq.  \n   \n     Variability:\n     A measure of how much the data is spread around. Variance is\n    mathematically defined as the average of the squared difference between\n    observations and the expected value. Simply put, a larger variance means\n    it is harder to identify differentially expressed genes.\n   \n     Feature:\n     A defined genomic region. Usually a gene in RNA-seq, but can also\n    refer to any region such as an exon or an isoform. In RNA-seq, an estimate\n    of abundance is obtained for each feature.\n   \n     Biological replicates:\n     Samples that have been obtained from biologically separate samples.\n    This can mean different individual organisms (e.g. tissue samples from\n    different mice), different samplings of the same tumour, or different\n    population of cells grown separately from each other but originating from\n    the same cell-line. For example, the samples obtained from three different\n    knock-out mice could be considered biological replicates in a knock-out\n    versus wild-type experiment. A biological replicate combines both technical\n    and biological variability as it is also an independent case of all the\n    technical steps.\n   \n     Technical replicates:\n     Samples in which the starting biological sample is the same, but the\n    replicates are processed separately. For example, if a biological sample\n    is divided and two different library preps are processed and sequenced,\n    those two samples would be considered technical replicates.\n   \n     Covariate:\n     The term 'covariate' is often used interchangeably with 'factor' or\n    'variable' in RNA-seq. The term refers to a property of the sample which\n    may have some influence on gene expression and should be represented in\n    the RNA-seq model. Covariates in RNA-seq are often categorical (e.g.\n    treatment condition, sex, batch), but continuous factors are also possible\n    (e.g. time points, age). A linear model will contain terms to represent\n    the relationships between covariates and each sample. Each possible value\n    a factor can take is called a level (e.g. 'male' and 'female' are two\n    levels in the factor 'sex'). Factors can either be directly of interest\n    to the experiment (e.g. treatment condition) or not of interest (also\n    known as nuisance variables) (e.g. sex, batch). The purpose of covariates\n    is to explain the variance seen in samples.\n   \n     Confounding variable:\n     A confounding variable is a nuisance variable that is associated with\n    the factor of interest. Possible confounding factors should be controlled\n    for so they don't interfere with analysis. For example, if all knock-out\n    mice samples were harvested in the morning and all wild-type mice samples\n    were harvested in the afternoon, the time of sample collection would be a\n    confounding factor as the effects from sample collection time and from the\n    knock-out cannot be separated.\n   \n     Statistical power:\n     The ability to identify differentially expressed genes when there\n    really is a difference. This is partly dependent on variance and therefore\n    is affected by the number of replicates available and sequencing depth.", 
            "title": "Terminology"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#the-importance-of-replicates-to-estimate-variance", 
            "text": "When performing a differential gene expression analysis, we look at the\nexpression values of each gene and try to determine if the expression is\nsignificantly different in the different conditions (e.g. knock-out and\nwild-type). The ability to distinguish whether a gene is differentially\nexpressed is partly determined by the estimates of variability obtained by\nusing multiple observations in each condition.  Variability is present in two forms: technical variability and biological\nvariability.  Combined biological and technical variability is measured using biological \nreplicates. Biological variability is the main source of variability and is \ndue to natural variation in the population and within cells. This includes \ndifferent individuals having different levels of a particular gene and the \nstochastic nature of expression levels in different cells.  Technical variability is measured using technical replicates. Technical\nvariability is often very small compared to biological variability.  Usually\nthe question is whether an observed difference is greater than the total \nvariability (i.e. significant).  As combined variability is measured by \nbiological replicates technical replicates are only important if you need to\nknow the degree of biological variability or technical variability.  An\nexample of wanting technical variability would be method development. The\nmain source of technical variation comes from RNA processing and from \nlibrary prep. Variability from sequencing in different flow cells or different\nlanes is usually minimal. Generally, creating technical replicates from multiple\nlibrary preps is unnecessary for RNA-seq experiments.  The amount of variance between your biological replicates will affect the\noutcome of your analysis. Ideally, you aim to have minimal variability between\nsamples so you only measure the effect of the condition of interest. Too much\nvariability between samples can drown out the signal of truly differentially\nexpressed genes. Controlling for possible confounding factors between\nconditions is also important to prevent falsely attributing differential\nexpression to the condition of interest.  Strategies to minimise variation between samples and to control confounding\nvariables include:   choosing organisms from the same litter,  choosing organisms of the same sex if possible,  using a constant sample collection time,  having the same laboratory technician perform each library prep,  randomising samples to prevent a confounding batch effect if all samples\n   can't be processed at one time.   If variation between samples can not be removed it should be balanced between\nconditions of interest as much as possible, and carefully recorded to allow\nits effect to be measured and potentially removed during analysis.", 
            "title": "The importance of replicates to estimate variance"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#how-many-replicates-and-how-many-reads-do-i-need", 
            "text": "Two very common question asked are:   how many biological replicates do I need, and  what sequencing depth is needed for each sample   in order to have enough statistical power for my RNA-seq experiment?  These questions cannot be precisely answered without a pilot study. A small\namount of data (minimum of two biological replicates for each condition with\nat least 10M reads) can estimate the amount of biological variation, which\ndetermines how many biological replicates are required. Performing a pilot\nstudy is highly recommended to estimate statistical power and identify\npossible problems before investing more time and money into the project.  Scotty  is a web-based tool\nthat uses data generated from a pilot study to optimize a design for\nstatistical power. With a limited budget, one must balance sequence coverage\nand number of biological replicates. Scotty also has a cost estimate feature\nwhich returns the most powerful design within budget constraints.  As a general rule, the number of biological replicates should never be below 3.\nFor a basic RNA-seq differential expression experiment, 10M to 20M reads per\nsample is usually enough.  If similar data exists it can be helpful to check\nthe read counts for key genes of interest to estimate the required depth.  Biological variability is usually the largest effect limiting the power of \nRNA-seq analysis.  The most improvement in an experiment will usually be \nachieved by increasing the biological replication to improve estimation of \nthe biological variation.  It is often possible to design experiments where the analysis is done \nincrementally such that a pilot study is added to with an additional block of\nsamples or a pool of libraries is sequenced to additional depth. In these cases\ncare must be taken to balance the design in a manner that each stage is a\nvalid experiment in its own right.  This can allow a focused question to be \nanswered in the first stage, with an ability to either address issues or \nprogress to a second stage with additional questions.", 
            "title": "How many replicates and how many reads do I need?"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#sequencing-options-to-consider", 
            "text": "How much total RNA is needed: \nMany sequencing centres such as AGRF \nrecommend at least 250ng of total RNA for RNA sequencing. It is possible to go\nas low as 100ng of total RNA, but results are not guaranteed. The quality of\nRNA is also important when making libraries. A RNA Integrity Number (RIN) is a\nnumber from 1 (poor) to 10 (good) and can indicate how much degradation there\nis in the sample. A poor score can lead to over representation at the 3' end\nof the transcript and low yield. Samples with low RIN scores (below 8) are\nnot recommended for sequencing.  Care should also be taken to ensure RIN is\nconsistent between conditions to avoid confounding this technical effect with\nthe biological question.  Choosing an enrichment method: \nRibosomal RNA makes up  95% of total cellular RNA, so a preparation for\nRNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA.\nPoly-A enrichment is recommended for most standard RNA-seq experiments, but\nwill not provide information about microRNAs and other non-coding RNA species.\nIn general, ribo-depleted RNA-seq data will contain more noise, however, the\nprotocol is recommended if you have poor or variable quality of RNA as the 3\u2019\nbias of poly-A enrichment will be more pronounced with increased RNA\ndegradation. The amount of RNA needed for each method differs. For Poly-A\nenrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of\n200ng is recommended.  Choosing read type: \nFor basic differential expression analysis RNA-seq experiments, single-end\nsequencing is recommended to obtain gene transcript counts. In more advanced\nexperiments, paired-ends are useful for determining transcript structure and\ndiscovering splice variants.  Choosing strandedness:     With a non-directional (unstranded) protocol, there is no way to identify\nwhether a read originated from the coding strand or its reverse complement.\nNon-directional protocols allow mapping of a read to a genomic location, but\nnot the direction in which the RNA was transcribed. They are therefore used to\ncount transcripts for known genes, and are recommended for basic RNA-seq\nexperiments. Directional protocols (stranded) preserve strand information and\nare useful for novel transcript discovery.  Multiplexing: \nMultiplexing is an approach to sequence multiple samples in the same\nsequencing lane. By sequencing all samples in the same lane, multiplexing can\nalso minimise bias from lane effects.  Spike-in controls: \nRNA-seq spike-in controls are a set of synthetic RNAs of known concentration\nwhich act as negative or positive controls. These controls have been used for\nnormalisation and quality control, but recent work has shown that the amount\nof technical variability in their use dramatically reduces their utility.", 
            "title": "Sequencing options to consider"
        }, 
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#summary", 
            "text": "A good experimental design is vital for a successful experiment. If you're\n   planning to work with a data analyst or bioinformatician, include them in\n   the design stage.  Aim to minimise variability by identifying possible sources of variance in\n   your samples.  Biological replicates are important. The number of biological replicates\n   you should have should never be below 3. Technical replicates are often\n   unnecessary.  Pilot studies are highly recommended for identifying how many replicates\n   and how many reads you should have for enough statistical power in your\n   experiment.  For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded\n   sequencing at depths of 10M to 20M is probably what you want.", 
            "title": "Summary"
        }, 
        {
            "location": "/tutorials/assembly/assembly/", 
            "text": "Microbial de novo Assembly for Illumina Data\n\n\nIntroductory Tutorial\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism.\n\n\nWhat\u2019s not covered\n\n\nThis tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads.\nIt does not cover more complicated aspects of assembly such as:\n\n\n\n\nIncorporation of other raw data types (454 reads, Sanger reads)\n\n\nGap filling techniques for \u201cfinishing\u201d an assembly\n\n\nMeasuring the accuracy of assemblies\n\n\n\n\nBackground\n\n\nRead the \nbackground to the workshop here\n\n\nWhere is the data in this tutorial from?\n\n\nThe data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the \nNCBI Short Read Archive (SRA)\n (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396.\n\n\nWe will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced.\n\n\nWe will use software called \nVelvet\n (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above.\n\n\nThe protocol:\n\n\nWe are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the \nVelvet Optimiser\n (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly.\n\n\nFollow this \nlink for an overview of the protocol\n\n\nThe protocol in a nutshell:\n\n\nInput:\n Raw reads from sequencer run on microbial DNA sample.\n\n\nOutput:\n File of assembled scaffolds/contigs and associated information.\n\n\nPreparation\n\n\nLogin to Galaxy\n\n\n\n\nOpen a browser and go to a Galaxy server. (what is \nGalaxy\n?)\n\n\nYou can use a galaxy server of your own \nor\n\n\nGalaxy Tute\n at genome.edu.au\n\n\n\n\n\n\nRegister as a new user if you don\u2019t already have an account on that particular server\n\n\n\n\n\n\n\n  NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.\n\n\n\n\nImport the DNA read data for the tutorial.\n\n\nYou can do this in a few ways. If you're using \ngalaxy-tut.genome.edu.au\n:\n\n\n\n\nGo to \nShared Data -\n Published Histories\n and click on \"\nMicrobial_assembly_input_data\n\". Then click \n'Import History'\n at top right, wait for the history to be imported to your account, and then \n\u2018start using this history\u2019\n.\n\n\nThis will create a new Galaxy history in your account with all of the required data files.\n\n\nProceed to step 4.\n\n\n\n\nIf you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs.\n\n\n\n\nOn the Galaxy tools panel, click on \nGet data -\n Upload File\n.\n\n\nClick on the \nPaste/Fetch Data\n button.\n\n\nPaste the URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz\n into the text box. Change the type to \nfastqsanger\n (Not \nfastqcsanger\n).\n\n\nClick on the \nPaste/Fetch Data\n button again.\n\n\nPaste the URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz\n into the text box and change it's type to \nfastqsanger\n as well.\n\n\nRepeat the process for the last URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna\n , but make it's type \nfasta\n\n\nClick on the \nStart\n button. Once all of the uploads are at 100%, click on the \nClose\n button.\n\n\nWhen the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the \n icon to the top right of the file name in the right hand Galaxy panel (the history panel)\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n\n\nERR048396_1.fastq\n - forward reads in fastq format\n\n\nERR048396_2.fastq\n - reverse reads in fastq format\n\n\nillumina_adapters.fa\n - Illumina adapter sequences in fasta format\n\n\n\n\nView the fastq files\n\n\nClick on the \n  icon to the top right of each fastq file to view the first part of the file\nIf you\u2019re not familiar with the FASTQ format, click here for an overview\n\n\nNOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019\n\n\n\n\nSection 1: Quality control\n\n\nThe basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.)\n\n\nMore detailed description of FastQC quality analysis can be found here.\n\n\nMore detailed description of Trimmomatic read quality filtering can be found here.\n\n\nRun FastQC on both input read files\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS QC and manipulation \n FastQC: Comprehensive QC\n (down the bottom of this category) and run with these parameters:\n\n\n\"FASTQ reads\": \nERR048396_1.fastq\n\n\nUse default for other fields\n\n\n\n\n\n\nClick \nExecute\n\n\nNow repeat the above process on the second read file: \nERR048396_2.fastq\n\n\n\n\n\n\n Note: This may take a few minutes, depending on how busy Galaxy is.\n\n\n\n\nIt is important to do both read files as the quality can be very different between them.\n\n\nFigure 1: Screenshot of FastQC interface in Galaxy\n\n\n\n\nExamine the FastQC output\n\n\nYou should have two output objects from the first step:\n\n\n\n\nFastQC_ERR048396_1.fastqc.html\n\n\nFastQC_ERR048396_2.fastqc.html\n\n\n\n\nThese are a html outputs which show the results of all of the tests FastQC performed on the read files.\n\n\n\n\nClick on the \n icon of each of these objects in turn to see the FastQC output.\n\n\n\n\nThe main parts of the output to evaluate are:\n\n\n\n\nBasic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%.\n\n\nPer base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.\n\n\n\n\nFigure 2: Screenshot of FastQC output in Galaxy\n\n\n\n\nQuality trim the reads using Trimmomatic.\n\n\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS QC and manipulation \n Trimmomatic\n and run with these parameters (only the non-default selections are listed here):\n\n\n\n\n\"Input FASTQ file (R1/first of pair)\": \nERR048396_1.fastq\n\n\n\"Input FASTQ file (R2/second of pair)\": \nERR048396_2.fastq\n\n\n\"Perform initial ILLUMINACLIP step?\": \nYes\n\n\n\"Adapter sequences to use\": \nTruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq)\n\n\n\"How accurate ... read alignment\": \n40\n\n\n\"How accurate ... against a read\": \n15\n\n\nWe will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations.\n\n\nClick \nInsert Trimmomatic Operation\n\n\nAdd \nCut bases ... (LEADING)\n\n\n\"Minimum quality required to keep a base\": \n15\n\n\n\n\n\n\nRepeat the \nInsert Trimmomatic Operation\n for:\n\n\nTrim trailing bases, minimum quality: \n15\n\n\nMinimum length read: \n35\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nExecute\n\n      \n\n\n\n\n\n\nFigure 3: Screenshot of Trimmomatic inputs in Galaxy\n\n\n\n\nExamine the Trimmomatic output FastQ files.\n\n\nYou should have 4 new objects in your history from the output of Trimmomatic:\n\n\n\n\nTrimmomatic on data 2 and data 1 (R1 Paired)\n\n\nTrimmomatic on data 2 and data 1 (R1 Unpaired)\n\n\nTrimmomatic on data 2 and data 1 (R2 Paired)\n\n\nTrimmomatic on data 2 and data 1 (R2 Unpaired)\n\n\n\n\nClick on the \n on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.\n\n\n\n\nSection 2: Assemble reads into contigs with Velvet and the Velvet Optimiser\n\n\nThe aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser.\n\n\nWe will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process.\n\n\nClick here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser\n\n\nDe novo assembly of the reads into contigs\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS: Assembly -\n Velvet Optimiser\n and run with these parameters (only the non-default selections are listed here):\n\n\n\"Start k-mer value\": \n55\n\n\n\"End k-mer value\": \n69\n\n\nIn the input files section:\n\n\n\"Select first set of reads\": \nTrimmomatic on data 2 and data 1 (R1 paired)\n\n\n\"Select second set of reads\": \nTrimmomatic on data 2 and data 1 (R2 paired)\n\n\n\n\n\n\nClick the \nInsert Input Files\n button and add the following:\n\n\n\"Single or paired end reads\": \nSingle\n\n\n\"Select the reads\": \nTrimmomatic on data 2 and data 1 (R1 unpaired)\n\n\n\n\n\n\nRepeat the above process to add the other unpaired read set \nTrimmomatic on data 2 and data 1 (R2 unpaired)\n as well.\n\n\n\n\n\n\nClick \nExecute\n.\n\n\n\n\nFigure 4: Screenshot of Velvet Optimiser inputs in Galaxy\n\n\n\n\nExamine assembly output\n\n\nOnce step 1 is complete, you should now have 2 new objects in your history:\n\n \nVelvetOptimiser on data 9, data 7, and others: Contigs\n\n\n \nVelvetOptimiser on data 9, data 7, and others: Contig Stats\n\n\nClick on the \n icon of the various objects.\n\n\n\n\n\n\nContigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.)\n\n\n\n\n\n\nContig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)\n\n\n\n\n\n\nFigure 5: Screenshot of assembled contigs (a) and contig stats (b)\n\n\na\n\n\n\n\nb\n\n\n\n\nCalculate some statistics on the assembled contigs\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nFASTA Manipulation -\n Fasta Statistics\n and run with these parameters:\n\n\n\"Fasta or multifasta file\": \nVelvet Optimiser ... Contigs\n\n\n\n\n\n\nClick \nExecute\n\n\nExamine the Fasta Stats output\n\n\n\n\nYou should now have one more object in your history: \nFasta Statistics on data 10: Fasta summary stats\n\n\nClick on the \n icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.\n\n\n\n\nSection 3: Extension.\n\n\nExamine the contig coverage depth and blast a high coverage contig against a protein database.\n\n\nExamine the contig coverage depth.\n\n\nLook at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the \n icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1).\n\n\n\n\nWe can easily filter out these short contigs from this information list by using the \nFilter and Sort -\n Filter tool.\n\n\nSet the following:\n\n\n\"Filter\": \nVelvet Optimiser on data 8, data 7 and others: Contig stats\n\n\n\"With the following condition\": \nc2 \n 100\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThe new data object in the history is called: \nFilter on data 11\n.\n\n\nClick on its \n icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32.  There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are \n400! These contigs will appear in the genome more than 20 times!\n\n\nLets have a look at one of these contigs and see if we can find out what it is.\n\n\nExtract a single sequence from the contigs file.\n\n\nNote the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily.\n\n\nTo do this we will use the tool:\n\n\n\n\nFasta manipulation -\n Fasta Extract Sequence\n\n\nSet the following:\n\n\n\"Fasta or multifasta file\": \nVelvet Optimiser ... : Contigs\n\n\n\"Sequence ID (or partial): \nNODE_1_...\n (for example)\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThe new data object in the history is called: \nFasta Extract Sequence on data 10: Fasta\n.\n\n\nClick on its \n icon to view it. It is a single sequence in fasta format.\n\n\nBlast sequence to determine what it contains.\n\n\nWe want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this:\n\n\n\n\nBring up the sequence of the contig into the main window of the browser by clicking on the \n icon if it isn\u2019t already.\n\n\nSelect the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser.\n\n\nCopy the selected sequence to the clipboard.\n\n\nOpen a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi\n\n\nUnder the BASIC BLAST section, click \u201cblastx\u201d.\n\n\nPaste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s).\n\n\nChange the Genetic code to: Bacteria and Archaea (11)\n\n\nClick the button labelled: BLAST\n\n\n\n\nAfter a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.\n\n\nFigure 6: Screenshot of the output from the NCBI Blast website", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#microbial-de-novo-assembly-for-illumina-data", 
            "text": "", 
            "title": "Microbial de novo Assembly for Illumina Data"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#introductory-tutorial", 
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)", 
            "title": "Introductory Tutorial"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#tutorial-overview", 
            "text": "In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism.  What\u2019s not covered  This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads.\nIt does not cover more complicated aspects of assembly such as:   Incorporation of other raw data types (454 reads, Sanger reads)  Gap filling techniques for \u201cfinishing\u201d an assembly  Measuring the accuracy of assemblies", 
            "title": "Tutorial Overview"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#background", 
            "text": "Read the  background to the workshop here  Where is the data in this tutorial from?  The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the  NCBI Short Read Archive (SRA)  (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396.  We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced.  We will use software called  Velvet  (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above.  The protocol:  We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the  Velvet Optimiser  (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly.  Follow this  link for an overview of the protocol  The protocol in a nutshell:  Input:  Raw reads from sequencer run on microbial DNA sample.  Output:  File of assembled scaffolds/contigs and associated information.", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#preparation", 
            "text": "", 
            "title": "Preparation"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#login-to-galaxy", 
            "text": "Open a browser and go to a Galaxy server. (what is  Galaxy ?)  You can use a galaxy server of your own  or  Galaxy Tute  at genome.edu.au    Register as a new user if you don\u2019t already have an account on that particular server    \n  NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.", 
            "title": "Login to Galaxy"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#import-the-dna-read-data-for-the-tutorial", 
            "text": "You can do this in a few ways. If you're using  galaxy-tut.genome.edu.au :   Go to  Shared Data -  Published Histories  and click on \" Microbial_assembly_input_data \". Then click  'Import History'  at top right, wait for the history to be imported to your account, and then  \u2018start using this history\u2019 .  This will create a new Galaxy history in your account with all of the required data files.  Proceed to step 4.   If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs.   On the Galaxy tools panel, click on  Get data -  Upload File .  Click on the  Paste/Fetch Data  button.  Paste the URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz  into the text box. Change the type to  fastqsanger  (Not  fastqcsanger ).  Click on the  Paste/Fetch Data  button again.  Paste the URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz  into the text box and change it's type to  fastqsanger  as well.  Repeat the process for the last URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna  , but make it's type  fasta  Click on the  Start  button. Once all of the uploads are at 100%, click on the  Close  button.  When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the   icon to the top right of the file name in the right hand Galaxy panel (the history panel)   You should now have the following files in your Galaxy history:   ERR048396_1.fastq  - forward reads in fastq format  ERR048396_2.fastq  - reverse reads in fastq format  illumina_adapters.fa  - Illumina adapter sequences in fasta format", 
            "title": "Import the DNA read data for the tutorial."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#view-the-fastq-files", 
            "text": "Click on the    icon to the top right of each fastq file to view the first part of the file\nIf you\u2019re not familiar with the FASTQ format, click here for an overview  NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019", 
            "title": "View the fastq files"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#section-1-quality-control", 
            "text": "The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.)  More detailed description of FastQC quality analysis can be found here.  More detailed description of Trimmomatic read quality filtering can be found here.", 
            "title": "Section 1: Quality control"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#run-fastqc-on-both-input-read-files", 
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS QC and manipulation   FastQC: Comprehensive QC  (down the bottom of this category) and run with these parameters:  \"FASTQ reads\":  ERR048396_1.fastq  Use default for other fields    Click  Execute  Now repeat the above process on the second read file:  ERR048396_2.fastq     Note: This may take a few minutes, depending on how busy Galaxy is.   It is important to do both read files as the quality can be very different between them.", 
            "title": "Run FastQC on both input read files"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-1-screenshot-of-fastqc-interface-in-galaxy", 
            "text": "", 
            "title": "Figure 1: Screenshot of FastQC interface in Galaxy"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#examine-the-fastqc-output", 
            "text": "You should have two output objects from the first step:   FastQC_ERR048396_1.fastqc.html  FastQC_ERR048396_2.fastqc.html   These are a html outputs which show the results of all of the tests FastQC performed on the read files.   Click on the   icon of each of these objects in turn to see the FastQC output.   The main parts of the output to evaluate are:   Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%.  Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.", 
            "title": "Examine the FastQC output"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-2-screenshot-of-fastqc-output-in-galaxy", 
            "text": "", 
            "title": "Figure 2: Screenshot of FastQC output in Galaxy"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#quality-trim-the-reads-using-trimmomatic", 
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS QC and manipulation   Trimmomatic  and run with these parameters (only the non-default selections are listed here):   \"Input FASTQ file (R1/first of pair)\":  ERR048396_1.fastq  \"Input FASTQ file (R2/second of pair)\":  ERR048396_2.fastq  \"Perform initial ILLUMINACLIP step?\":  Yes  \"Adapter sequences to use\":  TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq)  \"How accurate ... read alignment\":  40  \"How accurate ... against a read\":  15  We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations.  Click  Insert Trimmomatic Operation  Add  Cut bases ... (LEADING)  \"Minimum quality required to keep a base\":  15    Repeat the  Insert Trimmomatic Operation  for:  Trim trailing bases, minimum quality:  15  Minimum length read:  35       Click  Execute", 
            "title": "Quality trim the reads using Trimmomatic."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy", 
            "text": "", 
            "title": "Figure 3: Screenshot of Trimmomatic inputs in Galaxy"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#examine-the-trimmomatic-output-fastq-files", 
            "text": "You should have 4 new objects in your history from the output of Trimmomatic:   Trimmomatic on data 2 and data 1 (R1 Paired)  Trimmomatic on data 2 and data 1 (R1 Unpaired)  Trimmomatic on data 2 and data 1 (R2 Paired)  Trimmomatic on data 2 and data 1 (R2 Unpaired)   Click on the   on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.", 
            "title": "Examine the Trimmomatic output FastQ files."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser", 
            "text": "The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser.  We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process.  Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser", 
            "title": "Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#de-novo-assembly-of-the-reads-into-contigs", 
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS: Assembly -  Velvet Optimiser  and run with these parameters (only the non-default selections are listed here):  \"Start k-mer value\":  55  \"End k-mer value\":  69  In the input files section:  \"Select first set of reads\":  Trimmomatic on data 2 and data 1 (R1 paired)  \"Select second set of reads\":  Trimmomatic on data 2 and data 1 (R2 paired)    Click the  Insert Input Files  button and add the following:  \"Single or paired end reads\":  Single  \"Select the reads\":  Trimmomatic on data 2 and data 1 (R1 unpaired)    Repeat the above process to add the other unpaired read set  Trimmomatic on data 2 and data 1 (R2 unpaired)  as well.    Click  Execute .", 
            "title": "De novo assembly of the reads into contigs"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy", 
            "text": "", 
            "title": "Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#examine-assembly-output", 
            "text": "Once step 1 is complete, you should now have 2 new objects in your history:   VelvetOptimiser on data 9, data 7, and others: Contigs    VelvetOptimiser on data 9, data 7, and others: Contig Stats  Click on the   icon of the various objects.    Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.)    Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)", 
            "title": "Examine assembly output"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b", 
            "text": "", 
            "title": "Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#a", 
            "text": "", 
            "title": "a"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#b", 
            "text": "", 
            "title": "b"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#calculate-some-statistics-on-the-assembled-contigs", 
            "text": "From the tools menu in the left hand panel of Galaxy, select  FASTA Manipulation -  Fasta Statistics  and run with these parameters:  \"Fasta or multifasta file\":  Velvet Optimiser ... Contigs    Click  Execute  Examine the Fasta Stats output   You should now have one more object in your history:  Fasta Statistics on data 10: Fasta summary stats  Click on the   icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.", 
            "title": "Calculate some statistics on the assembled contigs"
        }, 
        {
            "location": "/tutorials/assembly/assembly/#section-3-extension", 
            "text": "Examine the contig coverage depth and blast a high coverage contig against a protein database.", 
            "title": "Section 3: Extension."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#examine-the-contig-coverage-depth", 
            "text": "Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the   icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1).   We can easily filter out these short contigs from this information list by using the  Filter and Sort -  Filter tool.  Set the following:  \"Filter\":  Velvet Optimiser on data 8, data 7 and others: Contig stats  \"With the following condition\":  c2   100    Click  Execute   The new data object in the history is called:  Filter on data 11 .  Click on its   icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32.  There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are  400! These contigs will appear in the genome more than 20 times!  Lets have a look at one of these contigs and see if we can find out what it is.", 
            "title": "Examine the contig coverage depth."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#extract-a-single-sequence-from-the-contigs-file", 
            "text": "Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily.  To do this we will use the tool:   Fasta manipulation -  Fasta Extract Sequence  Set the following:  \"Fasta or multifasta file\":  Velvet Optimiser ... : Contigs  \"Sequence ID (or partial):  NODE_1_...  (for example)    Click  Execute   The new data object in the history is called:  Fasta Extract Sequence on data 10: Fasta .  Click on its   icon to view it. It is a single sequence in fasta format.", 
            "title": "Extract a single sequence from the contigs file."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#blast-sequence-to-determine-what-it-contains", 
            "text": "We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this:   Bring up the sequence of the contig into the main window of the browser by clicking on the   icon if it isn\u2019t already.  Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser.  Copy the selected sequence to the clipboard.  Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi  Under the BASIC BLAST section, click \u201cblastx\u201d.  Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s).  Change the Genetic code to: Bacteria and Archaea (11)  Click the button labelled: BLAST   After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.", 
            "title": "Blast sequence to determine what it contains."
        }, 
        {
            "location": "/tutorials/assembly/assembly/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website", 
            "text": "", 
            "title": "Figure 6: Screenshot of the output from the NCBI Blast website"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/", 
            "text": "De novo genome assembly using Velvet\n\n\nBackground\n\n\nIntroduction to de novo assembly\n\n\nDNA sequence assembly from short fragments (\n 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated.\n\n\nThe sequence assembly issue was neatly summed up by the following quote:\n\n\n\"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces.  The book may have many repeated paragraphs,  and some shreds may be modified to have typos.  Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\"\n \u2013 Wikipedia: Sequence assembly.\n\n\nAn addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known.\n\n\nThis tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino  et al. 2008, 2009) and the Velvet Optimiser (Gladman \n Seemann, 2009) from within the Galaxy workflow management system.\n\n\n\n\nThe Galaxy workflow platform\n\n\nWhat is Galaxy?\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nFigure 1: The Galaxy interface\n\n\nTools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\n\n\nDe novo assembly with Velvet and the Velvet Optimiser.\n\n\nVelvet\n\n\nVelvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.\n\n\nde Bruijn graphs\n\n\nA de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence.\n\n\nFigure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0.\n\n\n\n\nFrom: https://cameroncounts.wordpress.com/2015/02/28/1247/\n\n\nVelvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters.\n\n\nVelvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.\n\n\nThe Velvet algorithm\n\n\nStep 1: Hashing the reads.\n\n\n\n\nVelvet breaks up each read into k-mers of length k.\n\n\nA k-mer is a k length subsequence of the read.\n\n\nA 36 base pair long read would have 6 different 31-mers.\n\n\nThe k-mers and their reverse complements are added to a hash table to categorize them.\n\n\nEach k-mer is stored once but the number of times it appears is also recorded.\n\n\nThis step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.\n\n\n\n\nStep 2: Constructing the de Bruijn graph.\n\n\n\n\nVelvet adds the k-mers one-by-one to the graph.\n\n\nAdjacent k-mers overlap by k-1 nucleotides.\n\n\nA k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node.\n\n\nEach node stores the average number of times its k-mers appear in the hash table.\n\n\nFigure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5.\n\n\nDifferent sequences can be read off the graph by following a different path through it. (Figure 3)\n\n\n\n\nFigure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph.\n \n(Base figure Zerbino et al 2008.)\n\n\n\n\nStep 3: Simplification of the graph.\n\n\n\n\nChain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes.\n\n\nTip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is \n 2 x k or their average k-mer depth is much less than the continuing path.\n\n\nBubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences.\n\n\nVelvet compares the paths using dynamic programming.\n\n\nIf they are highly similar, the paths are merged.\n\n\nError removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes.\n\n\nPaired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs.\n\n\n\n\nFigure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph.\n  \n(Base figure Zerbino et al 2008.)\n\n\n\n\nStep 4: Read off the contigs.\n\n\n\n\nFollow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs.\n\n\nWhere there is an ambiguous divergence/convergence, stop the current contig and start a new one.\n\n\n\n\nK-mer size and coverage cutoff values\n\n\nThe size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down.\n\n\nThe coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data.\n\n\nEach dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.\n\n\nVelvet Optimiser\n\n\nThe Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies.  It uses different optimisation functions for k and c and these can be user controlled.\n\n\nIt requires the user to input a range of k values to search (to cut down on running time).\n\n\nReferences\n\n\nhttp://en.wikipedia.org/wiki/Sequence_assembly\n\n\nZerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829\n\n\nZerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407.\n\n\nGladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.", 
            "title": "Method Background"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#de-novo-genome-assembly-using-velvet", 
            "text": "", 
            "title": "De novo genome assembly using Velvet"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#background", 
            "text": "", 
            "title": "Background"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#introduction-to-de-novo-assembly", 
            "text": "DNA sequence assembly from short fragments (  200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated.  The sequence assembly issue was neatly summed up by the following quote:  \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces.  The book may have many repeated paragraphs,  and some shreds may be modified to have typos.  Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\"  \u2013 Wikipedia: Sequence assembly.  An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known.  This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino  et al. 2008, 2009) and the Velvet Optimiser (Gladman   Seemann, 2009) from within the Galaxy workflow management system.", 
            "title": "Introduction to de novo assembly"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#the-galaxy-workflow-platform", 
            "text": "", 
            "title": "The Galaxy workflow platform"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#what-is-galaxy", 
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here  Figure 1: The Galaxy interface  Tools on the left, data in the middle, analysis workflow on the right.", 
            "title": "What is Galaxy?"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#de-novo-assembly-with-velvet-and-the-velvet-optimiser", 
            "text": "", 
            "title": "De novo assembly with Velvet and the Velvet Optimiser."
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#velvet", 
            "text": "Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.", 
            "title": "Velvet"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#de-bruijn-graphs", 
            "text": "A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence.  Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0.   From: https://cameroncounts.wordpress.com/2015/02/28/1247/  Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters.  Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.", 
            "title": "de Bruijn graphs"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#the-velvet-algorithm", 
            "text": "", 
            "title": "The Velvet algorithm"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#step-1-hashing-the-reads", 
            "text": "Velvet breaks up each read into k-mers of length k.  A k-mer is a k length subsequence of the read.  A 36 base pair long read would have 6 different 31-mers.  The k-mers and their reverse complements are added to a hash table to categorize them.  Each k-mer is stored once but the number of times it appears is also recorded.  This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.", 
            "title": "Step 1: Hashing the reads."
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#step-2-constructing-the-de-bruijn-graph", 
            "text": "Velvet adds the k-mers one-by-one to the graph.  Adjacent k-mers overlap by k-1 nucleotides.  A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node.  Each node stores the average number of times its k-mers appear in the hash table.  Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5.  Different sequences can be read off the graph by following a different path through it. (Figure 3)   Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph.   (Base figure Zerbino et al 2008.)", 
            "title": "Step 2: Constructing the de Bruijn graph."
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#step-3-simplification-of-the-graph", 
            "text": "Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes.  Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is   2 x k or their average k-mer depth is much less than the continuing path.  Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences.  Velvet compares the paths using dynamic programming.  If they are highly similar, the paths are merged.  Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes.  Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs.   Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph.    (Base figure Zerbino et al 2008.)", 
            "title": "Step 3: Simplification of the graph."
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#step-4-read-off-the-contigs", 
            "text": "Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs.  Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.", 
            "title": "Step 4: Read off the contigs."
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#k-mer-size-and-coverage-cutoff-values", 
            "text": "The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down.  The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data.  Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.", 
            "title": "K-mer size and coverage cutoff values"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#velvet-optimiser", 
            "text": "The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies.  It uses different optimisation functions for k and c and these can be user controlled.  It requires the user to input a range of k values to search (to cut down on running time).", 
            "title": "Velvet Optimiser"
        }, 
        {
            "location": "/tutorials/assembly/assembly-background/#references", 
            "text": "http://en.wikipedia.org/wiki/Sequence_assembly  Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829  Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407.  Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.", 
            "title": "References"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/", 
            "text": "Identifying proteins from mass spec data\n\n\nOverview\n\n\nThis tutorial describes how to identify a list of proteins from tandem mass spectrometry data. \n\n\nAnalyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained \nde novo\n with this method.\n\n\nThe data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer.  The sample itself corresponds to a purified organelle from Mouse cells.  \n\n\nThe aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle.\n\n\nThis tutorial uses free software including;\n\n\n\n\nThe \nX!Tandem\n search engine\n\n\nThe \nTrans Proteomic Pipeline\n (TPP) for post-search validation\n\n\nThe \nProtk\n tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier\n\n\nThe \nGalaxy\n platform to bring all these tools together\n\n\n\n\nLogin to Galaxy\n\n\n\n\n\n\nOpen a browser and go to a Galaxy server.\n\n\n\n\nYou can use a galaxy server of your own \nor\n\n\nGalaxy Tute\n at genome.edu.au\n\n\n\n\n\n\n Use a supported browser. Firefox/Safari/Chrome all work well\n\n\nIf you use your own galaxy server you will need to make sure you have the \nprotk proteomics tools\n installed.\n\n\n\n\n\n\n\n\nRegister as a new user if you don\u2019t already have an account on that particular server\n\n\n\n\n\n\nImport mass spec data\n\n\n\n\nCreate a new history\n in Galaxy and name it \"Organelle Tutorial\"\n\n\n\n\nDownload datasets\n using the Galaxy uploader tool. \n\n\nOpen this tool by clicking the button as shown below\n\n\n\n\nAfter opening the tool select \nPaste/Fetch data\n and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML\n\n\n\nAfter the download is finished you should have a single item in your history.\n\n\n\n\n\n\nRename\n the history item by clicking the pencil icon beside it to \"Edit attributes\".\n\n\n\n\nThis should bring up a dialog box where you can edit the name.  \n\n\nChange the name by removing everything up to the last forward slash \"/\"\n\n\nYour item should then be named \nOrganelleSample.mzML\n\n\nDont forget to click \"Save\"\n\n\n\n\n\n\nBasic properties of the data\n\n\nFormat:\n\n\nMass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about \nmass spectrometry data formats here\n \n\n\n\n\n\n\n 1) What format is the OrganelleSample.mzML file?\n\n\n\n\n\n\n(function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)\n\n\n\n\n\n\nHint\n\n\n\n\nTry clicking the title bar on the data in your galaxy history.  This will toggle display of some additional information about the data.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmzML\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nMS versus MS/MS:\n\n\nA key feature of \nTandem\n mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide.  These two types of measurements are called \nMS\n and \nMS/MS\n spectra respectively.  The following schematic shows how an \nMS/MS\n scan results from the fragmentation of a selected product ion.  Often multiple \nMS/MS\n spectra are obtained for each \nMS\n scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed.\n\n\n\n\nNumber of spectra:\n\n\nClick the eye icon on the history item to view the \nmzML\n file as text.  The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of \nMS\n and \nMS/MS\n spectra in the file.\n\n\nNow try searching for the text \"MS1 spectrum\" in the page using your web browser's search function.  Looking closely you should see that this text appears once for every \nMS1\n spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description).  The file is large though and the browser can only see the first megabyte of it. \n\n\nNow search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\".  There are a total of 24941 spectra in the entire file including both \nMS\n and \nMS/MS\n spectra.\n\n\n\n\n\n\n 2) How many \nMS\n spectra are there in this dataset?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTo answer this question you will need to use the \nSelect\n tool from the \nFilter and Sort\n submenu to select lines matching the text \"MS1 spectrum\" in the whole file.  Then use the \nLine/Word/Character count\n tool from the \nText Manipulation\n submenu to count the number of lines returned by running the \nSelect\n tool.\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nThe text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable5\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink5\").text(\"More\");\n            } else {\n                $(\"#showablelink5\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3142\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\nIn the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. \nSee the instructions here\n\n\n\n\nPrior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching \nMS\n peaks against the masses of whole peptides via \nPeptide Mass Fingerprinting\n.  This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the \nMS/MS\n spectra.  In this tutorial only the \nMS/MS\n spectra will be used.\n\n\n\n\n\n\n 3) How many \nMS/MS\n spectra are there in this dataset?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the fact that the file contains a total of 24941 spectra with your answer to the previous question about \nMS\n spectra.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n21799\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nAlternate data formats\n\n\nAnother format you are likely to encounter for tandem mass spectrometry data is \nMascot Generic Format\n or \nmgf\n.  Mascot Generic Format (\nmgf\n) is the data file format preferred by the \nMascot\n search engine.  It is a text based format is much easier to read by hand than the \nmzML\n file.  Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of (\nmz\n,\nintensity\n) pairs.  Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. \n\n\n\n\n\n\nDownload the Organelle Sample data in \nmgf\n format\n\n\nUse the \nPaste/Fetch data\n tool again and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf\n\n\n\n\n\n\n\nInspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file.\n\n\n\n\n\n\n\n\n\n\n 4) How many spectra are there in this dataset and what type of spectra do you think they are?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file.\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nConsider your answers to questions 3 and 4\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable12\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink12\").text(\"More\");\n            } else {\n                $(\"#showablelink12\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n21799\n\n\nMS/MS\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nObtain a Search Database\n\n\nSetting up a search database is a critical step.  For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues;\n\n\n\n\nDatabase size\n\n\nWhether to include decoys\n\n\nWhat types of variants to include if any\n\n\nHow to format your database identifiers\n\n\n\n\nMore details are provided \nhere\n.\n\n\n\n\n\n\nDownload a database of Mouse proteins in \nfasta\n format\n\n\nUse the \nPaste/Fetch data\n tool again and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta\n\n\n\n\n\n\n\nInspect the first few items in the database in Galaxy.  The file is in \nFasta\n format which means that each entry has a single description line that starts with a \"\n\" followed by a unique identifier and then some general descriptive information.  The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this;\n\n\nsp|Q9CQV8|1433B_MOUSE\n\n\n\nThe database also includes decoy sequences, appended at the end.  They have identifiers like this;\n\n\ndecoy_rp75404\n\n\n\n\n\n\n\n\n\n\n\n 5) What is the ratio of decoys to non-decoys in the database?\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nDecoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title).\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n1:1\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nRun a search using X!Tandem\n\n\nA large number of search engines now exist for proteomics data.  This exercise uses \nX!Tandem\n which is one of the fastest and most widely used. Other search engines include \nOMSSA\n, \nMS-GF+\n and \nMascot\n.\n\n\n\n\nSelect the \nX!Tandem Search\n tool\n\n\nEnter parameters as shown in the table below (leave all others at their defaults)\n\n\nClick Execute\n\n\n\n\n\n\n\n\n\n\nParameter Name\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nUploaded Fasta File\n\n\nUniprotMouseD_20140716.fasta\n\n\n\n\n\n\nMSMS File\n\n\nOrganelleSample.mgf\n\n\n\n\n\n\nVariable Modifications\n\n\nOxidation M\n\n\n\n\n\n\nFixed Modifications\n\n\nCarbamidomethyl C\n\n\n\n\n\n\nMissed Cleavages Allowed\n\n\n2\n\n\n\n\n\n\nEnzyme\n\n\nTrypsin\n\n\n\n\n\n\nFragment Ion Tolerance\n\n\n0.5\n\n\n\n\n\n\nPrecursor Ion Tolerance\n\n\n10 ppm\n\n\n\n\n\n\n\n\nThe search should run for about 5-10 minutes and will produce an output file in \nX!Tandem\n xml format. A much more useful format is \npepXML\n so the next step in the analysis will be to run a tool to convert from tandem to pepXML.\n\n\n\n\nSelect the \nTandem to pepXML\n tool\n\n\nSelect the output from the previous step as input and click \nExecute\n\n\n\n\nWhile the search is running, read some background theory on \nhow search engine's work\n.\n\n\nConvert Results to tabular format\n\n\nAlthough the \npepXML\n format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert \npepXML\n into tabular (tab separated) format, which is much easier to read.  Tabular format also has the advantage that it can be downloaded and opened using many other programs including \nExcel\n and \nR\n.\n\n\n\n\nSelect the \npepXML to Table\n tool\n\n\nSelect the \npepXML\n file produced in the previous step as input and click \nExecute\n\n\n\n\nTo get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest.  Explore the column assignments in your tabular file by clicking on its title in your galaxy history.  This will show extra details about the item, including a handy preview where column numbers are displayed.\n\n\n\n\n\n\n 6) In what column number is the \nassumed_charge\n of peptides in the \npepXML\n tabular file?\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nSort tabular outputs\n\n\nExamine the tabular output file from the previous step.  It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results.  These decoys should tend to have quite poor scores compared with real hits.  The \nraw score\n for \nX!Tandem\n searches is an \nE-value\n.  To push these decoys to the bottom of the list we can sort the data by \nraw score\n. \n\n\n\n\nSelect the \nSort\n data tool from the \nFilter and Sort\n menu in the left pane of Galaxy\n\n\nChoose to sort on \nraw_score\n.  This is column c10 \n\n\nSelect \nAscending order\n for the sort direction (small E-values are good) and click \nExecute\n\n\n\n\nBrowse the resulting dataset.  The top of the file should now have very few decoys.\n\n\nConvert raw scores to probabilities\n\n\nRaw \nX!Tandem\n scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct.  A number of tools exist for this, and in this tutorial we use \nPeptide Prophet\n, which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by \nPeptide Prophet\n can be used to set a threshold for acceptance.  For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to \nPeptide Prophet\n is \nPercolator\n.\n\n\nIf you're curious about how \nPeptide Prophet\n works, take a look at \nthis explainer\n, or \nthe original paper\n\n\n\n\nSelect the \nPeptide Prophet\n tool\n\n\nSelect the \nX!Tandem\n output in \npepXML\n format generated earlier as input\n\n\nCheck the box that says \nUse decoys to pin down the negative distribution\n.\n\n\nConvert the resulting \npepXML\n file to tabular using the \nPepXML to Table\n tool\n\n\n\n\nTake a look at the resulting tabular file.  Note that this time the \npeptideprophet_prob\n column is populated and contains numbers between 0 and 1.\n\n\n\n\n\n\n 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nFilter\n tool from the \nFilter and Sort\n submenu. Also remember that Peptide Prophet probability is given in a column called \npeptideprophet_prob\n.  The syntax for \"greater than or equal to\" in the \nFilter\n tool is \n=.\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nUse this text in \nmatch with condition\n field of the \nFilter and Sort\n tool.\n\n\nc11\n=0.95\n\n\n\nTo answer the second question use the \nSelect\n tool on the filtered table to select lines matching \"decoy_\"\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable21\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink21\").text(\"More\");\n            } else {\n                $(\"#showablelink21\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3808\n\n\n21\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\n 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM)\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nConsider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3)\n\n\n\n\n\n\nMore\n \n- and here to show more\n\n\n\n\nTo take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect.\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable25\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink25\").text(\"More\");\n            } else {\n                $(\"#showablelink25\").text(\"Less\");\n            }\n        });\n    });\n    \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n17.27%\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nPerform Protein Inference\n\n\nUp to this point we have looked at peptide to spectrum matches \nPSMs\n.  Each of the peptides observed will have come from a protein sequence in the \nfasta\n file that we used as a database, and this protein is recorded along with the \nPSM\n itself in all of the result tables we've viewed so far.  Unfortunately, the process of inferring the existence of proteins based on these \nPSMs\n is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one \nPSM\n.\n\n\nThe \nProtein Prophet\n tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins.\n\n\n\n\nSelect the \nProtein Prophet\n tool\n\n\nChoose the \npepXML\n formatted output from \nPeptide Prophet\n as input and click \nExecute\n\n\nConvert the resulting \nprotXML\n to tabular using the \nprotXML to Table\n tool.\n\n\n\n\n\n\n\n\n 9) How many proteins are there with protein prophet probability greater than or equal to 0.99?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nFilter on column 6 \nprotein_probability\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n601\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink29\").click(function(e){\n            e.preventDefault();\n            $(\"#showable29\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\nIf you have time, read over \nthese notes\n on the Protein Prophet output.  Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.\n\n\nFunctional enrichment analysis\n\n\nThis step will allow you to discover the identity of the Organelle that was used to create the sample.  \n\n\nWe use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom).\n\n\n\n\nStart with unfiltered tabular protein prophet results\n\n\nUse the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains \nprotein_name\n). \n\n\nConvert the \"pipes\" that separate parts of the protein_name into separate columns using the \nConvert delimiters to TAB\n tool in the \nText manipulation\n submenu of Galaxy. This should result in a file with 3 columns\n\n\nUse the \nCut columns\n tool again to cut the second column from this dataset\n\n\nDownload this file to your desktop and rename it to \norganelle.txt\n\n\nOpen the \nGOrilla\n web page in your web browser\n\n\nSelect \nOrganism\n as Mouse\n\n\nUpload the \norganelle.txt\n file as a ranged gene list\n\n\nChoose \nComponent\n for the ontology\n\n\nSubmit\n\n\n\n\n\n\n\n\n 9) What \nintracellular\n organelle was enriched in the sample?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIgnore terms relating to \nexosomes\n\n\nIn the resulting output look to the most enriched and most specific GO terms. \n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n        });\n    });\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nMitochondria\n\n\n\n\n\n\n\n\n\n\n\n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#identifying-proteins-from-mass-spec-data", 
            "text": "", 
            "title": "Identifying proteins from mass spec data"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#overview", 
            "text": "This tutorial describes how to identify a list of proteins from tandem mass spectrometry data.   Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained  de novo  with this method.  The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer.  The sample itself corresponds to a purified organelle from Mouse cells.    The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle.  This tutorial uses free software including;   The  X!Tandem  search engine  The  Trans Proteomic Pipeline  (TPP) for post-search validation  The  Protk  tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier  The  Galaxy  platform to bring all these tools together", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#login-to-galaxy", 
            "text": "Open a browser and go to a Galaxy server.   You can use a galaxy server of your own  or  Galaxy Tute  at genome.edu.au     Use a supported browser. Firefox/Safari/Chrome all work well  If you use your own galaxy server you will need to make sure you have the  protk proteomics tools  installed.     Register as a new user if you don\u2019t already have an account on that particular server", 
            "title": "Login to Galaxy"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#import-mass-spec-data", 
            "text": "Create a new history  in Galaxy and name it \"Organelle Tutorial\"   Download datasets  using the Galaxy uploader tool.   Open this tool by clicking the button as shown below   After opening the tool select  Paste/Fetch data  and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML  After the download is finished you should have a single item in your history.    Rename  the history item by clicking the pencil icon beside it to \"Edit attributes\".   This should bring up a dialog box where you can edit the name.    Change the name by removing everything up to the last forward slash \"/\"  Your item should then be named  OrganelleSample.mzML  Dont forget to click \"Save\"", 
            "title": "Import mass spec data"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#basic-properties-of-the-data", 
            "text": "Format:  Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about  mass spectrometry data formats here       1) What format is the OrganelleSample.mzML file?    (function(w,d,u){w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}})(window,document)    Hint   Try clicking the title bar on the data in your galaxy history.  This will toggle display of some additional information about the data.     \n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    mzML      \n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    });\n      MS versus MS/MS:  A key feature of  Tandem  mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide.  These two types of measurements are called  MS  and  MS/MS  spectra respectively.  The following schematic shows how an  MS/MS  scan results from the fragmentation of a selected product ion.  Often multiple  MS/MS  spectra are obtained for each  MS  scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed.   Number of spectra:  Click the eye icon on the history item to view the  mzML  file as text.  The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of  MS  and  MS/MS  spectra in the file.  Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function.  Looking closely you should see that this text appears once for every  MS1  spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description).  The file is large though and the browser can only see the first megabyte of it.   Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\".  There are a total of 24941 spectra in the entire file including both  MS  and  MS/MS  spectra.     2) How many  MS  spectra are there in this dataset?       Hint   To answer this question you will need to use the  Select  tool from the  Filter and Sort  submenu to select lines matching the text \"MS1 spectrum\" in the whole file.  Then use the  Line/Word/Character count  tool from the  Text Manipulation  submenu to count the number of lines returned by running the  Select  tool.    More   - and here to show more   The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer     \n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable5\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink5\").text(\"More\");\n            } else {\n                $(\"#showablelink5\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    3142      \n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    });\n       In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure.  See the instructions here   Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching  MS  peaks against the masses of whole peptides via  Peptide Mass Fingerprinting .  This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the  MS/MS  spectra.  In this tutorial only the  MS/MS  spectra will be used.     3) How many  MS/MS  spectra are there in this dataset?       Hint   Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about  MS  spectra.     \n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    21799      \n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Basic properties of the data"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#alternate-data-formats", 
            "text": "Another format you are likely to encounter for tandem mass spectrometry data is  Mascot Generic Format  or  mgf .  Mascot Generic Format ( mgf ) is the data file format preferred by the  Mascot  search engine.  It is a text based format is much easier to read by hand than the  mzML  file.  Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs.  Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included.     Download the Organelle Sample data in  mgf  format  Use the  Paste/Fetch data  tool again and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf    Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file.       4) How many spectra are there in this dataset and what type of spectra do you think they are?       Hint   Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file.    More   - and here to show more   Consider your answers to questions 3 and 4     \n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable12\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink12\").text(\"More\");\n            } else {\n                $(\"#showablelink12\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    21799  MS/MS      \n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Alternate data formats"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#obtain-a-search-database", 
            "text": "Setting up a search database is a critical step.  For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues;   Database size  Whether to include decoys  What types of variants to include if any  How to format your database identifiers   More details are provided  here .    Download a database of Mouse proteins in  fasta  format  Use the  Paste/Fetch data  tool again and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta    Inspect the first few items in the database in Galaxy.  The file is in  Fasta  format which means that each entry has a single description line that starts with a \" \" followed by a unique identifier and then some general descriptive information.  The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this;  sp|Q9CQV8|1433B_MOUSE  The database also includes decoy sequences, appended at the end.  They have identifiers like this;  decoy_rp75404       5) What is the ratio of decoys to non-decoys in the database?       Hint   Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title).     \n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    1:1      \n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Obtain a Search Database"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#run-a-search-using-xtandem", 
            "text": "A large number of search engines now exist for proteomics data.  This exercise uses  X!Tandem  which is one of the fastest and most widely used. Other search engines include  OMSSA ,  MS-GF+  and  Mascot .   Select the  X!Tandem Search  tool  Enter parameters as shown in the table below (leave all others at their defaults)  Click Execute      Parameter Name  Value      Uploaded Fasta File  UniprotMouseD_20140716.fasta    MSMS File  OrganelleSample.mgf    Variable Modifications  Oxidation M    Fixed Modifications  Carbamidomethyl C    Missed Cleavages Allowed  2    Enzyme  Trypsin    Fragment Ion Tolerance  0.5    Precursor Ion Tolerance  10 ppm     The search should run for about 5-10 minutes and will produce an output file in  X!Tandem  xml format. A much more useful format is  pepXML  so the next step in the analysis will be to run a tool to convert from tandem to pepXML.   Select the  Tandem to pepXML  tool  Select the output from the previous step as input and click  Execute   While the search is running, read some background theory on  how search engine's work .", 
            "title": "Run a search using X!Tandem"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#convert-results-to-tabular-format", 
            "text": "Although the  pepXML  format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert  pepXML  into tabular (tab separated) format, which is much easier to read.  Tabular format also has the advantage that it can be downloaded and opened using many other programs including  Excel  and  R .   Select the  pepXML to Table  tool  Select the  pepXML  file produced in the previous step as input and click  Execute   To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest.  Explore the column assignments in your tabular file by clicking on its title in your galaxy history.  This will show extra details about the item, including a handy preview where column numbers are displayed.     6) In what column number is the  assumed_charge  of peptides in the  pepXML  tabular file?      Answer    3      \n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Convert Results to tabular format"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#sort-tabular-outputs", 
            "text": "Examine the tabular output file from the previous step.  It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results.  These decoys should tend to have quite poor scores compared with real hits.  The  raw score  for  X!Tandem  searches is an  E-value .  To push these decoys to the bottom of the list we can sort the data by  raw score .    Select the  Sort  data tool from the  Filter and Sort  menu in the left pane of Galaxy  Choose to sort on  raw_score .  This is column c10   Select  Ascending order  for the sort direction (small E-values are good) and click  Execute   Browse the resulting dataset.  The top of the file should now have very few decoys.", 
            "title": "Sort tabular outputs"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#convert-raw-scores-to-probabilities", 
            "text": "Raw  X!Tandem  scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct.  A number of tools exist for this, and in this tutorial we use  Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by  Peptide Prophet  can be used to set a threshold for acceptance.  For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to  Peptide Prophet  is  Percolator .  If you're curious about how  Peptide Prophet  works, take a look at  this explainer , or  the original paper   Select the  Peptide Prophet  tool  Select the  X!Tandem  output in  pepXML  format generated earlier as input  Check the box that says  Use decoys to pin down the negative distribution .  Convert the resulting  pepXML  file to tabular using the  PepXML to Table  tool   Take a look at the resulting tabular file.  Note that this time the  peptideprophet_prob  column is populated and contains numbers between 0 and 1.     7) How many PSM's have a peptideprophet probability greater than or equal to 0.95      Hint   Use the  Filter  tool from the  Filter and Sort  submenu. Also remember that Peptide Prophet probability is given in a column called  peptideprophet_prob .  The syntax for \"greater than or equal to\" in the  Filter  tool is  =.    More   - and here to show more   Use this text in  match with condition  field of the  Filter and Sort  tool.  c11 =0.95  To answer the second question use the  Select  tool on the filtered table to select lines matching \"decoy_\"     \n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable21\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink21\").text(\"More\");\n            } else {\n                $(\"#showablelink21\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    3808  21      \n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    });\n         8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM)      Hint   Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3)    More   - and here to show more   To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect.     \n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable25\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink25\").text(\"More\");\n            } else {\n                $(\"#showablelink25\").text(\"Less\");\n            }\n        });\n    });\n         \n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    17.27%      \n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Convert raw scores to probabilities"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#perform-protein-inference", 
            "text": "Up to this point we have looked at peptide to spectrum matches  PSMs .  Each of the peptides observed will have come from a protein sequence in the  fasta  file that we used as a database, and this protein is recorded along with the  PSM  itself in all of the result tables we've viewed so far.  Unfortunately, the process of inferring the existence of proteins based on these  PSMs  is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one  PSM .  The  Protein Prophet  tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins.   Select the  Protein Prophet  tool  Choose the  pepXML  formatted output from  Peptide Prophet  as input and click  Execute  Convert the resulting  protXML  to tabular using the  protXML to Table  tool.      9) How many proteins are there with protein prophet probability greater than or equal to 0.99?      Hint   Filter on column 6  protein_probability     \n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    601      \n    $(document).ready(function(){\n        $(\"#showablelink29\").click(function(e){\n            e.preventDefault();\n            $(\"#showable29\").toggleClass(\"showable-hidden\");\n        });\n    });\n      If you have time, read over  these notes  on the Protein Prophet output.  Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.", 
            "title": "Perform Protein Inference"
        }, 
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#functional-enrichment-analysis", 
            "text": "This step will allow you to discover the identity of the Organelle that was used to create the sample.    We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom).   Start with unfiltered tabular protein prophet results  Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains  protein_name ).   Convert the \"pipes\" that separate parts of the protein_name into separate columns using the  Convert delimiters to TAB  tool in the  Text manipulation  submenu of Galaxy. This should result in a file with 3 columns  Use the  Cut columns  tool again to cut the second column from this dataset  Download this file to your desktop and rename it to  organelle.txt  Open the  GOrilla  web page in your web browser  Select  Organism  as Mouse  Upload the  organelle.txt  file as a ranged gene list  Choose  Component  for the ontology  Submit      9) What  intracellular  organelle was enriched in the sample?      Hint   Ignore terms relating to  exosomes  In the resulting output look to the most enriched and most specific GO terms.      \n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n        });\n    });\n        Answer    Mitochondria      \n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    });", 
            "title": "Functional enrichment analysis"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/", 
            "text": "How to use Galaxy\n\n\nThis background wiki gives very brief guides on performing specific tasks in Galaxy.  For much more extensive documentation including many videos, online tutorials and discussion forums please \nconsult the galaxy wiki\n.\n\n\nCreate a new History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nCreate new\n\n\nThe new history will be called \"Unnamed History\". Click its title to rename it.\n\n\n\n\nRename a history item\n\n\n\n\nLocate the item in your history and click its pencil icon\n\n\nEnter a new name in the \nName:\n field and click \nSave\n\n\n\n\n\n\nFind someone's exact username\n\n\nClick the \nUser\n menu at the top of Galaxy.  The menu that appears will show the currently logged in username\n\n\n\n\nShare a History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nShare or Publish\n\n    \n\n\nThen select \nShare with another user\n and enter the user's \nfull username\n\n\n\n\nImport a History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nHistories Shared with Me\n\n    \n\n\nSelect and view the desired history by clicking on its name\n    \n\n\n\n\nCopy Datasets\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nHistories Shared with Me\n\n    \n\n\nThe screen that appears allows copying of specific datasets between any of your histories\n\n\n\n\nMulti File Inputs\n\n\nTools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input.\nAfter clicking the multi-file icon the display should change to allow multiple inputs to be selected.\n\n\n\n\nSaved Histories\n\n\nTo view a list of your saved histories click the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy and select \nSaved Histories\n\nOnce the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags", 
            "title": "Galaxy Background"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#how-to-use-galaxy", 
            "text": "This background wiki gives very brief guides on performing specific tasks in Galaxy.  For much more extensive documentation including many videos, online tutorials and discussion forums please  consult the galaxy wiki .", 
            "title": "How to use Galaxy"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#create-a-new-history", 
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Create new  The new history will be called \"Unnamed History\". Click its title to rename it.", 
            "title": "Create a new History"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#rename-a-history-item", 
            "text": "Locate the item in your history and click its pencil icon  Enter a new name in the  Name:  field and click  Save", 
            "title": "Rename a history item"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#find-someones-exact-username", 
            "text": "Click the  User  menu at the top of Galaxy.  The menu that appears will show the currently logged in username", 
            "title": "Find someone's exact username"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#share-a-history", 
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Share or Publish \n      Then select  Share with another user  and enter the user's  full username", 
            "title": "Share a History"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#import-a-history", 
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Histories Shared with Me \n      Select and view the desired history by clicking on its name", 
            "title": "Import a History"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#copy-datasets", 
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Histories Shared with Me \n      The screen that appears allows copying of specific datasets between any of your histories", 
            "title": "Copy Datasets"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#multi-file-inputs", 
            "text": "Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input.\nAfter clicking the multi-file icon the display should change to allow multiple inputs to be selected.", 
            "title": "Multi File Inputs"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#saved-histories", 
            "text": "To view a list of your saved histories click the  History Options  menu (cog icon) in the top-right corner of Galaxy and select  Saved Histories \nOnce the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags", 
            "title": "Saved Histories"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_data_formats/", 
            "text": "Data Formats and Pre-processing\n\n\nMass Spectrometry data analysis is plagued by an overabundance of file formats.  The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, \nmzML\n.  The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively.  The reference implementation of the mzML standard is a software suite called \nProteoWizard\n. ProteoWizard includes a very handy tool called \nmsconvert\n that is capable of converting raw data from most instruments into \nmzML\n or into one of many other formats.  In addition to format conversion, \nmsconvert\n can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis.  A typical pre-processing involves;\n\n\n\n\nConversion from instrument .raw to mzML\n\n\nPeak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert)\n\n\nDenoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window\n\n\nConvert spectrum identifiers into a standardized format\n\n\n\n\nTo convert files from raw instrument native formats to \nmzML\n a windows PC is required.  If you need to do this, be sure to download \nProteoWizard\n with \nvendor reader support\n .  This package comes with \nMSConvertGUI\n which allows conversion of raw files using a graphical interface.  Once files are in \nmzML\n or \nmgf\n format they can be converted to various other formats using the \nmsconvert3\n tool in Galaxy.", 
            "title": "Data Formats Background"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_data_formats/#data-formats-and-pre-processing", 
            "text": "Mass Spectrometry data analysis is plagued by an overabundance of file formats.  The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data,  mzML .  The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively.  The reference implementation of the mzML standard is a software suite called  ProteoWizard . ProteoWizard includes a very handy tool called  msconvert  that is capable of converting raw data from most instruments into  mzML  or into one of many other formats.  In addition to format conversion,  msconvert  can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis.  A typical pre-processing involves;   Conversion from instrument .raw to mzML  Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert)  Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window  Convert spectrum identifiers into a standardized format   To convert files from raw instrument native formats to  mzML  a windows PC is required.  If you need to do this, be sure to download  ProteoWizard  with  vendor reader support  .  This package comes with  MSConvertGUI  which allows conversion of raw files using a graphical interface.  Once files are in  mzML  or  mgf  format they can be converted to various other formats using the  msconvert3  tool in Galaxy.", 
            "title": "Data Formats and Pre-processing"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_findcountitems_workflow/", 
            "text": "Create a workflow\n\n\n\n\nClick the \nWorkflow\n menu item at the top of galaxy\n\n\n\n\nClick the button called \nCreate New Workflow\n\n\n\n\n\n\n\n\nEnter a name and description for the new workflow and click \nCreate\n\n\n\n\n\n\nYou should now have a blank workflow canvas.  Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu.  Under \nWorkflow Control\n and then under \nInputs\n you should see an \nInput dataset\n item. Click it to create a blank input box\n\n\n\n\n\n\n\n\nNow add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are;\n\n\n\n\nThe \nSelect\n tool from the \nFind and Sort\n submenu\n\n\nThe \nLine/Word/Character\n count tool from the \nText Manipulation\n submenu\n\n\n\n\n\n\n\n\nAfter adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next\n\n\n\n\n\n\n\n\nSave your workflow\n\n\n\n\n\n\n\n\nAfter saving the workflow return to the main \nWorkflow\n menu (top of Galaxy) and select your new workflow to run it.  Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.", 
            "title": "Create find count items workflow"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_findcountitems_workflow/#create-a-workflow", 
            "text": "Click the  Workflow  menu item at the top of galaxy   Click the button called  Create New Workflow     Enter a name and description for the new workflow and click  Create    You should now have a blank workflow canvas.  Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu.  Under  Workflow Control  and then under  Inputs  you should see an  Input dataset  item. Click it to create a blank input box     Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are;   The  Select  tool from the  Find and Sort  submenu  The  Line/Word/Character  count tool from the  Text Manipulation  submenu     After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next     Save your workflow     After saving the workflow return to the main  Workflow  menu (top of Galaxy) and select your new workflow to run it.  Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.", 
            "title": "Create a workflow"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/", 
            "text": "Protein Databases\n\n\nIn a perfect experiment we would obtain fragment ions for all the \nb,y\n pairs of each peptide.  If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself.  Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins.  This \ndatabase\n is usually just a \nFASTA\n formatted file containing amino acid sequences for all known proteins from your study organism.  Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results.  In order to detect a peptide, its exact sequence must be explicitly included in the database. \n\n\nLarge vs Small Database\n\n\nSince it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of \nNCBInr\n.  There are two problems with this.  The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits.  Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. \n\n\nNote that very small databases can also cause problems.  In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions.  With a very small database (or with very few spectra) it may not be possible to model these distributions accurately.  In most practical situations this is not an issue.\n\n\nTypical sources of data for search databases\n\n\nUniprot.org\n: This is the canonical resource for publicly available protein sequences. It includes two large databases \nSwissProt\n, which contains manually curated sequences and \nTrembl\n which contains sequences automatically generated from genomic and transcriptomic data.\n\n\nAn Organism Specific database\n: In some cases, a community of researchers working on specific organisms will create their own sequence data repositories.  Some of these are well maintained and are the best source of data for that study organism.  Examples include \nPlasmoDB\n for Malaria for \nFlybase\n for drosophila.\n\n\nTranscriptome derived sequences\n: If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it.  If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database.\n\n\nOther\n: Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome. \n\n\nShould I include decoys?\n\n\nDecoys are often useful, but not always needed.  Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of \nPeptide Prophet\n typically use decoys because it can use these to 'pin down' the negative distribution.", 
            "title": "Protein Databases"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#protein-databases", 
            "text": "In a perfect experiment we would obtain fragment ions for all the  b,y  pairs of each peptide.  If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself.  Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins.  This  database  is usually just a  FASTA  formatted file containing amino acid sequences for all known proteins from your study organism.  Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results.  In order to detect a peptide, its exact sequence must be explicitly included in the database.", 
            "title": "Protein Databases"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#large-vs-small-database", 
            "text": "Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of  NCBInr .  There are two problems with this.  The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits.  Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine.   Note that very small databases can also cause problems.  In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions.  With a very small database (or with very few spectra) it may not be possible to model these distributions accurately.  In most practical situations this is not an issue.", 
            "title": "Large vs Small Database"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#typical-sources-of-data-for-search-databases", 
            "text": "Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases  SwissProt , which contains manually curated sequences and  Trembl  which contains sequences automatically generated from genomic and transcriptomic data.  An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories.  Some of these are well maintained and are the best source of data for that study organism.  Examples include  PlasmoDB  for Malaria for  Flybase  for drosophila.  Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it.  If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database.  Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome.", 
            "title": "Typical sources of data for search databases"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#should-i-include-decoys", 
            "text": "Decoys are often useful, but not always needed.  Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of  Peptide Prophet  typically use decoys because it can use these to 'pin down' the negative distribution.", 
            "title": "Should I include decoys?"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_search_engines/", 
            "text": "How Search Engines Work\n\n\nWhen choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below.\n\n\n\n\nThe next spectrum in the dataset is loaded for consideration\n\n\nThe spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database.  This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space.  Database search space size also depends on many other factors including\n\n\nThe size of the protein database\n\n\nVariable modifications allowed on peptides\n\n\nParent ion mass tolerance\n\n\nNumber of allowed missed enzymatic cleavages\n\n\nSpecificity of the enzyme used for digestion\n\n\n\n\n\n\nEach of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other.\n\n\nThe highest scoring \npeptide spectrum match\n (PSM) is recorded along with its score.\n\n\nSome form of global analysis of (PSM) scores is performed in order to determine a threshold of significance\n\n\n\n\nThere are many excellent presentations online that explain this in more detail.  Although it's old, I recommend \nthis presentation by Brian Searle", 
            "title": "Search Engines"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_search_engines/#how-search-engines-work", 
            "text": "When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below.   The next spectrum in the dataset is loaded for consideration  The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database.  This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space.  Database search space size also depends on many other factors including  The size of the protein database  Variable modifications allowed on peptides  Parent ion mass tolerance  Number of allowed missed enzymatic cleavages  Specificity of the enzyme used for digestion    Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other.  The highest scoring  peptide spectrum match  (PSM) is recorded along with its score.  Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance   There are many excellent presentations online that explain this in more detail.  Although it's old, I recommend  this presentation by Brian Searle", 
            "title": "How Search Engines Work"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_prophet/", 
            "text": "Protein Prophet\n\n\nThe development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference.  The original paper describing protein prophet is worth reading. It's citation is;\n\n\n\n\nNesvizhskii, A. I., Keller, A., Kolker, E. \n Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003).\n\n\n\n\nThe practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging.  Let's look at a few examples;\n\n\nUniquely identified protein\n\n\n\n\nsp|P00761|TRYP_PIG\n\n\n\n\nSearch for this protein in the Protein Prophet results file. It should have \ngroup_probability\n and \nprotein_probability\n scores of \n1.0\n.  All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group.\n\n\nIndistinguishable Protein\n\n\n\n\nsp|O08600|NUCG_MOUSE\n\n\n\n\nIn this case there still just one entry for the protein, but \nProtein Prophet\n lists another protein \ntr|Q3UN47|Q3UN47_MOUSE\n in the indistinguishable proteins column.  This protein is indistinguishable from the primary entry \nsp|O08600|NUCG_MOUSE\n because all of the identified peptides are shared between both.\n\n\nA well behaved protein group\n\n\n\n\nsp|O08677|KNG1_MOUSE\n\n\n\n\nThis protein is part of a smallish group of similar proteins.  The overall group probability is high (\n1.0\n) but probabilities group members are different.  The first member of the group has a high probability \n0.99\n but all other members have probabilities of \n0.0\n.  This is because all of the high scoring peptides are contained in the first entry.  Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor;\n\n\n\n\nplurality should not be posited with out necessity\n\n\n\n\nIn other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein.\n\n\nAnomalous groups\n\n\nIn rare cases Protein Prophet fails produces strange results when its algorithm fails to converge.  This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.", 
            "title": "Protein Prophet"
        }, 
        {
            "location": "/tutorials/proteomics_basic/background_protein_prophet/#protein-prophet", 
            "text": "The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference.  The original paper describing protein prophet is worth reading. It's citation is;   Nesvizhskii, A. I., Keller, A., Kolker, E.   Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003).   The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging.  Let's look at a few examples;  Uniquely identified protein   sp|P00761|TRYP_PIG   Search for this protein in the Protein Prophet results file. It should have  group_probability  and  protein_probability  scores of  1.0 .  All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group.  Indistinguishable Protein   sp|O08600|NUCG_MOUSE   In this case there still just one entry for the protein, but  Protein Prophet  lists another protein  tr|Q3UN47|Q3UN47_MOUSE  in the indistinguishable proteins column.  This protein is indistinguishable from the primary entry  sp|O08600|NUCG_MOUSE  because all of the identified peptides are shared between both.  A well behaved protein group   sp|O08677|KNG1_MOUSE   This protein is part of a smallish group of similar proteins.  The overall group probability is high ( 1.0 ) but probabilities group members are different.  The first member of the group has a high probability  0.99  but all other members have probabilities of  0.0 .  This is because all of the high scoring peptides are contained in the first entry.  Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor;   plurality should not be posited with out necessity   In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein.  Anomalous groups  In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge.  This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.", 
            "title": "Protein Prophet"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/", 
            "text": "De novo Genome Assembly for Illumina Data\n\n\nProtocol\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nProtocol Overview / Introduction\n\n\nIn this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.\n\n\nWhat is de novo genome assembly?\n\n\nGenome assembly refers to the process of taking a large number of short \nDNA sequences\n and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used.  Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found \nhere\n.\n\n\nPaired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process.\n\n\nThe goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose.\n\n\nThe mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See \nthis document\n for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d\n\n\nGenome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]\n\n\nWhy do we want to assemble an organism\u2019s DNA?\n\n\nDetermining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].\n\n\nThe protocol in a nutshell:\n\n\n\n\nObtain sequence read file(s) from sequencing machine(s).\n\n\nLook at the reads - get an understanding of what you\u2019ve got and what the quality is like.\n\n\nRaw data cleanup/quality trimming if necessary.\n\n\nChoose an appropriate assembly parameter set.\n\n\nAssemble the data into contigs/scaffolds.\n\n\nExamine the output of the assembly and assess assembly quality.\n\n\n\n\nFigure 1: Flowchart of de novo assembly protocol.\n\n\n\n\nRaw read sequence file formats.\n\n\nRaw read sequences can be stored in a variety of formats. The reads can be stored as text in a \nFasta\n file or with their qualities as a \nFastQ\n file. They can also be stored as alignments to references in other formats such as \nSAM\n or its binary compressed implementation \nBAM\n. All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.)\n\n\nThe most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.\n\n\nBioinformatics tools for this protocol.\n\n\nThere are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.\n\n\nGenomics Virtual Laboratory resources for this protocol.\n\n\nDepending on your requirements and skill base there are two options for running this protocol using GVL computing resources.\n\n\n\n\n\n\nYou can use \nGalaxy-tut\n or your own \nGVL\n server.\n\n\n\n\nAll of the suggested tools for this protocol are installed and available.\n\n\n\n\n\n\n\n\n\nIf you\u2019re happy and comfortable using the command line, you can do this with your own \nGVL Linux\n instance on the \nNeCTAR Research Cloud\n.\n\n\n\n\nMost of the suggested tools are available on the command line as environment modules.\n\n\nEnter \nmodule avail\n at a command prompt on your instance for details.\n\n\n\n\n\n\n\n\nYou can also use your own computing resources.\n\n\n\n\nSection 1: Read Quality Control\n\n\nPurpose:\n\n\nThe purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.\n\n\nSteps involved and suggested tools:\n\n\nExamine the quality of your raw read files.\n\n\nFor FastQ files (the most common), the suggested tool is \nFastQC\n. Details can be found \nhere\n. FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.)\n\n\nFastQC on any GVL Galaxy is located in: \nNGS: QC and Manipulation \u2192 FastQC: Comprehensive QC\n\n\nCommand line: \nfastqc\n\n\n\n\nDetails on installation and use can be found \nhere\n.\n\n\n\n\nSome of the important outputs of FastQC for our purposes are:\n\n\n\n\nRead length - Will be important in setting maximum k-mer size value for assembly\n\n\nQuality encoding type - Important for quality trimming software\n\n\n% GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution.\n\n\nTotal number of reads - Gives you an idea of coverage..\n\n\nDips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run.\n\n\nPresence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc.\n\n\nPresence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.\n\n\n\n\nQuality trimming/cleanup of read files.\n\n\nNow that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called \nTrimmomatic\n. Details on Trimmomatic can be found \nhere\n.\n\n\nTrimmomatic on GVL systems: \nNGS: QC and Manipulation -\n Trimmomatic\n\n\nCommand line: details and examples \nhere\n.\n\n\n\n\njava -cp \npath to trimmomatic jar\n org.usadellab.trimmomatic.TrimmomaticPE\n for Paired End Files\n\n\njava -cp \npath to trimmomatic jar\n org.usadellab.trimmomatic.TrimmomaticSE\n for Single End Files\n\n\n\n\nTrimmomatic can perform many read trimming functions sequentially.\n\n\nSuggested Trimmomatic functions to use:\n\n\n\n\nAdapter trimming\n\n\nThis function trims adapters, barcodes and other contaminants from the reads.\n\n\nYou need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions.\n\n\nThe default quality settings are sensible.\n\n\nThis should always be the first trimming step if it is used.\n\n\n\n\n\n\nSliding window trimming\n\n\nThis function uses a sliding window to measure average quality and trims accordingly.\n\n\nThe default quality parameters are sensible for this step.\n\n\n\n\n\n\nTrailing bases quality trimming\n\n\nThis function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases.\n\n\nUse FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point.\n\n\n\n\n\n\nLeading bases quality trimming\n\n\nThis function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads.\n\n\nUse FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary.\n\n\n\n\n\n\nMinimum read length\n\n\nOnce all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file.\n\n\nThe most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph.\n\n\n\n\n\n\n\n\nThings to look for in the output:\n\n\n\n\nNumber of reads orphaned by the trimming / cleanup process.\n\n\nNumber of pairs lost totally.\n\n\n\n\nTrimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.\n\n\nPossible alternate tools:\n\n\nRead quality trimming: \nnesoni clip\n, part of the \nnesoni\n suite of bioinformatics tools. Available at \nhttp://www.bioinformatics.net.au/software.shtml\n\n\n\n\nSection 2: Assembly\n\n\nPurpose:\n\n\nThe purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method.\n\n\nYou shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d\n\n\nSteps involved and suggested tools:\n\n\nAssembly of the reads.\n\n\nThe suggested assembly software for this protocol is the \nVelvet Optimiser\n which wraps the Velvet Assembler. The \nVelvet\n assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see \nhere\n for details).\n\n\nVelvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously.\n\n\nThe quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c).\n\n\nVelvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line.\n\n\nIn Galaxy: \nNGS-Assembly \u2192 Velvet Optimiser\n\n\nCommand line: details and examples \nhere\n.\n\n\n\n\nExample command line for paired end reads in read files \nreads_R1.fq\n and \nreads_R2.fq\n using a kmer-size search range of \n63\n - \n75\n.\n\n\nVelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory\n\n\n\n\nThe critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start \n end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.\n\n\nExamine the draft contigs and assessment of the assembly quality.\n\n\nThe Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file.\n\n\nThe assembly parameters used in the final assembly can also be found as part of the last entry in the log file.\n\n\nThe \ncontig_stats.txt\n file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc.\n\n\nMore detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. (\nFasta Manipulation \u2192 Fasta Statistics\n).\n\n\nPossible alternative software:\n\n\nAssembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include:\n\n\n\n\nSpades\n\n\nSOAP-denovo\n\n\nMIRA\n\n\nALLPATHS\n\n\n\n\nSee \nhere\n for a comprehensive list of - and links to - short read assembly programs.\n\n\n\n\nSection 3: What next?\n\n\nPurpose:\n\n\nHelp determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now.\n\n\nSome things to remember about the contigs you have just produced:\n\n\n\n\nThey\u2019re draft contigs.\n\n\nThey may contain some gaps or regions of \u201cN\u201ds.\n\n\nThere may be some mis-assemblies.\n\n\n\n\nWhat happens with your contigs next is determined by what you need them for:\n\n\n\n\nYou only want to look at certain loci or genes in your genome\n\n\nCheck and see if the regions of interest have been assembled in their entirety.\n\n\nIf they have then just use the contigs of interest.\n\n\nIf they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions.\n\n\nPerforming an automatic annotation on your draft contigs can help with this.\n\n\n\n\n\n\nYou want to perform comparative genomics analyses with your contigs\n\n\nDo your contigs cover all of the regions you are interested in?\n\n\nSome of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats.\n\n\nDo your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below.\n\n\n\n\n\n\nYou want to \u201cfinish\u201d the genome and publish it in genbank.\n\n\nCan your assembly be improved with more and/or different read data?\n\n\nCan you use other tools to improve your assembly with your current read data?\n\n\n\n\n\n\n\n\nPossible tools for improving your assemblies:\n\n\nMost of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied!\n\n\nMis-assembly checking and assembly metric tools:\n\n\n\n\nQUAST - Quality assessment tool for genome assembly \nhttp://bioinf.spbau.ru/quast\n\n\nMauve assembly metrics - \nhttp://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve\n\n\nInGAP-SV - \nhttps://sites.google.com/site/nextgengenomics/ingap\n and \nhttp://ingap.sourceforge.net/\n\n\ninGAP is also useful for finding structural variants between genomes from read mappings.\n\n\n\n\n\n\n\n\nGenome finishing tools:\n\n\nSemi-automated gap fillers:\n\n\n\n\nGap filler - \nhttp://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/\n\n\nIMAGE (V2) - \nhttp://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page\n\n\n\n\nGenome visualisers and editors\n\n\n\n\nArtemis - \nhttp://www.sanger.ac.uk/resources/software/artemis/\n\n\nIGV - \nhttp://www.broadinstitute.org/igv/\n\n\nGeneious - \nhttp://www.geneious.com/\n\n\nCLC BioWorkbench - \nhttp://www.clcbio.com/products/clc-genomics-workbench/\n\n\n\n\nAutomated and semi automated annotation tools\n\n\n\n\nProkka - \nhttps://github.com/tseemann/prokka\n\n\nRAST - \nhttp://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer\n\n\nJCVI Annotation Service - \nhttp://www.jcvi.org/cms/research/projects/annotation-service/", 
            "title": "De Novo Genome Assembly for Illumina Data"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#de-novo-genome-assembly-for-illumina-data", 
            "text": "", 
            "title": "De novo Genome Assembly for Illumina Data"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#protocol", 
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)", 
            "title": "Protocol"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#protocol-overview-introduction", 
            "text": "In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.", 
            "title": "Protocol Overview / Introduction"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#what-is-de-novo-genome-assembly", 
            "text": "Genome assembly refers to the process of taking a large number of short  DNA sequences  and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used.  Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found  here .  Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process.  The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose.  The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See  this document  for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d  Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]", 
            "title": "What is de novo genome assembly?"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#why-do-we-want-to-assemble-an-organisms-dna", 
            "text": "Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].", 
            "title": "Why do we want to assemble an organism\u2019s DNA?"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#the-protocol-in-a-nutshell", 
            "text": "Obtain sequence read file(s) from sequencing machine(s).  Look at the reads - get an understanding of what you\u2019ve got and what the quality is like.  Raw data cleanup/quality trimming if necessary.  Choose an appropriate assembly parameter set.  Assemble the data into contigs/scaffolds.  Examine the output of the assembly and assess assembly quality.   Figure 1: Flowchart of de novo assembly protocol.", 
            "title": "The protocol in a nutshell:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#raw-read-sequence-file-formats", 
            "text": "Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a  Fasta  file or with their qualities as a  FastQ  file. They can also be stored as alignments to references in other formats such as  SAM  or its binary compressed implementation  BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.)  The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.", 
            "title": "Raw read sequence file formats."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#bioinformatics-tools-for-this-protocol", 
            "text": "There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.", 
            "title": "Bioinformatics tools for this protocol."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#genomics-virtual-laboratory-resources-for-this-protocol", 
            "text": "Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources.    You can use  Galaxy-tut  or your own  GVL  server.   All of the suggested tools for this protocol are installed and available.     If you\u2019re happy and comfortable using the command line, you can do this with your own  GVL Linux  instance on the  NeCTAR Research Cloud .   Most of the suggested tools are available on the command line as environment modules.  Enter  module avail  at a command prompt on your instance for details.     You can also use your own computing resources.", 
            "title": "Genomics Virtual Laboratory resources for this protocol."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-1-read-quality-control", 
            "text": "", 
            "title": "Section 1: Read Quality Control"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose", 
            "text": "The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.", 
            "title": "Purpose:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools", 
            "text": "", 
            "title": "Steps involved and suggested tools:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#examine-the-quality-of-your-raw-read-files", 
            "text": "For FastQ files (the most common), the suggested tool is  FastQC . Details can be found  here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.)  FastQC on any GVL Galaxy is located in:  NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC  Command line:  fastqc   Details on installation and use can be found  here .   Some of the important outputs of FastQC for our purposes are:   Read length - Will be important in setting maximum k-mer size value for assembly  Quality encoding type - Important for quality trimming software  % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution.  Total number of reads - Gives you an idea of coverage..  Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run.  Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc.  Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.", 
            "title": "Examine the quality of your raw read files."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#quality-trimmingcleanup-of-read-files", 
            "text": "Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called  Trimmomatic . Details on Trimmomatic can be found  here .  Trimmomatic on GVL systems:  NGS: QC and Manipulation -  Trimmomatic  Command line: details and examples  here .   java -cp  path to trimmomatic jar  org.usadellab.trimmomatic.TrimmomaticPE  for Paired End Files  java -cp  path to trimmomatic jar  org.usadellab.trimmomatic.TrimmomaticSE  for Single End Files   Trimmomatic can perform many read trimming functions sequentially.  Suggested Trimmomatic functions to use:   Adapter trimming  This function trims adapters, barcodes and other contaminants from the reads.  You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions.  The default quality settings are sensible.  This should always be the first trimming step if it is used.    Sliding window trimming  This function uses a sliding window to measure average quality and trims accordingly.  The default quality parameters are sensible for this step.    Trailing bases quality trimming  This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases.  Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point.    Leading bases quality trimming  This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads.  Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary.    Minimum read length  Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file.  The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph.     Things to look for in the output:   Number of reads orphaned by the trimming / cleanup process.  Number of pairs lost totally.   Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.", 
            "title": "Quality trimming/cleanup of read files."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-alternate-tools", 
            "text": "Read quality trimming:  nesoni clip , part of the  nesoni  suite of bioinformatics tools. Available at  http://www.bioinformatics.net.au/software.shtml", 
            "title": "Possible alternate tools:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-2-assembly", 
            "text": "", 
            "title": "Section 2: Assembly"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose_1", 
            "text": "The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method.  You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d", 
            "title": "Purpose:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools_1", 
            "text": "", 
            "title": "Steps involved and suggested tools:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#assembly-of-the-reads", 
            "text": "The suggested assembly software for this protocol is the  Velvet Optimiser  which wraps the Velvet Assembler. The  Velvet  assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see  here  for details).  Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously.  The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c).  Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line.  In Galaxy:  NGS-Assembly \u2192 Velvet Optimiser  Command line: details and examples  here .   Example command line for paired end reads in read files  reads_R1.fq  and  reads_R2.fq  using a kmer-size search range of  63  -  75 .  VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory   The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start   end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.", 
            "title": "Assembly of the reads."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#examine-the-draft-contigs-and-assessment-of-the-assembly-quality", 
            "text": "The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file.  The assembly parameters used in the final assembly can also be found as part of the last entry in the log file.  The  contig_stats.txt  file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc.  More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ).", 
            "title": "Examine the draft contigs and assessment of the assembly quality."
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-alternative-software", 
            "text": "Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include:   Spades  SOAP-denovo  MIRA  ALLPATHS   See  here  for a comprehensive list of - and links to - short read assembly programs.", 
            "title": "Possible alternative software:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-3-what-next", 
            "text": "", 
            "title": "Section 3: What next?"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose_2", 
            "text": "Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now.  Some things to remember about the contigs you have just produced:   They\u2019re draft contigs.  They may contain some gaps or regions of \u201cN\u201ds.  There may be some mis-assemblies.   What happens with your contigs next is determined by what you need them for:   You only want to look at certain loci or genes in your genome  Check and see if the regions of interest have been assembled in their entirety.  If they have then just use the contigs of interest.  If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions.  Performing an automatic annotation on your draft contigs can help with this.    You want to perform comparative genomics analyses with your contigs  Do your contigs cover all of the regions you are interested in?  Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats.  Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below.    You want to \u201cfinish\u201d the genome and publish it in genbank.  Can your assembly be improved with more and/or different read data?  Can you use other tools to improve your assembly with your current read data?", 
            "title": "Purpose:"
        }, 
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-tools-for-improving-your-assemblies", 
            "text": "Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied!  Mis-assembly checking and assembly metric tools:   QUAST - Quality assessment tool for genome assembly  http://bioinf.spbau.ru/quast  Mauve assembly metrics -  http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve  InGAP-SV -  https://sites.google.com/site/nextgengenomics/ingap  and  http://ingap.sourceforge.net/  inGAP is also useful for finding structural variants between genomes from read mappings.     Genome finishing tools:  Semi-automated gap fillers:   Gap filler -  http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/  IMAGE (V2) -  http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page   Genome visualisers and editors   Artemis -  http://www.sanger.ac.uk/resources/software/artemis/  IGV -  http://www.broadinstitute.org/igv/  Geneious -  http://www.geneious.com/  CLC BioWorkbench -  http://www.clcbio.com/products/clc-genomics-workbench/   Automated and semi automated annotation tools   Prokka -  https://github.com/tseemann/prokka  RAST -  http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer  JCVI Annotation Service -  http://www.jcvi.org/cms/research/projects/annotation-service/", 
            "title": "Possible tools for improving your assemblies:"
        }
    ]
}