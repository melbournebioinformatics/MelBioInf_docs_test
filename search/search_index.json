{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutorials and protocols These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the Australian-made Genomics Virtual Laboratory and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.","title":"Home"},{"location":"#tutorials-and-protocols","text":"These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the Australian-made Genomics Virtual Laboratory and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.","title":"Tutorials and protocols"},{"location":"guides/bioinfo/","text":"What is bioinformatics? Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites. Getting started Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform. Resources Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published recently in BMC Bioinformatics .","title":"What is bioinformatics?"},{"location":"guides/bioinfo/#what-is-bioinformatics","text":"Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites.","title":"What is bioinformatics?"},{"location":"guides/bioinfo/#getting-started","text":"Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform.","title":"Getting started"},{"location":"guides/bioinfo/#resources","text":"Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published recently in BMC Bioinformatics .","title":"Resources"},{"location":"guides/galaxy/","text":"The Galaxy Platform Galaxy is a web platform for bioinformatics analysis. Which Galaxy should I use? There are many different Galaxy servers - each one has a different web address. For researchers based in Australia, we recommend you use Galaxy Australia . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server, you need to register and log in separately for each server. They don't talk to each other. Tutorials Galaxy Australia tutorials: https://galaxy-au-training.github.io/tutorials/ The Galaxy Training Network also hosts a large collection of useful training material: http://galaxyproject.github.io/training-material/","title":"The Galaxy platform"},{"location":"guides/galaxy/#the-galaxy-platform","text":"Galaxy is a web platform for bioinformatics analysis.","title":"The Galaxy Platform"},{"location":"guides/galaxy/#which-galaxy-should-i-use","text":"There are many different Galaxy servers - each one has a different web address. For researchers based in Australia, we recommend you use Galaxy Australia . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server, you need to register and log in separately for each server. They don't talk to each other.","title":"Which Galaxy should I use?"},{"location":"guides/galaxy/#tutorials","text":"Galaxy Australia tutorials: https://galaxy-au-training.github.io/tutorials/ The Galaxy Training Network also hosts a large collection of useful training material: http://galaxyproject.github.io/training-material/","title":"Tutorials"},{"location":"includes/connecting/","text":"Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer.","title":"Connecting"},{"location":"tutorials/alignment/","text":"NB: This tutorial is from a lab and probably obsolete; alignment is covered by variant calling workshops. PR reviewers and advice: Clare Sloggett Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/alignment/alignment/","text":"Alignment Tutorial In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We'll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials. Section 1: Alignment using Galaxy Preparation Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There's only one input file, so instead of importing a History, let's import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ . Alignment Run BWA For our alignment, we will use the tool BWA, which stands for \"Burrows-Wheeler Aligner\". You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \"@\", followed by one alignment row per read. Refer to the docs above to understand what you're seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \">\" lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that's aligned to a particular region of the genome. Section 2: View the alignment Load the VNC interface We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer's VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You'll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer's visual desktop. You'll notice two icons on the desktop. A terminal and a shortcut to IGV. Download the data The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You'll need to download both the BAM file and BAM index file. View the BAM file in IGV Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user's Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \"All\", instead select \"22\". Zoom in. If you're still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \"Go\". At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV's features - try right-clicking on a read to change the display options. Section 3: Alignment using the command line In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty. Get the data Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq This will download the datafile to the current directory. Set up the environment Now you'll need to load the bwa and samtools modules. Try module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: module load bwa module load samtools/1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands. Perform the alignment The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won't have time to index it during the lab, but your instance should already have access to an indexed genome. Try ls galaxy_genomes ls galaxy_genomes/hg19 ls galaxy_genomes/hg19/bwa_mem_index ls galaxy_genomes/hg19/bwa_mem_index/hg19/ Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: bwa mem galaxy_genomes/hg19/bwa_mem_index/hg19/hg19.fa NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq > aligned_reads.sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it. Make the BAM file Try to convert your SAM file to a BAM file using samtools view. The command you'll need is: samtools view -b -h aligned_reads.sam > aligned_reads.bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation. Other SAM tools commands Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You'll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV... That's it, I hope you enjoyed the tutorial!","title":"Alignment"},{"location":"tutorials/alignment/alignment/#alignment-tutorial","text":"In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We'll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials.","title":"Alignment Tutorial"},{"location":"tutorials/alignment/alignment/#section-1-alignment-using-galaxy","text":"","title":"Section 1: Alignment using Galaxy"},{"location":"tutorials/alignment/alignment/#preparation","text":"Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There's only one input file, so instead of importing a History, let's import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ .","title":"Preparation"},{"location":"tutorials/alignment/alignment/#alignment","text":"","title":"Alignment"},{"location":"tutorials/alignment/alignment/#run-bwa","text":"For our alignment, we will use the tool BWA, which stands for \"Burrows-Wheeler Aligner\". You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \"@\", followed by one alignment row per read. Refer to the docs above to understand what you're seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \">\" lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that's aligned to a particular region of the genome.","title":"Run BWA"},{"location":"tutorials/alignment/alignment/#section-2-view-the-alignment","text":"","title":"Section 2: View the alignment"},{"location":"tutorials/alignment/alignment/#load-the-vnc-interface","text":"We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer's VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You'll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer's visual desktop. You'll notice two icons on the desktop. A terminal and a shortcut to IGV.","title":"Load the VNC interface"},{"location":"tutorials/alignment/alignment/#download-the-data","text":"The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You'll need to download both the BAM file and BAM index file.","title":"Download the data"},{"location":"tutorials/alignment/alignment/#view-the-bam-file-in-igv","text":"Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user's Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \"All\", instead select \"22\". Zoom in. If you're still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \"Go\". At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV's features - try right-clicking on a read to change the display options.","title":"View the BAM file in IGV"},{"location":"tutorials/alignment/alignment/#section-3-alignment-using-the-command-line","text":"In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty.","title":"Section 3: Alignment using the command line"},{"location":"tutorials/alignment/alignment/#get-the-data","text":"Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq This will download the datafile to the current directory.","title":"Get the data"},{"location":"tutorials/alignment/alignment/#set-up-the-environment","text":"Now you'll need to load the bwa and samtools modules. Try module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: module load bwa module load samtools/1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands.","title":"Set up the environment"},{"location":"tutorials/alignment/alignment/#perform-the-alignment","text":"The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won't have time to index it during the lab, but your instance should already have access to an indexed genome. Try ls galaxy_genomes ls galaxy_genomes/hg19 ls galaxy_genomes/hg19/bwa_mem_index ls galaxy_genomes/hg19/bwa_mem_index/hg19/ Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: bwa mem galaxy_genomes/hg19/bwa_mem_index/hg19/hg19.fa NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq > aligned_reads.sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it.","title":"Perform the alignment"},{"location":"tutorials/alignment/alignment/#make-the-bam-file","text":"Try to convert your SAM file to a BAM file using samtools view. The command you'll need is: samtools view -b -h aligned_reads.sam > aligned_reads.bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation.","title":"Make the BAM file"},{"location":"tutorials/alignment/alignment/#other-sam-tools-commands","text":"Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You'll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV... That's it, I hope you enjoyed the tutorial!","title":"Other SAM tools commands"},{"location":"tutorials/assembly/","text":"PR reviewers and advice: Simon Gladman, Torsten Seemann, Dieter Bulach, Anna Syme Current slides: in folder https://drive.google.com/drive/u/0/folders/0B2iomOA3e6SuZHNxNjlPWG9hdTQ Other slides: Markdown slides at galaxyproject.github.io : http://galaxyproject.github.io/training-material/topics/assembly/","title":"Home"},{"location":"tutorials/assembly/assembly-background/","text":"De novo genome assembly using Velvet Background Introduction to de novo assembly DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\" \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system. The Galaxy workflow platform What is Galaxy? Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right. De novo assembly with Velvet and the Velvet Optimiser. Velvet Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically. de Bruijn graphs A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others. The Velvet algorithm Step 1: Hashing the reads. Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite. Step 2: Constructing the de Bruijn graph. Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.) Step 3: Simplification of the graph. Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.) Step 4: Read off the contigs. Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one. K-mer size and coverage cutoff values The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software. Velvet Optimiser The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time). References http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"Method Background"},{"location":"tutorials/assembly/assembly-background/#de-novo-genome-assembly-using-velvet","text":"","title":"De novo genome assembly using Velvet"},{"location":"tutorials/assembly/assembly-background/#background","text":"","title":"Background"},{"location":"tutorials/assembly/assembly-background/#introduction-to-de-novo-assembly","text":"DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\" \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system.","title":"Introduction to de novo assembly"},{"location":"tutorials/assembly/assembly-background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/assembly/assembly-background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right.","title":"What is Galaxy?"},{"location":"tutorials/assembly/assembly-background/#de-novo-assembly-with-velvet-and-the-velvet-optimiser","text":"","title":"De novo assembly with Velvet and the Velvet Optimiser."},{"location":"tutorials/assembly/assembly-background/#velvet","text":"Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.","title":"Velvet"},{"location":"tutorials/assembly/assembly-background/#de-bruijn-graphs","text":"A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.","title":"de Bruijn graphs"},{"location":"tutorials/assembly/assembly-background/#the-velvet-algorithm","text":"","title":"The Velvet algorithm"},{"location":"tutorials/assembly/assembly-background/#step-1-hashing-the-reads","text":"Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.","title":"Step 1: Hashing the reads."},{"location":"tutorials/assembly/assembly-background/#step-2-constructing-the-de-bruijn-graph","text":"Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.)","title":"Step 2: Constructing the de Bruijn graph."},{"location":"tutorials/assembly/assembly-background/#step-3-simplification-of-the-graph","text":"Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.)","title":"Step 3: Simplification of the graph."},{"location":"tutorials/assembly/assembly-background/#step-4-read-off-the-contigs","text":"Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.","title":"Step 4: Read off the contigs."},{"location":"tutorials/assembly/assembly-background/#k-mer-size-and-coverage-cutoff-values","text":"The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.","title":"K-mer size and coverage cutoff values"},{"location":"tutorials/assembly/assembly-background/#velvet-optimiser","text":"The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time).","title":"Velvet Optimiser"},{"location":"tutorials/assembly/assembly-background/#references","text":"http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"References"},{"location":"tutorials/assembly/assembly-protocol/","text":"De novo Genome Assembly for Illumina Data Protocol Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Protocol Overview / Introduction In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes. What is de novo genome assembly? Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1] Why do we want to assemble an organism\u2019s DNA? Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2]. The protocol in a nutshell: Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol. Raw read sequence file formats. Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth. Bioinformatics tools for this protocol. There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type. Genomics Virtual Laboratory resources for this protocol. Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources. Section 1: Read Quality Control Purpose: The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities. Steps involved and suggested tools: Examine the quality of your raw read files. For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s. Quality trimming/cleanup of read files. Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE for Paired End Files java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly. Possible alternate tools: Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml Section 2: Assembly Purpose: The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d Steps involved and suggested tools: Assembly of the reads. The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c). Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range. Examine the draft contigs and assessment of the assembly quality. The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ). Possible alternative software: Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs. Section 3: What next? Purpose: Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data? Possible tools for improving your assemblies: Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"De Novo Genome Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly-protocol/#de-novo-genome-assembly-for-illumina-data","text":"","title":"De novo Genome Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly-protocol/#protocol","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Protocol"},{"location":"tutorials/assembly/assembly-protocol/#protocol-overview-introduction","text":"In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.","title":"Protocol Overview / Introduction"},{"location":"tutorials/assembly/assembly-protocol/#what-is-de-novo-genome-assembly","text":"Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]","title":"What is de novo genome assembly?"},{"location":"tutorials/assembly/assembly-protocol/#why-do-we-want-to-assemble-an-organisms-dna","text":"Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].","title":"Why do we want to assemble an organism\u2019s DNA?"},{"location":"tutorials/assembly/assembly-protocol/#the-protocol-in-a-nutshell","text":"Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol.","title":"The protocol in a nutshell:"},{"location":"tutorials/assembly/assembly-protocol/#raw-read-sequence-file-formats","text":"Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.","title":"Raw read sequence file formats."},{"location":"tutorials/assembly/assembly-protocol/#bioinformatics-tools-for-this-protocol","text":"There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.","title":"Bioinformatics tools for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#genomics-virtual-laboratory-resources-for-this-protocol","text":"Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources.","title":"Genomics Virtual Laboratory resources for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#section-1-read-quality-control","text":"","title":"Section 1: Read Quality Control"},{"location":"tutorials/assembly/assembly-protocol/#purpose","text":"The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#examine-the-quality-of-your-raw-read-files","text":"For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.","title":"Examine the quality of your raw read files."},{"location":"tutorials/assembly/assembly-protocol/#quality-trimmingcleanup-of-read-files","text":"Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE for Paired End Files java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.","title":"Quality trimming/cleanup of read files."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternate-tools","text":"Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml","title":"Possible alternate tools:"},{"location":"tutorials/assembly/assembly-protocol/#section-2-assembly","text":"","title":"Section 2: Assembly"},{"location":"tutorials/assembly/assembly-protocol/#purpose_1","text":"The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools_1","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#assembly-of-the-reads","text":"The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c). Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.","title":"Assembly of the reads."},{"location":"tutorials/assembly/assembly-protocol/#examine-the-draft-contigs-and-assessment-of-the-assembly-quality","text":"The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ).","title":"Examine the draft contigs and assessment of the assembly quality."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternative-software","text":"Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs.","title":"Possible alternative software:"},{"location":"tutorials/assembly/assembly-protocol/#section-3-what-next","text":"","title":"Section 3: What next?"},{"location":"tutorials/assembly/assembly-protocol/#purpose_2","text":"Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data?","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#possible-tools-for-improving-your-assemblies","text":"Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"Possible tools for improving your assemblies:"},{"location":"tutorials/assembly/assembly/","text":"Microbial de novo Assembly for Illumina Data Introductory Tutorial Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Tutorial Overview In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies Background Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information. Preparation Login to Galaxy Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well. Import the DNA read data for the tutorial. You can do this in a few ways. If you're using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \" Microbial_assembly_input_data \". Then click 'Import History' at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it's type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it's type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format View the fastq files Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019 Section 1: Quality control The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here. Run FastQC on both input read files From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \"FASTQ reads\": ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them. Figure 1: Screenshot of FastQC interface in Galaxy Examine the FastQC output You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality. Figure 2: Screenshot of FastQC output in Galaxy Quality trim the reads using Trimmomatic. From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \"Input FASTQ file (R1/first of pair)\": ERR048396_1.fastq \"Input FASTQ file (R2/second of pair)\": ERR048396_2.fastq \"Perform initial ILLUMINACLIP step?\": Yes \"Adapter sequences to use\": TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \"How accurate ... read alignment\": 40 \"How accurate ... against a read\": 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases ... (LEADING) \"Minimum quality required to keep a base\": 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute Figure 3: Screenshot of Trimmomatic inputs in Galaxy Examine the Trimmomatic output FastQ files. You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed. Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser De novo assembly of the reads into contigs From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \"Start k-mer value\": 55 \"End k-mer value\": 69 In the input files section: \"Select first set of reads\": Trimmomatic on data 2 and data 1 (R1 paired) \"Select second set of reads\": Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \"Single or paired end reads\": Single \"Select the reads\": Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute . Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy Examine assembly output Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.) Figure 5: Screenshot of assembled contigs (a) and contig stats (b) a b Calculate some statistics on the assembled contigs From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \"Fasta or multifasta file\": Velvet Optimiser ... Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them. Section 3: Extension. Examine the contig coverage depth and blast a high coverage contig against a protein database. Examine the contig coverage depth. Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \"Filter\": Velvet Optimiser on data 8, data 7 and others: Contig stats \"With the following condition\": c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is. Extract a single sequence from the contigs file. Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \"Fasta or multifasta file\": Velvet Optimiser ... : Contigs \"Sequence ID (or partial): NODE_1_... (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format. Blast sequence to determine what it contains. We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage. Figure 6: Screenshot of the output from the NCBI Blast website","title":"Tutorial"},{"location":"tutorials/assembly/assembly/#microbial-de-novo-assembly-for-illumina-data","text":"","title":"Microbial de novo Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly/#introductory-tutorial","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introductory Tutorial"},{"location":"tutorials/assembly/assembly/#tutorial-overview","text":"In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies","title":"Tutorial Overview"},{"location":"tutorials/assembly/assembly/#background","text":"Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information.","title":"Background"},{"location":"tutorials/assembly/assembly/#preparation","text":"","title":"Preparation"},{"location":"tutorials/assembly/assembly/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.","title":"Login to Galaxy"},{"location":"tutorials/assembly/assembly/#import-the-dna-read-data-for-the-tutorial","text":"You can do this in a few ways. If you're using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \" Microbial_assembly_input_data \". Then click 'Import History' at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it's type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it's type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format","title":"Import the DNA read data for the tutorial."},{"location":"tutorials/assembly/assembly/#view-the-fastq-files","text":"Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019","title":"View the fastq files"},{"location":"tutorials/assembly/assembly/#section-1-quality-control","text":"The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here.","title":"Section 1: Quality control"},{"location":"tutorials/assembly/assembly/#run-fastqc-on-both-input-read-files","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \"FASTQ reads\": ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them.","title":"Run FastQC on both input read files"},{"location":"tutorials/assembly/assembly/#figure-1-screenshot-of-fastqc-interface-in-galaxy","text":"","title":"Figure 1: Screenshot of FastQC interface in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-fastqc-output","text":"You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.","title":"Examine the FastQC output"},{"location":"tutorials/assembly/assembly/#figure-2-screenshot-of-fastqc-output-in-galaxy","text":"","title":"Figure 2: Screenshot of FastQC output in Galaxy"},{"location":"tutorials/assembly/assembly/#quality-trim-the-reads-using-trimmomatic","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \"Input FASTQ file (R1/first of pair)\": ERR048396_1.fastq \"Input FASTQ file (R2/second of pair)\": ERR048396_2.fastq \"Perform initial ILLUMINACLIP step?\": Yes \"Adapter sequences to use\": TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \"How accurate ... read alignment\": 40 \"How accurate ... against a read\": 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases ... (LEADING) \"Minimum quality required to keep a base\": 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute","title":"Quality trim the reads using Trimmomatic."},{"location":"tutorials/assembly/assembly/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy","text":"","title":"Figure 3: Screenshot of Trimmomatic inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-trimmomatic-output-fastq-files","text":"You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.","title":"Examine the Trimmomatic output FastQ files."},{"location":"tutorials/assembly/assembly/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser","text":"The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser","title":"Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"},{"location":"tutorials/assembly/assembly/#de-novo-assembly-of-the-reads-into-contigs","text":"From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \"Start k-mer value\": 55 \"End k-mer value\": 69 In the input files section: \"Select first set of reads\": Trimmomatic on data 2 and data 1 (R1 paired) \"Select second set of reads\": Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \"Single or paired end reads\": Single \"Select the reads\": Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute .","title":"De novo assembly of the reads into contigs"},{"location":"tutorials/assembly/assembly/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy","text":"","title":"Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-assembly-output","text":"Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)","title":"Examine assembly output"},{"location":"tutorials/assembly/assembly/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b","text":"","title":"Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"},{"location":"tutorials/assembly/assembly/#a","text":"","title":"a"},{"location":"tutorials/assembly/assembly/#b","text":"","title":"b"},{"location":"tutorials/assembly/assembly/#calculate-some-statistics-on-the-assembled-contigs","text":"From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \"Fasta or multifasta file\": Velvet Optimiser ... Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.","title":"Calculate some statistics on the assembled contigs"},{"location":"tutorials/assembly/assembly/#section-3-extension","text":"Examine the contig coverage depth and blast a high coverage contig against a protein database.","title":"Section 3: Extension."},{"location":"tutorials/assembly/assembly/#examine-the-contig-coverage-depth","text":"Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \"Filter\": Velvet Optimiser on data 8, data 7 and others: Contig stats \"With the following condition\": c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is.","title":"Examine the contig coverage depth."},{"location":"tutorials/assembly/assembly/#extract-a-single-sequence-from-the-contigs-file","text":"Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \"Fasta or multifasta file\": Velvet Optimiser ... : Contigs \"Sequence ID (or partial): NODE_1_... (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format.","title":"Extract a single sequence from the contigs file."},{"location":"tutorials/assembly/assembly/#blast-sequence-to-determine-what-it-contains","text":"We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.","title":"Blast sequence to determine what it contains."},{"location":"tutorials/assembly/assembly/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website","text":"","title":"Figure 6: Screenshot of the output from the NCBI Blast website"},{"location":"tutorials/assembly/spades/","text":"Assembly using Spades Background Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . Learning objectives At the end of this tutorial you should be able to: assemble the reads using Spades, and examine the output assembly. Import and view data Galaxy If you are using Galaxy-Mel or Galaxy-Qld, import the files: In your browser, go to Galaxy-Mel or Galaxy-Qld In the top Galaxy panel, go to User and log in (or register, and then log in) In the top Galaxy panel, go to Shared Data and click on the drop down arrow Click on Histories Click on Genomics-workshop and then (over in the top right) Import history The files will now be listed in the right hand panel (your current history). (Alternatively, see here for information about how to start with Galaxy, and here for the link to import the Galaxy history for this tutorial, if you don't already have them in your history.) The data The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don't need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files. Assemble reads with Spades We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to Tools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files \u2192 Forward reads: mutant_R1.fastq Files \u2192 Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute Examine the output Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this:","title":"Microbial assembly: Spades"},{"location":"tutorials/assembly/spades/#assembly-using-spades","text":"","title":"Assembly using Spades"},{"location":"tutorials/assembly/spades/#background","text":"Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link .","title":"Background"},{"location":"tutorials/assembly/spades/#learning-objectives","text":"At the end of this tutorial you should be able to: assemble the reads using Spades, and examine the output assembly.","title":"Learning objectives"},{"location":"tutorials/assembly/spades/#import-and-view-data","text":"","title":"Import and view data"},{"location":"tutorials/assembly/spades/#galaxy","text":"If you are using Galaxy-Mel or Galaxy-Qld, import the files: In your browser, go to Galaxy-Mel or Galaxy-Qld In the top Galaxy panel, go to User and log in (or register, and then log in) In the top Galaxy panel, go to Shared Data and click on the drop down arrow Click on Histories Click on Genomics-workshop and then (over in the top right) Import history The files will now be listed in the right hand panel (your current history). (Alternatively, see here for information about how to start with Galaxy, and here for the link to import the Galaxy history for this tutorial, if you don't already have them in your history.)","title":"Galaxy"},{"location":"tutorials/assembly/spades/#the-data","text":"The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don't need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files.","title":"The data"},{"location":"tutorials/assembly/spades/#assemble-reads-with-spades","text":"We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to Tools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files \u2192 Forward reads: mutant_R1.fastq Files \u2192 Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute","title":"Assemble reads with Spades"},{"location":"tutorials/assembly/spades/#examine-the-output","text":"Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this:","title":"Examine the output"},{"location":"tutorials/bionitio/bionitio/","text":"Command-line software development To set up a new bioinformatics tool, we recommend that you follow best practice in software development. Many of these best-practice features are implemented in a template called \"Bionitio\". You can set up the template in one of many programming languages and then modify/extend your tool. Link to Bionitio: https://github.com/bionitio-team/bionitio See the README for an overview, and see the Wiki for a step-by-step guide to project setup.","title":"Command-line software development"},{"location":"tutorials/bionitio/bionitio/#command-line-software-development","text":"To set up a new bioinformatics tool, we recommend that you follow best practice in software development. Many of these best-practice features are implemented in a template called \"Bionitio\". You can set up the template in one of many programming languages and then modify/extend your tool. Link to Bionitio: https://github.com/bionitio-team/bionitio See the README for an overview, and see the Wiki for a step-by-step guide to project setup.","title":"Command-line software development"},{"location":"tutorials/cwl/cwl/","text":"Summary Common Workflow Language (or CWL), is a growing language for defining workflows in a cross-platform and cross-domain manner. In biology in particular, we need workflows to automate complex analyses such as DNA variant calling, RNA sequencing, and genome assembly. CWL provides a simple and well-defined format for automating these analysis by specifying their stages and connections using readable CWL documents. CWL makes use of a number of existing standards, including support for cluster computing using SLURM or PBS, containerisation using Docker, and deployment using common packaging formats. In addition, the CWL ecosystem has grown to include workflow visualisation tools, graphical workflow editors, libraries for interacting with CWL programatically and tools that convert to and from CWL and other workflow formats. Outcomes At the end of the course, you will be able to: Find and use CWL tool definitions online Use the Rabix Composer, a graphical editor for CWL Understand how to write CWL tool definitions for command line tools Use Docker with CWL to provide software dependencies and ensure reproducibility Join CWL tools into a workflow Read and write CWL files written in YAML Understand advanced CWL features like secondary files, parameter references and subworkflows Run CWL workflows on local and HPC systems Requirements General This workshop is aimed at anyone with basic Unix command-line experience Attendees are required to bring their own laptop computers Software All of this software is free, and should run on any operating system (Mac, Windows, or Linux): Rabix Composer: A GUI for writing CWL https://github.com/rabix/composer/releases Download the .dmg (Mac), .exe (Windows) or .AppImage (Linux) Docker A system for managing containers https://store.docker.com/search?type=edition&offering=community Python 2.7 or above If you don't have any version of python installed, Python 3.6 is preferable https://www.python.org/downloads/ cwltool A command-line executor for CWL This has to be installed using the command line https://github.com/common-workflow-language/cwltool#install A text editor for code If you don't already have a favourite, I recommend Atom https://atom.io/ Slides Workshop Slides (use the arrow keys to navigate) Part 1: Introduction Part 2: Tools Part 3: Writing Workflows Part 4: YAML","title":"Common Workflow Language for Bioinformatics"},{"location":"tutorials/cwl/cwl/#summary","text":"Common Workflow Language (or CWL), is a growing language for defining workflows in a cross-platform and cross-domain manner. In biology in particular, we need workflows to automate complex analyses such as DNA variant calling, RNA sequencing, and genome assembly. CWL provides a simple and well-defined format for automating these analysis by specifying their stages and connections using readable CWL documents. CWL makes use of a number of existing standards, including support for cluster computing using SLURM or PBS, containerisation using Docker, and deployment using common packaging formats. In addition, the CWL ecosystem has grown to include workflow visualisation tools, graphical workflow editors, libraries for interacting with CWL programatically and tools that convert to and from CWL and other workflow formats.","title":"Summary"},{"location":"tutorials/cwl/cwl/#outcomes","text":"At the end of the course, you will be able to: Find and use CWL tool definitions online Use the Rabix Composer, a graphical editor for CWL Understand how to write CWL tool definitions for command line tools Use Docker with CWL to provide software dependencies and ensure reproducibility Join CWL tools into a workflow Read and write CWL files written in YAML Understand advanced CWL features like secondary files, parameter references and subworkflows Run CWL workflows on local and HPC systems","title":"Outcomes"},{"location":"tutorials/cwl/cwl/#requirements","text":"","title":"Requirements"},{"location":"tutorials/cwl/cwl/#general","text":"This workshop is aimed at anyone with basic Unix command-line experience Attendees are required to bring their own laptop computers","title":"General"},{"location":"tutorials/cwl/cwl/#software","text":"All of this software is free, and should run on any operating system (Mac, Windows, or Linux): Rabix Composer: A GUI for writing CWL https://github.com/rabix/composer/releases Download the .dmg (Mac), .exe (Windows) or .AppImage (Linux) Docker A system for managing containers https://store.docker.com/search?type=edition&offering=community Python 2.7 or above If you don't have any version of python installed, Python 3.6 is preferable https://www.python.org/downloads/ cwltool A command-line executor for CWL This has to be installed using the command line https://github.com/common-workflow-language/cwltool#install A text editor for code If you don't already have a favourite, I recommend Atom https://atom.io/","title":"Software"},{"location":"tutorials/cwl/cwl/#slides","text":"Workshop Slides (use the arrow keys to navigate) Part 1: Introduction Part 2: Tools Part 3: Writing Workflows Part 4: YAML","title":"Slides"},{"location":"tutorials/docker/docker/","text":"Overview Containerisation is a method of bundling an application or pipeline with all its dependencies, from language runtimes like Python and R to the operating system itself. This technology has already revolutionised web development by providing a simple way to run web applications a in precisely controlled environment, regardless of which computer system they are running on. This workshop will explain how these advantages can be easily applied to bioinformatics analysis, to ensure 100% reproducibility of your work, along with easy distribution of your pipelines to other users without the need for complex installation. Learning Objectives At the end of the course, you will be able to: understand what containerisation is, and why you might use it in bioinformatics be familiar with some common containerisation tools are, and when to use each of them find and run containers built by other people run containers on HPC systems (like Melbourne Bioinformatics) build your own application into a container (containerisation) use containers as elements of a bioinformatics pipeline, and distribute your container online. Requirements This workshop is aimed at anyone with basic Unix command-line experience. Attendees are required to bring their own laptop computers. Windows users should have PuTTY installed Slides Workshop Slides (use the arrow keys to navigate) Part 1: Docker and Containers Part 2: Running Containers Part 3: Making your Own Image Part 4: Docker on HPC","title":"Containerized Bioinformatics"},{"location":"tutorials/docker/docker/#overview","text":"Containerisation is a method of bundling an application or pipeline with all its dependencies, from language runtimes like Python and R to the operating system itself. This technology has already revolutionised web development by providing a simple way to run web applications a in precisely controlled environment, regardless of which computer system they are running on. This workshop will explain how these advantages can be easily applied to bioinformatics analysis, to ensure 100% reproducibility of your work, along with easy distribution of your pipelines to other users without the need for complex installation.","title":"Overview"},{"location":"tutorials/docker/docker/#learning-objectives","text":"At the end of the course, you will be able to: understand what containerisation is, and why you might use it in bioinformatics be familiar with some common containerisation tools are, and when to use each of them find and run containers built by other people run containers on HPC systems (like Melbourne Bioinformatics) build your own application into a container (containerisation) use containers as elements of a bioinformatics pipeline, and distribute your container online.","title":"Learning Objectives"},{"location":"tutorials/docker/docker/#requirements","text":"This workshop is aimed at anyone with basic Unix command-line experience. Attendees are required to bring their own laptop computers. Windows users should have PuTTY installed","title":"Requirements"},{"location":"tutorials/docker/docker/#slides","text":"Workshop Slides (use the arrow keys to navigate) Part 1: Docker and Containers Part 2: Running Containers Part 3: Making your Own Image Part 4: Docker on HPC","title":"Slides"},{"location":"tutorials/galaxy-workflows/","text":"PR reviewers and advice: Simon Gladman, Clare Sloggett, Anna Syme Current slides: https://docs.google.com/presentation/d/1jOp5hH-NHRZahcUkPEQ3jNv-MSr0wSGpL4UsBqZECC4 (Simon's) Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Galaxy Workflows Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Background This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow Section 1: Preparation. The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy Australia Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button. Section 2: Create and run a workflow. This section will show you two different methods to create a workflow and then how to run one. Import the workflow history In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\" From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \" workflow_finished \" and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right. Workflow creation: Method 1 We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows. Some discussion Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor.. Workflow Creation: Method 2 We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file. Step 1: Create a workflow name and edit space. Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \"Workflow Name\" text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid. Step 2: Open the editor and place component tools Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\" \"Reads 1\" output to \"Select first set of reads\" \"Reads 2\" output to \"Select second set of reads\" Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \"Choose the source for the reference list:\" to History Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input. Connect the \"Reference data\" output to \"Freebayes\u2019\" Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \"Choose parameter selection level\": Complete list of all options \"Population model options\": Set population model options \"Set ploidy for the analysis\": 1 \"Input filters\": Set input filters \"Exclude alignments from analysis if they have a mapping quality less than\": 20 \"Exclude alleles from analysis if their supporting base quality is less than\": 20 \"Require at least this fraction of observations \u2026 to evaluate the position\": 0.9 \"Require at least this count of observations .. to evaluate the position\": 10 \"Population and mappability priors\": Set population and mappability priors \"Disable incorporation of prior expectations about observations\": Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input. In the right hand pane, change the following: \"With the following condition\": c6 > 500 \"Number of header lines to skip\": 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \"Map with BWA\u2019s\" bam file output. Click on the star next to \"Filter\u2019s\" output vcf. Step 3: Save it! Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it! Running the workflow We will now make a new history called \"Test\" and run the workflow on it\u2019s data. Create the new history From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \"New History Named:\" Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen Run the workflow On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli ... .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn. What now? Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"Tutorial"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#galaxy-workflows","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Galaxy Workflows"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#background","text":"This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow","title":"Background"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy Australia Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button.","title":"Section 1: Preparation."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-2-create-and-run-a-workflow","text":"This section will show you two different methods to create a workflow and then how to run one.","title":"Section 2: Create and run a workflow."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#import-the-workflow-history","text":"In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\" From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \" workflow_finished \" and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right.","title":"Import the workflow history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-1","text":"We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.","title":"Workflow creation: Method 1"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#some-discussion","text":"Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor..","title":"Some discussion"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-2","text":"We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.","title":"Workflow Creation: Method 2"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-1-create-a-workflow-name-and-edit-space","text":"Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \"Workflow Name\" text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid.","title":"Step 1: Create a workflow name and edit space."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-2-open-the-editor-and-place-component-tools","text":"Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\" \"Reads 1\" output to \"Select first set of reads\" \"Reads 2\" output to \"Select second set of reads\" Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \"Choose the source for the reference list:\" to History Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input. Connect the \"Reference data\" output to \"Freebayes\u2019\" Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \"Choose parameter selection level\": Complete list of all options \"Population model options\": Set population model options \"Set ploidy for the analysis\": 1 \"Input filters\": Set input filters \"Exclude alignments from analysis if they have a mapping quality less than\": 20 \"Exclude alleles from analysis if their supporting base quality is less than\": 20 \"Require at least this fraction of observations \u2026 to evaluate the position\": 0.9 \"Require at least this count of observations .. to evaluate the position\": 10 \"Population and mappability priors\": Set population and mappability priors \"Disable incorporation of prior expectations about observations\": Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input. In the right hand pane, change the following: \"With the following condition\": c6 > 500 \"Number of header lines to skip\": 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \"Map with BWA\u2019s\" bam file output. Click on the star next to \"Filter\u2019s\" output vcf.","title":"Step 2: Open the editor and place component tools"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-3-save-it","text":"Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it!","title":"Step 3: Save it!"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#running-the-workflow","text":"We will now make a new history called \"Test\" and run the workflow on it\u2019s data.","title":"Running the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#create-the-new-history","text":"From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \"New History Named:\" Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen","title":"Create the new history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#run-the-workflow","text":"On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli ... .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.","title":"Run the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#what-now","text":"Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"What now?"},{"location":"tutorials/galaxy-workflows/history_creation/","text":"History creation instructions for Workflow tutorial Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial. Step 1: Import the raw datafiles Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url's (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file. Step 2: Run BWA Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \"Will you select a reference genome from your history or use a built-in index?\": Use a genome from history and build index \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \"Select first set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \"Select second set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference. Step 3: Run Freebayes Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \"Choose the source for the reference genome\": History \"BAM file\": Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf). Step 4: Filter the VCF file. Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \"Filter\": FreeBayes on data 3 and data 4 (variants) \"With following condition\": c6 > 500 \"Number of header lines to skip\": 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"History Creation"},{"location":"tutorials/galaxy-workflows/history_creation/#history-creation-instructions-for-workflow-tutorial","text":"Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.","title":"History creation instructions for Workflow tutorial"},{"location":"tutorials/galaxy-workflows/history_creation/#step-1-import-the-raw-datafiles","text":"Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url's (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.","title":"Step 1: Import the raw datafiles"},{"location":"tutorials/galaxy-workflows/history_creation/#step-2-run-bwa","text":"Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \"Will you select a reference genome from your history or use a built-in index?\": Use a genome from history and build index \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \"Select first set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \"Select second set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.","title":"Step 2: Run BWA"},{"location":"tutorials/galaxy-workflows/history_creation/#step-3-run-freebayes","text":"Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \"Choose the source for the reference genome\": History \"BAM file\": Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf).","title":"Step 3: Run Freebayes"},{"location":"tutorials/galaxy-workflows/history_creation/#step-4-filter-the-vcf-file","text":"Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \"Filter\": FreeBayes on data 3 and data 4 (variants) \"With following condition\": c6 > 500 \"Number of header lines to skip\": 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"Step 4: Filter the VCF file."},{"location":"tutorials/galaxy_101/","text":"PR reviewers and advice: Simon Gladman, Gayle Philip, Clare Sloggett, Jessica Chung, Anna Syme Current slides: https://docs.google.com/presentation/d/1dzHagGkswjH7MOZ7OACVXGU-riBs33K3J5lWpnCpPhs (Simon's) Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy_101/galaxy_101/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Introduction to Galaxy Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Overview This beginners tutorial will introduce Galaxy's interface, tool use, histories, and get new users of the Genomics Virtual Laboratory up and running. You can follow this tutorial with the Galaxy Workflows tutorial to learn about workflows. Galaxy is an open source, web-based platform for accessible, reproducible, and transparent computational biomedical research. It allows users without programming experience to easily specify parameters and run individual tools as well as larger workflows. It also captures run information so that any user can repeat and understand a complete computational analysis. Finally, it allows users to share and publish analyses via the web. Learning Objectives At the end of the course, you will be able to: login to a Galaxy server. upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output. Requirements This is a hands-on workshop and attendees should bring their own laptops. Background Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools Section 1: Preparation. The purpose of this section is to get you to log in to the server. Open your browser. We recommend Firefox or Chrome (please don't use Internet Explorer or Safari). Go to the Galaxy Australia server. Alternatively, you can use a different Galaxy server - a list of available servers is here . If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit Section 2: Getting data into Galaxy There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it. Method 1: Upload a file from your own computer With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz (To download this file, copy the link into a new browser tab, and press enter. The file should now download.) From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \"Type\" (= file format) to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Method 2: Upload a file from a URL If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it. Method 2 (again): Get data from a Data Library Now we are going to get another file from a shared Data Library. Go to the menu at the top of the screen and click Shared Data -> Data Libraries . Click on the Library: \"Galaxy Australia Training Material\" then \"Galaxy_101\" To add the MRSA0252.fa file to our history click on the checkbox next to it Then click the \"To History\" button at the top of the page and select \"As Datasets\" Click the \"Import\" button Finally, click \"Analyse Data\" in the menu at the top of the screen to return to your history. The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this. The data Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illumina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format. Section 3: Play with the tools The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \"Name\" text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools. Example 1: Histogram and summary statistics The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \"Cut Columns\" to: c1,c6 \"Delimited by\": Tab \"Cut from\": Contig_stats.txt Click Execute Examine the new file by clicking on its icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \"Remove First\": 1 \"from\": Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \"Dataset\": Remove beginning on Data X \"Numerical column for X axis\": c2 \"Number of breaks\": 25 \"Plot title\": Histogram of Contig Coverage \"Label for X axis\": Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \"Summary statistics on\": Remove beginning on Data X \"Column or expression\": c2 Click Execute Example 2: Convert Fastq to Fasta This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \"FASTQ file to convert\": Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2. Example 3: Find Ribosomal RNA Features in a DNA Sequence This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA's in a sequence From the tool panel, click on NGS: Annotation -> barrnap and set the following: \"Fasta file\": MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the icon.) Now let's say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \"Select lines from\": (whatever you called the barrnap gff3 output) \"that\": Matching \"the pattern\": 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations! What now? Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"Introduction to Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#introduction-to-galaxy","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introduction to Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#overview","text":"This beginners tutorial will introduce Galaxy's interface, tool use, histories, and get new users of the Genomics Virtual Laboratory up and running. You can follow this tutorial with the Galaxy Workflows tutorial to learn about workflows. Galaxy is an open source, web-based platform for accessible, reproducible, and transparent computational biomedical research. It allows users without programming experience to easily specify parameters and run individual tools as well as larger workflows. It also captures run information so that any user can repeat and understand a complete computational analysis. Finally, it allows users to share and publish analyses via the web.","title":"Overview"},{"location":"tutorials/galaxy_101/galaxy_101/#learning-objectives","text":"At the end of the course, you will be able to: login to a Galaxy server. upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output.","title":"Learning Objectives"},{"location":"tutorials/galaxy_101/galaxy_101/#requirements","text":"This is a hands-on workshop and attendees should bring their own laptops.","title":"Requirements"},{"location":"tutorials/galaxy_101/galaxy_101/#background","text":"Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools","title":"Background"},{"location":"tutorials/galaxy_101/galaxy_101/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server. Open your browser. We recommend Firefox or Chrome (please don't use Internet Explorer or Safari). Go to the Galaxy Australia server. Alternatively, you can use a different Galaxy server - a list of available servers is here . If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit","title":"Section 1: Preparation."},{"location":"tutorials/galaxy_101/galaxy_101/#section-2-getting-data-into-galaxy","text":"There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it.","title":"Section 2: Getting data into Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#method-1-upload-a-file-from-your-own-computer","text":"With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz (To download this file, copy the link into a new browser tab, and press enter. The file should now download.) From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \"Type\" (= file format) to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Method 1: Upload a file from your own computer"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-upload-a-file-from-a-url","text":"If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.","title":"Method 2: Upload a file from a URL"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-again-get-data-from-a-data-library","text":"Now we are going to get another file from a shared Data Library. Go to the menu at the top of the screen and click Shared Data -> Data Libraries . Click on the Library: \"Galaxy Australia Training Material\" then \"Galaxy_101\" To add the MRSA0252.fa file to our history click on the checkbox next to it Then click the \"To History\" button at the top of the page and select \"As Datasets\" Click the \"Import\" button Finally, click \"Analyse Data\" in the menu at the top of the screen to return to your history. The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this.","title":"Method 2 (again): Get data from a Data Library"},{"location":"tutorials/galaxy_101/galaxy_101/#the-data","text":"Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illumina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format.","title":"The data"},{"location":"tutorials/galaxy_101/galaxy_101/#section-3-play-with-the-tools","text":"The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \"Name\" text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools.","title":"Section 3: Play with the tools"},{"location":"tutorials/galaxy_101/galaxy_101/#example-1-histogram-and-summary-statistics","text":"The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \"Cut Columns\" to: c1,c6 \"Delimited by\": Tab \"Cut from\": Contig_stats.txt Click Execute Examine the new file by clicking on its icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \"Remove First\": 1 \"from\": Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \"Dataset\": Remove beginning on Data X \"Numerical column for X axis\": c2 \"Number of breaks\": 25 \"Plot title\": Histogram of Contig Coverage \"Label for X axis\": Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \"Summary statistics on\": Remove beginning on Data X \"Column or expression\": c2 Click Execute","title":"Example 1: Histogram and summary statistics"},{"location":"tutorials/galaxy_101/galaxy_101/#example-2-convert-fastq-to-fasta","text":"This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \"FASTQ file to convert\": Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2.","title":"Example 2: Convert Fastq to Fasta"},{"location":"tutorials/galaxy_101/galaxy_101/#example-3-find-ribosomal-rna-features-in-a-dna-sequence","text":"This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA's in a sequence From the tool panel, click on NGS: Annotation -> barrnap and set the following: \"Fasta file\": MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the icon.) Now let's say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \"Select lines from\": (whatever you called the barrnap gff3 output) \"that\": Matching \"the pattern\": 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations!","title":"Example 3: Find Ribosomal RNA Features in a DNA Sequence"},{"location":"tutorials/galaxy_101/galaxy_101/#what-now","text":"Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"What now?"},{"location":"tutorials/genomespace/","text":"PR reviewers and advice: Yousef Kowsar Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/genomespace/genomespace/","text":"What is GenomeSpace? GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au . Prerequisites GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain. Registering a GenomeSpace account To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \"Register new GenomeSpace user\" link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website. Making a swift container (These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\". To make a container, click \"Create Container\". Mounting a swift container Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to. Basic file manipulation Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \"view file\". Adding a Galaxy service to your account: PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \"Add new\" button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName. File transfer to/from Galaxy PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"Genomespace"},{"location":"tutorials/genomespace/genomespace/#what-is-genomespace","text":"GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au .","title":"What is GenomeSpace?"},{"location":"tutorials/genomespace/genomespace/#prerequisites","text":"GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.","title":"Prerequisites"},{"location":"tutorials/genomespace/genomespace/#registering-a-genomespace-account","text":"To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \"Register new GenomeSpace user\" link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.","title":"Registering a GenomeSpace account"},{"location":"tutorials/genomespace/genomespace/#making-a-swift-container","text":"(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\". To make a container, click \"Create Container\".","title":"Making a swift container"},{"location":"tutorials/genomespace/genomespace/#mounting-a-swift-container","text":"Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to.","title":"Mounting a swift container"},{"location":"tutorials/genomespace/genomespace/#basic-file-manipulation","text":"Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \"view file\".","title":"Basic file manipulation"},{"location":"tutorials/genomespace/genomespace/#adding-a-galaxy-service-to-your-account","text":"PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \"Add new\" button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName.","title":"Adding a Galaxy service to your account:"},{"location":"tutorials/genomespace/genomespace/#file-transfer-tofrom-galaxy","text":"PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"File transfer to/from Galaxy"},{"location":"tutorials/gvl_launch/","text":"PR reviewers and advice: Clare Sloggett, Simon Gladman, Nuwan Goonasekera Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/gvl_launch/gvl_launch/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } Launching a Personal GVL Server on the NeCTAR Research Cloud Tutorial Overview This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down Background What is the GVL? The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Australia , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources. What is NeCTAR? The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free). Section 1: Access the NeCTAR dashboard Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose 'The University of Melbourne' and not 'The University of Melbourne (with ECP)'. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here . Section 2: Get your cloud credentials Launching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher to create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top 'API Access' tab. Click on the 'Download OpenStack RC File' button on the top right. A file containing your credentials will be saved to your downloads folder. This file will be needed later in the launch process. Next you must obtain your OpenStack password and record it securely for future use. If you have ever done this step before, you should reuse your previously saved password. To obtain your OpenStack password, click on your username on the top right hand corner and go to settings as shown below. Click on the reset password link. Once reset, your password will be displayed. Record this securely for all future GVL launches. Section 3: Launch your personal GVL instance In a new browser tab, go to launch.usegalaxy.org You will see the screen below. Select the first option from the list - \"Genomics Virtual Lab\". You will be asked to login with your preferred social network account. Once logged in, perform the following steps. Select NeCTAR for the question \"On which cloud would you like to launch your appliance\" Click \"load credentials from file\" and provide file you downloaded in Section 2.3 Provide the OpenStack password you obtained in Section 2.5. Click \"Test and Use these Credentials\". The Next button will now be activated. Click the next button, and provide the following options. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Optional advanced options Toggle the 'Advanced cloudlaunch options' option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Deployment name: You can override the name with a name of your choice. It is recommended you choose a unique name if you launch multiple instances. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Root Volume Storage: Keep the default: Instance storage Placement Zone: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Click 'Launch' to launch a GVL. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced cloudlaunch options -> Placement Zone field. Section 4: Access your GVL instance Once your instance has finished launching, click on the Access address to access your GVL dashboard. If you accidentally closed the launch page, you can find your access address at any time by logging back into launch.usegalaxy.org and navigating to the \"My Appliances\" section through the menu bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL. Section 5: GVL services Listed below are short descriptions of the services the GVL provides. Galaxy Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar. CloudMan CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username 'ubuntu' and your cluster password. You can also shut down your instance (permanently) with CloudMan. Lubuntu Desktop Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster password. SSH Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username 'ubuntu' or the username 'researcher' and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software). JupyterHub JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username 'researcher' and your cluster password. You may need to install Python packages you intend to use via the command line beforehand. RStudio RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username 'researcher' and your cluster password. Public HTML This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html will be publicly accessible. PacBio SMRT Portal PacBio's SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on 'Admin' in the top navigation bar. Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed. Scroll down to 'SMRT Analysis', and click 'install'. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user. Section 6: Shutting your machine down There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username 'ubuntu' and your cluster password. Click the Shut down... button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Launching a Personal GVL Server"},{"location":"tutorials/gvl_launch/gvl_launch/#launching-a-personal-gvl-server-on-the-nectar-research-cloud","text":"","title":"Launching a Personal GVL Server on the NeCTAR Research Cloud"},{"location":"tutorials/gvl_launch/gvl_launch/#tutorial-overview","text":"This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down","title":"Tutorial Overview"},{"location":"tutorials/gvl_launch/gvl_launch/#background","text":"","title":"Background"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-the-gvl","text":"The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Australia , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources.","title":"What is the GVL?"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-nectar","text":"The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free).","title":"What is NeCTAR?"},{"location":"tutorials/gvl_launch/gvl_launch/#section-1-access-the-nectar-dashboard","text":"Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose 'The University of Melbourne' and not 'The University of Melbourne (with ECP)'. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here .","title":"Section 1: Access the NeCTAR dashboard"},{"location":"tutorials/gvl_launch/gvl_launch/#section-2-get-your-cloud-credentials","text":"Launching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher to create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top 'API Access' tab. Click on the 'Download OpenStack RC File' button on the top right. A file containing your credentials will be saved to your downloads folder. This file will be needed later in the launch process. Next you must obtain your OpenStack password and record it securely for future use. If you have ever done this step before, you should reuse your previously saved password. To obtain your OpenStack password, click on your username on the top right hand corner and go to settings as shown below. Click on the reset password link. Once reset, your password will be displayed. Record this securely for all future GVL launches.","title":"Section 2: Get your cloud credentials"},{"location":"tutorials/gvl_launch/gvl_launch/#section-3-launch-your-personal-gvl-instance","text":"In a new browser tab, go to launch.usegalaxy.org You will see the screen below. Select the first option from the list - \"Genomics Virtual Lab\". You will be asked to login with your preferred social network account. Once logged in, perform the following steps. Select NeCTAR for the question \"On which cloud would you like to launch your appliance\" Click \"load credentials from file\" and provide file you downloaded in Section 2.3 Provide the OpenStack password you obtained in Section 2.5. Click \"Test and Use these Credentials\". The Next button will now be activated. Click the next button, and provide the following options. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Optional advanced options Toggle the 'Advanced cloudlaunch options' option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Deployment name: You can override the name with a name of your choice. It is recommended you choose a unique name if you launch multiple instances. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Root Volume Storage: Keep the default: Instance storage Placement Zone: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Click 'Launch' to launch a GVL. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced cloudlaunch options -> Placement Zone field.","title":"Section 3: Launch your personal GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-4-access-your-gvl-instance","text":"Once your instance has finished launching, click on the Access address to access your GVL dashboard. If you accidentally closed the launch page, you can find your access address at any time by logging back into launch.usegalaxy.org and navigating to the \"My Appliances\" section through the menu bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL.","title":"Section 4: Access your GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-5-gvl-services","text":"Listed below are short descriptions of the services the GVL provides.","title":"Section 5: GVL services"},{"location":"tutorials/gvl_launch/gvl_launch/#galaxy","text":"Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar.","title":"Galaxy"},{"location":"tutorials/gvl_launch/gvl_launch/#cloudman","text":"CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username 'ubuntu' and your cluster password. You can also shut down your instance (permanently) with CloudMan.","title":"CloudMan"},{"location":"tutorials/gvl_launch/gvl_launch/#lubuntu-desktop","text":"Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster password.","title":"Lubuntu Desktop"},{"location":"tutorials/gvl_launch/gvl_launch/#ssh","text":"Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username 'ubuntu' or the username 'researcher' and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software).","title":"SSH"},{"location":"tutorials/gvl_launch/gvl_launch/#jupyterhub","text":"JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username 'researcher' and your cluster password. You may need to install Python packages you intend to use via the command line beforehand.","title":"JupyterHub"},{"location":"tutorials/gvl_launch/gvl_launch/#rstudio","text":"RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username 'researcher' and your cluster password.","title":"RStudio"},{"location":"tutorials/gvl_launch/gvl_launch/#public-html","text":"This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html will be publicly accessible.","title":"Public HTML"},{"location":"tutorials/gvl_launch/gvl_launch/#pacbio-smrt-portal","text":"PacBio's SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on 'Admin' in the top navigation bar. Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed. Scroll down to 'SMRT Analysis', and click 'install'. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user.","title":"PacBio SMRT Portal"},{"location":"tutorials/gvl_launch/gvl_launch/#section-6-shutting-your-machine-down","text":"There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username 'ubuntu' and your cluster password. Click the Shut down... button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Section 6: Shutting your machine down"},{"location":"tutorials/hpc/","text":"PR reviewers and advice: Andrew Robinson, Peter Georgeson, Chol-hee Jung, Ben Moran Current slides: HTML in repository Other slides: None yet","title":"Home"},{"location":"tutorials/hpc/hpc/","text":"em {font-style: normal; font-family: courier new;} High-Performance Computing A hands-on-workshop covering High-Performance Computing (HPC). Overview Using High Performance Computing (HPC) resources such as Melbourne Bioinformatics in an effective and efficient manner is key to modern research. This workshop will introduce you to HPC environments and assist you to get on with your research. Learning Objectives At the end of the course, you will be able to: Define 'What is HPC?' Load software modules Submit jobs Select job queues Monitor your job\u2019s progress Know what resources you can request Select appropriate resources Requirements You will need a basic understanding of Unix, or you should have attended an Introduction to Unix workshop in the past. All participants are required to bring their own laptop computers. Introduction Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the main concepts of High-Performance Computing. The slides are available if you would like. Additionally the following reference material is available for later use: Reference Material ### What is an HPC? An HPC is simply a large collection of server-grade computers working together to solve large problems. * **Big**: HPCs typically have lots of CPUs and Memory and consequently large jobs. * **Shared**: There are usually lots of users making use of it at one time. * **Coordinated**: There is a coordinator program to ensure fair-use between its users. * **Compute Collection**: HPCs use a number of computers at once to solve lots of large jobs. **Figure**: The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. ### Why use HPCs? The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. * **Many CPUs**: HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. * **Large Memory**: Hundreds of GBs to multiple TBs of RAM are typical for each node. * **Efficient use**: Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. ### Software Modules There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. * **Packaged**: to avoid conflicts between software, each is packaged up into a module and only used on demand. * **Loadable**: before using a software module you need to load it. * **Versions**: given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. ### Job Submission Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. * **SLURM**: this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. * **Queues (Partition)**: when a job is submitted it is added to a work queue; in SLURM this is called a Partition. * **Batch**: HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen. #### Resources So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. 1. **CPUs**: most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. 2. **Memory**: you need to estimate (or guess) how much memory (RAM) your program needs. 3. **Nodes**: most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. 4. **Time**: like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. #### Job Types There are two types of jobs that you can submit: 1. **Shared**: a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. 2. **Exclusive**: an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node. Connecting to the HPC To begin this workshop you will need to connect to the HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC's tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Topic 1: Exploring an HPC An HPC (short for 'High-Performance Computer') is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions. Exercises 1.1) What is the contact email for your HPC's System Administrator? Hint When you login, you will be presented with a message; this is called the *Message Of The Day* and usually includes lots of useful information. On *barcoo* this includes a list of useful commands, the last login details for your account and the contact email of the system administrator Answer Depending on which computer you are working: * SNOWY & BARCOO: help@vlsci.unimelb.edu.au 1.2) Run the sinfo command. How many nodes are there in this hpc? Hint *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* and *barcoo[1,5]* is shorthand for *barcoo1* and *barcoo5* Additional Hint Have a look at the NODELIST column. Only count each node once. $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m NOTE: the above list will vary depending on the HPC setup. Answer The *sinfo* command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. * BARCOO: **70** (*barcoo001* through *barcoo070*) * SNOWY: **43** (*snowy001* through *snowy043*) Alternate Method An automatic (though more complex) way would have been running the following command: $ scontrol show node | grep NodeName | wc -l Where: * *scontrol show node*: lists details of all nodes (over multiple lines) * *grep NodeName*: only shows the NodeName line * *wc -l*: counts the number of lines Topic 2: Software Modules Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily Exercises 2.1) What happens if you run the module command without any options / arguments? Hint Literally type *module* and press *ENTER* key. Answer **Answer**: It prints an error followed by a list of available options / flags $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear 2.2) How do you find a list of available software? Hint Try the *module* command. Don't forget the *man* command to get help for a command Additional Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>) Answer The module command is used to show details of software modules (tools). **Answer**: $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the */usr/local/Modules/modulefiles* line are the science software; before this are a few built-in ones that you can ignore. 2.3) How many modules are there starting with ' f '? Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the *avail* subcommand. Additional Hint > If an argument is given, then each directory in the MODULEPATH is searched for modulefiles > whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. Answer The man page told us that we could put a search term after *module avail*. $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastStructure-gcc/20150320 freetype-gcc/2.5.3 fastx_toolkit-gcc/0.0.14 **Answer**: 26 modules NOTE: this was correct at time of writing this workshop and might increase over time so don't be alarmed if you got more Alternate Method To get a fully automated solution your could do the following command: $ module -l avail 2>&1 | grep \"^f\" | wc -l Where: * *module -l avail*: lists all modules (in long format, i.e. one per line) * *2>&1*: merges output from *standard error* to the *standard output* so it can be feed into grep. For some reason the developers of the *module* command thought it was a good idea to output the module names on the *error* stream rather than the logical *output* stream. * *grep \"^f\"*: only shows lines beginning with *f* * *wc -l*: counts the number of lines 2.4) Run the pear command (without loading it), does it work? Hint This question is very literal Answer $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' **Answer**: No, command not found 2.5) How would we load the pear module? Hint Check the man page for *module* again and look for a subcommand that might load modules; it is quite literal as well. Additional Hint Run the command *man module* Use a search to find out about the *load* subcommand (e.g. /load<enter>) Answer $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software. 2.6) Now it's load ed, run pear again, what does it do? Hint The paper citation gives a clue. Answer $ module load pear-gcc/0.9.4 [15:59:19] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____| / \\ | _ \\ | |_) | _| / _ \\ | |_) | | __/| |___ / ___ \\| _ < |_| |_____/_/ \\_\\_| \\_\\ PEAR v0.9.4 [August 8, 2014] - [+bzlib] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... **Answer**: \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap) 2.7) List all the loaded modules. How many are there? Where did all the others come from? Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. Answer ** *List* all the loaded modules. How many are there?** $ module list Currently Loaded Modulefiles: 1) gmp/5.1.3 3) mpc/1.0.2 5) bzip2-gcc/1.0.6 2) mpfr/3.1.2 4) gcc/4.8.2 6) pear-gcc/0.9.4 **Answer**: 6 **Where did all the others come from?** You may have noticed when we loaded *pear-gcc* the module called *gcc* was also loaded; this gives a hint as to where the others come from. **Answer**: They are *dependencies*; that is, they are supporting software that is used by the module we loaded. Additionally, some HPC's automatically load some modules for you when you login. 2.8) How do you undo the loading of the pear module? List the loaded modules again, did they all disappear? Hint Computer Scientists are not always inventive with naming commands, try something starting with *un* Answer **How do you undo the loading of the *pear* module?** $ module unload pear-gcc **Answer**: the *unload* sub-command removes the named module from our current SSH session. **List the loaded modules again, did they all disapear?** **Answer**: Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question) 2.9) How do you clear ALL loaded modules? Hint It's easier than running *unload* for all modules This one isn't that straight forward; try a [synonym](https://www.google.com.au/search?q=rid+synonym) of *rid*. Additional Hint We will *purge* the list of loaded modules. Answer $ module purge **Answer**: running the *purge* sub-command will unload all modules you loaded (and all dependencies). **Alternative**: if you close your SSH connection and re-open it the new session will be blank as well. BEFORE CONTINUING : If you are using BARCOO or SNOWY you will need to load the default commands again. Do so by running module load vlsci Topic 3: Job Submission Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is OK for small jobs, it's unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power Background On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal (screen) to a file; this file is called the output file. Reservation : much like a reservation for a resturant holds a table for you, the administrator can give you an HPC reservation which holds various resources for a period of time exclusively for you. Exercises Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime 3.1) Which nodes could a 'main' job go on? Hint Try the *sinfo* command Additional Hint Have a look at the PARTITION and NODELIST columns. The *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m Note: the output to the sinfo command will look different depending on which HPC you are using and it's current usage levels Answer The *sinfo* command will list the *partitions*. It summaries the nodes by their current status so there may be more than one line with *main* in the partition column. It lists the nodes in shorthand i.e. barcoo[1,3-5] means barcoo1, barcoo3, barcoo4, barcoo5. **Answer**: barcoo001, barcoo002, ..., barcoo070 Use the cat command to view the contents of task01 , task02 and task03 job script 3.2) How many cpu cores will each ask for? Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint **Non-exclusive (shared) jobs**: It is *--cpus-per-task x --ntasks* but if *--ntasks* is not present it defaults to 1 so it's *--cpus-per-task x 1* **Exclusive jobs**: The *--nodes* options tells us how many nodes we ask for and the *--exclusive* option says give us all it has. This one is a bit tricky as we don't really know until it runs. Answer **Answer**: * task01: **1 cpu core** * task02: **6 cpu cores** * task03: **at least 1** as this has requested all cpu cores on the node its running on (*--exclusive*). However, since we know that all nodes on *barcoo* have 16, we know it will get 16. 3.3) What about total memory? Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --cpus-per-task x --ntasks* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated Answer The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --ntasks x --cpus-per-task* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --share ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. **Answer**: * task01: **1024MB** (1GB) i.e. 1024 x 1 x 1 * task02: **12288MB** (12GB) i.e. 2048 x 3 x 2 * task03: **at least 1024MB** (1GB). The actual amount could be a lot more as most HPCs have 100GB+ per node 3.4) How long can each run for? Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer The *--time* option is what tells slurm how long your job will run for. **Answer**: * task01: requests **30:00 (30mins 0secs)**, uses ~30secs * task02: requests **5:00 (5mins 0secs)**, uses ~5secs * task03: requests **1:00 (1min 0secs)**, uses ~30secs 3.5) Is this maximum, minimum or both runtime? Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time. 3.6) Calculate the --time specification for the following runtimes: 1h30m: --time= 1m20s: --time= 1.5days: --time= 30m: --time= Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. 1h30m: --time=01:30:00 (alternatively: 0-01:30) 2. 1m20s: --time=01:20 3. 1.5days: --time=1-12 4. 30m: --time=30 3.7) What do the following --time specifications mean? --time=12-00:20 --time=45 --time=00:30 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. --time=12-00:20 12 days and 20 minutes 2. --time=45 45 minutes 3. --time=00:30 30 seconds Reservations Before we continue, a quick note on reservations. Reservations are not normally needed however sometimes we will, particularly when the HPC is busy. To make use of a reservation you need to know its name and provide it with the --reservation option Today we use the training reservation so that we have resources available to run our jobs. Your jobs will need to contain the line: #SBATCH --reservation=training Now use sbatch to submit the task01 job: 3.8) What job id was your job given? Hint Use the man page for the sbatch command. The *Synopsis* at the top will give you an idea how to run it. Answer $ sbatch task01 Submitted batch job 9998 **Answer**: it's unique for each job; in the above example mine was *9998* 3.9) Which node did your job go on? Hint The *squeue* command shows you the currently running jobs. If it's been longer than 30 seconds since you submitted it you might have to resubmit it. Answer Use the *squeue* command to show all jobs. Search for your *jobid* and look in the *NODELIST* column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 9999 compute task01 training R 0:05 1 lims-hpc-2 **Answer**: it's dependent on node availability at time; in the above example mine was *lims-hpc-2* Advanced 3.10) Make a copy of task01 and call it prime_numbers . Make it load the training module and use the prime command to calculate prime numbers for 20 seconds. Hint You can find the *prime* command in the *training-gcc/1.0* module Answer The key points to change in the task01 script are: 1. adding the *module load training-gcc/1.0* 2. replacing the *sleep* (and *echo*) statements with a call to *prime 20*. #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=PARTITION #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION module load training-gcc/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Where *RESERVATION* is replaced with *training* and *PARTITION* is replaced with *main* Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well. 3.11) Submit the job. What was the largest prime number it found in 20 seconds? Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the *SLURM output file*; this is called *slurm-JOBID.out* where JOBID is replaced by the actual job id. Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16:11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16:11:27 AEST 2015 3.12) Modify your prime_numbers script to notify you via email when it starts and ends. Submit it again. Did it start immediately or have some delay? How long did it actually run for? Hint There are two options that you will need to set. See sbatch manpage for details. Additional Hint Both start with *--mail* Answer #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=TRAINING #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Where *RESERVATION* is replaced with *training*, *PARTITION* is replaced with *main* and *name@email.address* by your email address **Answers**: * **Did it start immediately or have some delay?** The *Queued time* value in the subject of start email will tell you how long it waited. * **How long did it actually run for?** The *Run time* value in the subject of the end email will tell you how long it ran for which should be ~20 seconds. Topic 4: Job Monitoring It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU's, processing step etc.). In this topic we will cover some of the tools that are available that enable you to watch what is happening so we can make better predictions in the future. Exercises 4.1) What does the top command show? Hint When all else fails, try *man*; specifically, the description section Answer $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... **Answer**: in lay-person terms *\"Continually updating CPU and Memory usage\"* Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications). 4.2) How much total memory does this HPC (head-node) have? Hint This would be a system-wide statistic. Answer **Answer**: If you look at the first value on the *Mem* line (line 4) it will tell you the total memory on this computer (node). * **BARCOO**: 65942760k or ~64 GigaBytes * **SNOWY**: 132035040k or ~128 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again. 4.3) What is the current total CPU usage? Hint This might be easier to work out what is not used and subtract it from 100% Additional Hint *Idle* is another term for not used (or *id* for short) Answer **Answer**: If you subtract the *%id* value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage 4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low ? Hint It's not PID but from time to time it might be ordered sequentially. Answer **Answer**: *%CPU* which gives you an indication of how much CPU time each process uses and sorted high-to-low. Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates. 4.5) Why might the numbers disagree? Hint It might have something to do with the total number of CPU Cores on the system. Answer **Answer**: *%CPU* column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in *top* (excluding round errors) they would add up 100% x the number of cpu cores available. On BARCOO it is 0-2400% and SNOWY it is 0-3200% for individual processes. 4.6) What command-line flag instructs top to sort results by %MEM ? Can you think of a reason that this might be useful? Hint Use the *top* manpage. Additional Hint *\"m is for memory!\"* Answer **Answer**: *top -m* will cause *top* to sort the processes by memory usage. **Can you think of a reason that this might be useful?** Your program might be using a lot of memory and you want to know how much; by sorting by memory will cause your program to stay at the top. 4.7) Run \"top -c\" . What does it do? How might this be helpful? Hint Use the *top* manpage. Additional Hint *\"c is for complete!\"* *\"c is also for command!\"* which is another name for program Answer **What does it do?** It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. **How might this be helpful?** Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the *-c* flag to show the complete command. **NOTE**: If *top* is running you can press the *c* key to toggle show/hide complete command 4.8) How can you get top to only show your processes? Why might this be useful? Hint Use the *top* manpage. Additional Hint *\"u is for user[name]!\"* Answer **How can you get *top* to only show your processes?** **Answer 1**: *top -u YOURUSERNAME* **Answer 2**: while running *top* press the *u* key, type YOURUSERNAME and press key **Why might this be useful?** When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours. Topic 5: All Together This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job Task 1: Write a job script Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : main Time : 5 mins Memory : 1 GB (remember to specify it in MB) Reservation : training Task 2: Load/use software module Edit your job script so that it: Loads the training-gcc/1.0 module Runs the fakejob command with your name as the first parameter. FYI: fakejob is a command that was made to demonstrate what real commands might do in terms of CPU and Memory usage. It does not perform any useful task; if you must know, it just calculates prime numbers for 5 minutes and consumes some memory NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script. Task 3: Submit job NOTE : Task 4 is time dependent on task 3; you need to do it within 2 or 3 minutes of running step 3.1 so it might be a good idea to read ahead before hand. Don't stress if you don't complete it in time, you can simply run 3.1 again. Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task). Task 4: Monitor the job Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top won't be able to see the job. To be able to use top, you will first need to login to the compute node that is running your job. To login: $ ssh barcooXXX Where XXX is the actual node number you were allocated (See task 3.4). You are now connected from your computer to barcoo which is connected to barcooXXX. +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | BARCOO | -- SSH --> | BARCOOXXX | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt [USERNAME@barcoo USERNAME]$ Changes to: [USERNAME@barcooXXX USERNAME]$ Once logged in to the relevent compute node you can run top to view your job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs. How does the CPU and Memory usage change over time? Hint It should vary (within the limits you set in the job script) Answer The *fakejob* program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory. The percentage that it shows is based on the total memory of the node that runs your job; check Topic 4, Question 4.2 to remember how to find the total memory. Finished Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don't forget to complete the training survey and return it to the workshop facilitators.","title":"Introduction to HPC"},{"location":"tutorials/hpc/hpc/#high-performance-computing","text":"A hands-on-workshop covering High-Performance Computing (HPC).","title":"High-Performance Computing"},{"location":"tutorials/hpc/hpc/#overview","text":"Using High Performance Computing (HPC) resources such as Melbourne Bioinformatics in an effective and efficient manner is key to modern research. This workshop will introduce you to HPC environments and assist you to get on with your research.","title":"Overview"},{"location":"tutorials/hpc/hpc/#learning-objectives","text":"At the end of the course, you will be able to: Define 'What is HPC?' Load software modules Submit jobs Select job queues Monitor your job\u2019s progress Know what resources you can request Select appropriate resources","title":"Learning Objectives"},{"location":"tutorials/hpc/hpc/#requirements","text":"You will need a basic understanding of Unix, or you should have attended an Introduction to Unix workshop in the past. All participants are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/hpc/hpc/#introduction","text":"Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the main concepts of High-Performance Computing. The slides are available if you would like. Additionally the following reference material is available for later use: Reference Material ### What is an HPC? An HPC is simply a large collection of server-grade computers working together to solve large problems. * **Big**: HPCs typically have lots of CPUs and Memory and consequently large jobs. * **Shared**: There are usually lots of users making use of it at one time. * **Coordinated**: There is a coordinator program to ensure fair-use between its users. * **Compute Collection**: HPCs use a number of computers at once to solve lots of large jobs. **Figure**: The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. ### Why use HPCs? The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. * **Many CPUs**: HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. * **Large Memory**: Hundreds of GBs to multiple TBs of RAM are typical for each node. * **Efficient use**: Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. ### Software Modules There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. * **Packaged**: to avoid conflicts between software, each is packaged up into a module and only used on demand. * **Loadable**: before using a software module you need to load it. * **Versions**: given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. ### Job Submission Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. * **SLURM**: this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. * **Queues (Partition)**: when a job is submitted it is added to a work queue; in SLURM this is called a Partition. * **Batch**: HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen. #### Resources So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. 1. **CPUs**: most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. 2. **Memory**: you need to estimate (or guess) how much memory (RAM) your program needs. 3. **Nodes**: most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. 4. **Time**: like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. #### Job Types There are two types of jobs that you can submit: 1. **Shared**: a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. 2. **Exclusive**: an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Introduction"},{"location":"tutorials/hpc/hpc/#connecting-to-the-hpc","text":"To begin this workshop you will need to connect to the HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC's tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer.","title":"Connecting to the HPC"},{"location":"tutorials/hpc/hpc/#topic-1-exploring-an-hpc","text":"An HPC (short for 'High-Performance Computer') is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions.","title":"Topic 1: Exploring an HPC"},{"location":"tutorials/hpc/hpc/#exercises","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#11-what-is-the-contact-email-for-your-hpcs-system-administrator","text":"Hint When you login, you will be presented with a message; this is called the *Message Of The Day* and usually includes lots of useful information. On *barcoo* this includes a list of useful commands, the last login details for your account and the contact email of the system administrator Answer Depending on which computer you are working: * SNOWY & BARCOO: help@vlsci.unimelb.edu.au","title":"1.1) What is the contact email for your HPC's System Administrator?"},{"location":"tutorials/hpc/hpc/#12-run-the-sinfo-command-how-many-nodes-are-there-in-this-hpc","text":"Hint *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* and *barcoo[1,5]* is shorthand for *barcoo1* and *barcoo5* Additional Hint Have a look at the NODELIST column. Only count each node once. $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m NOTE: the above list will vary depending on the HPC setup. Answer The *sinfo* command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. * BARCOO: **70** (*barcoo001* through *barcoo070*) * SNOWY: **43** (*snowy001* through *snowy043*) Alternate Method An automatic (though more complex) way would have been running the following command: $ scontrol show node | grep NodeName | wc -l Where: * *scontrol show node*: lists details of all nodes (over multiple lines) * *grep NodeName*: only shows the NodeName line * *wc -l*: counts the number of lines","title":"1.2) Run the sinfo command.  How many nodes are there in this hpc?"},{"location":"tutorials/hpc/hpc/#topic-2-software-modules","text":"Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily","title":"Topic 2: Software Modules"},{"location":"tutorials/hpc/hpc/#exercises_1","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#21-what-happens-if-you-run-the-module-command-without-any-options-arguments","text":"Hint Literally type *module* and press *ENTER* key. Answer **Answer**: It prints an error followed by a list of available options / flags $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear","title":"2.1) What happens if you run the module command without any options / arguments?"},{"location":"tutorials/hpc/hpc/#22-how-do-you-find-a-list-of-available-software","text":"Hint Try the *module* command. Don't forget the *man* command to get help for a command Additional Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>) Answer The module command is used to show details of software modules (tools). **Answer**: $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the */usr/local/Modules/modulefiles* line are the science software; before this are a few built-in ones that you can ignore.","title":"2.2) How do you find a list of available software?"},{"location":"tutorials/hpc/hpc/#23-how-many-modules-are-there-starting-with-f","text":"Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the *avail* subcommand. Additional Hint > If an argument is given, then each directory in the MODULEPATH is searched for modulefiles > whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. Answer The man page told us that we could put a search term after *module avail*. $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastStructure-gcc/20150320 freetype-gcc/2.5.3 fastx_toolkit-gcc/0.0.14 **Answer**: 26 modules NOTE: this was correct at time of writing this workshop and might increase over time so don't be alarmed if you got more Alternate Method To get a fully automated solution your could do the following command: $ module -l avail 2>&1 | grep \"^f\" | wc -l Where: * *module -l avail*: lists all modules (in long format, i.e. one per line) * *2>&1*: merges output from *standard error* to the *standard output* so it can be feed into grep. For some reason the developers of the *module* command thought it was a good idea to output the module names on the *error* stream rather than the logical *output* stream. * *grep \"^f\"*: only shows lines beginning with *f* * *wc -l*: counts the number of lines","title":"2.3) How many modules are there starting with 'f'?"},{"location":"tutorials/hpc/hpc/#24-run-the-pear-command-without-loading-it-does-it-work","text":"Hint This question is very literal Answer $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' **Answer**: No, command not found","title":"2.4) Run the pear command (without loading it), does it work?"},{"location":"tutorials/hpc/hpc/#25-how-would-we-load-the-pear-module","text":"Hint Check the man page for *module* again and look for a subcommand that might load modules; it is quite literal as well. Additional Hint Run the command *man module* Use a search to find out about the *load* subcommand (e.g. /load<enter>) Answer $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software.","title":"2.5) How would we load the pear module?"},{"location":"tutorials/hpc/hpc/#26-now-its-loaded-run-pear-again-what-does-it-do","text":"Hint The paper citation gives a clue. Answer $ module load pear-gcc/0.9.4 [15:59:19] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____| / \\ | _ \\ | |_) | _| / _ \\ | |_) | | __/| |___ / ___ \\| _ < |_| |_____/_/ \\_\\_| \\_\\ PEAR v0.9.4 [August 8, 2014] - [+bzlib] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... **Answer**: \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap)","title":"2.6) Now it's loaded, run pear again, what does it do?"},{"location":"tutorials/hpc/hpc/#27-list-all-the-loaded-modules-how-many-are-there-where-did-all-the-others-come-from","text":"Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. Answer ** *List* all the loaded modules. How many are there?** $ module list Currently Loaded Modulefiles: 1) gmp/5.1.3 3) mpc/1.0.2 5) bzip2-gcc/1.0.6 2) mpfr/3.1.2 4) gcc/4.8.2 6) pear-gcc/0.9.4 **Answer**: 6 **Where did all the others come from?** You may have noticed when we loaded *pear-gcc* the module called *gcc* was also loaded; this gives a hint as to where the others come from. **Answer**: They are *dependencies*; that is, they are supporting software that is used by the module we loaded. Additionally, some HPC's automatically load some modules for you when you login.","title":"2.7) List all the loaded modules. How many are there? Where did all the others come from?"},{"location":"tutorials/hpc/hpc/#28-how-do-you-undo-the-loading-of-the-pear-module-list-the-loaded-modules-again-did-they-all-disappear","text":"Hint Computer Scientists are not always inventive with naming commands, try something starting with *un* Answer **How do you undo the loading of the *pear* module?** $ module unload pear-gcc **Answer**: the *unload* sub-command removes the named module from our current SSH session. **List the loaded modules again, did they all disapear?** **Answer**: Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question)","title":"2.8) How do you undo the loading of the pear module?  List the loaded modules again, did they all disappear?"},{"location":"tutorials/hpc/hpc/#29-how-do-you-clear-all-loaded-modules","text":"Hint It's easier than running *unload* for all modules This one isn't that straight forward; try a [synonym](https://www.google.com.au/search?q=rid+synonym) of *rid*. Additional Hint We will *purge* the list of loaded modules. Answer $ module purge **Answer**: running the *purge* sub-command will unload all modules you loaded (and all dependencies). **Alternative**: if you close your SSH connection and re-open it the new session will be blank as well. BEFORE CONTINUING : If you are using BARCOO or SNOWY you will need to load the default commands again. Do so by running module load vlsci","title":"2.9) How do you clear ALL loaded modules?"},{"location":"tutorials/hpc/hpc/#topic-3-job-submission","text":"Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is OK for small jobs, it's unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power","title":"Topic 3: Job Submission"},{"location":"tutorials/hpc/hpc/#background","text":"On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal (screen) to a file; this file is called the output file. Reservation : much like a reservation for a resturant holds a table for you, the administrator can give you an HPC reservation which holds various resources for a period of time exclusively for you.","title":"Background"},{"location":"tutorials/hpc/hpc/#exercises_2","text":"Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime","title":"Exercises"},{"location":"tutorials/hpc/hpc/#31-which-nodes-could-a-main-job-go-on","text":"Hint Try the *sinfo* command Additional Hint Have a look at the PARTITION and NODELIST columns. The *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m Note: the output to the sinfo command will look different depending on which HPC you are using and it's current usage levels Answer The *sinfo* command will list the *partitions*. It summaries the nodes by their current status so there may be more than one line with *main* in the partition column. It lists the nodes in shorthand i.e. barcoo[1,3-5] means barcoo1, barcoo3, barcoo4, barcoo5. **Answer**: barcoo001, barcoo002, ..., barcoo070 Use the cat command to view the contents of task01 , task02 and task03 job script","title":"3.1) Which nodes could a 'main' job go on?"},{"location":"tutorials/hpc/hpc/#32-how-many-cpu-cores-will-each-ask-for","text":"Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint **Non-exclusive (shared) jobs**: It is *--cpus-per-task x --ntasks* but if *--ntasks* is not present it defaults to 1 so it's *--cpus-per-task x 1* **Exclusive jobs**: The *--nodes* options tells us how many nodes we ask for and the *--exclusive* option says give us all it has. This one is a bit tricky as we don't really know until it runs. Answer **Answer**: * task01: **1 cpu core** * task02: **6 cpu cores** * task03: **at least 1** as this has requested all cpu cores on the node its running on (*--exclusive*). However, since we know that all nodes on *barcoo* have 16, we know it will get 16.","title":"3.2) How many cpu cores will each ask for?"},{"location":"tutorials/hpc/hpc/#33-what-about-total-memory","text":"Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --cpus-per-task x --ntasks* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated Answer The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --ntasks x --cpus-per-task* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --share ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. **Answer**: * task01: **1024MB** (1GB) i.e. 1024 x 1 x 1 * task02: **12288MB** (12GB) i.e. 2048 x 3 x 2 * task03: **at least 1024MB** (1GB). The actual amount could be a lot more as most HPCs have 100GB+ per node","title":"3.3) What about total memory?"},{"location":"tutorials/hpc/hpc/#34-how-long-can-each-run-for","text":"Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer The *--time* option is what tells slurm how long your job will run for. **Answer**: * task01: requests **30:00 (30mins 0secs)**, uses ~30secs * task02: requests **5:00 (5mins 0secs)**, uses ~5secs * task03: requests **1:00 (1min 0secs)**, uses ~30secs","title":"3.4) How long can each run for?"},{"location":"tutorials/hpc/hpc/#35-is-this-maximum-minimum-or-both-runtime","text":"Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time.","title":"3.5) Is this maximum, minimum or both runtime?"},{"location":"tutorials/hpc/hpc/#36-calculate-the-time-specification-for-the-following-runtimes","text":"1h30m: --time= 1m20s: --time= 1.5days: --time= 30m: --time= Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. 1h30m: --time=01:30:00 (alternatively: 0-01:30) 2. 1m20s: --time=01:20 3. 1.5days: --time=1-12 4. 30m: --time=30","title":"3.6) Calculate the --time specification for the following runtimes:"},{"location":"tutorials/hpc/hpc/#37-what-do-the-following-time-specifications-mean","text":"--time=12-00:20 --time=45 --time=00:30 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. --time=12-00:20 12 days and 20 minutes 2. --time=45 45 minutes 3. --time=00:30 30 seconds","title":"3.7) What do the following --time specifications mean?"},{"location":"tutorials/hpc/hpc/#reservations","text":"Before we continue, a quick note on reservations. Reservations are not normally needed however sometimes we will, particularly when the HPC is busy. To make use of a reservation you need to know its name and provide it with the --reservation option Today we use the training reservation so that we have resources available to run our jobs. Your jobs will need to contain the line: #SBATCH --reservation=training Now use sbatch to submit the task01 job:","title":"Reservations"},{"location":"tutorials/hpc/hpc/#38-what-job-id-was-your-job-given","text":"Hint Use the man page for the sbatch command. The *Synopsis* at the top will give you an idea how to run it. Answer $ sbatch task01 Submitted batch job 9998 **Answer**: it's unique for each job; in the above example mine was *9998*","title":"3.8) What job id was your job given?"},{"location":"tutorials/hpc/hpc/#39-which-node-did-your-job-go-on","text":"Hint The *squeue* command shows you the currently running jobs. If it's been longer than 30 seconds since you submitted it you might have to resubmit it. Answer Use the *squeue* command to show all jobs. Search for your *jobid* and look in the *NODELIST* column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 9999 compute task01 training R 0:05 1 lims-hpc-2 **Answer**: it's dependent on node availability at time; in the above example mine was *lims-hpc-2*","title":"3.9) Which node did your job go on?"},{"location":"tutorials/hpc/hpc/#advanced","text":"","title":"Advanced"},{"location":"tutorials/hpc/hpc/#310-make-a-copy-of-task01-and-call-it-prime_numbers-make-it-load-the-training-module-and-use-the-prime-command-to-calculate-prime-numbers-for-20-seconds","text":"Hint You can find the *prime* command in the *training-gcc/1.0* module Answer The key points to change in the task01 script are: 1. adding the *module load training-gcc/1.0* 2. replacing the *sleep* (and *echo*) statements with a call to *prime 20*. #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=PARTITION #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION module load training-gcc/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Where *RESERVATION* is replaced with *training* and *PARTITION* is replaced with *main* Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well.","title":"3.10) Make a copy of task01 and call it prime_numbers.  Make it load the training module and use the prime command to calculate prime numbers for 20 seconds."},{"location":"tutorials/hpc/hpc/#311-submit-the-job-what-was-the-largest-prime-number-it-found-in-20-seconds","text":"Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the *SLURM output file*; this is called *slurm-JOBID.out* where JOBID is replaced by the actual job id. Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16:11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16:11:27 AEST 2015","title":"3.11) Submit the job.  What was the largest prime number it found in 20 seconds?"},{"location":"tutorials/hpc/hpc/#312-modify-your-prime_numbers-script-to-notify-you-via-email-when-it-starts-and-ends-submit-it-again","text":"Did it start immediately or have some delay? How long did it actually run for? Hint There are two options that you will need to set. See sbatch manpage for details. Additional Hint Both start with *--mail* Answer #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=TRAINING #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Where *RESERVATION* is replaced with *training*, *PARTITION* is replaced with *main* and *name@email.address* by your email address **Answers**: * **Did it start immediately or have some delay?** The *Queued time* value in the subject of start email will tell you how long it waited. * **How long did it actually run for?** The *Run time* value in the subject of the end email will tell you how long it ran for which should be ~20 seconds.","title":"3.12) Modify your prime_numbers script to notify you via email when it starts and ends.  Submit it again."},{"location":"tutorials/hpc/hpc/#topic-4-job-monitoring","text":"It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU's, processing step etc.). In this topic we will cover some of the tools that are available that enable you to watch what is happening so we can make better predictions in the future.","title":"Topic 4: Job Monitoring"},{"location":"tutorials/hpc/hpc/#exercises_3","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#41-what-does-the-top-command-show","text":"Hint When all else fails, try *man*; specifically, the description section Answer $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... **Answer**: in lay-person terms *\"Continually updating CPU and Memory usage\"* Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications).","title":"4.1) What does the top command show?"},{"location":"tutorials/hpc/hpc/#42-how-much-total-memory-does-this-hpc-head-node-have","text":"Hint This would be a system-wide statistic. Answer **Answer**: If you look at the first value on the *Mem* line (line 4) it will tell you the total memory on this computer (node). * **BARCOO**: 65942760k or ~64 GigaBytes * **SNOWY**: 132035040k or ~128 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again.","title":"4.2) How much total memory does this HPC (head-node) have?"},{"location":"tutorials/hpc/hpc/#43-what-is-the-current-total-cpu-usage","text":"Hint This might be easier to work out what is not used and subtract it from 100% Additional Hint *Idle* is another term for not used (or *id* for short) Answer **Answer**: If you subtract the *%id* value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage","title":"4.3) What is the current total CPU usage?"},{"location":"tutorials/hpc/hpc/#44-what-column-does-it-appear-to-be-sorting-the-processes-by-is-this-low-to-high-or-high-to-low","text":"Hint It's not PID but from time to time it might be ordered sequentially. Answer **Answer**: *%CPU* which gives you an indication of how much CPU time each process uses and sorted high-to-low. Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates.","title":"4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low?"},{"location":"tutorials/hpc/hpc/#45-why-might-the-numbers-disagree","text":"Hint It might have something to do with the total number of CPU Cores on the system. Answer **Answer**: *%CPU* column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in *top* (excluding round errors) they would add up 100% x the number of cpu cores available. On BARCOO it is 0-2400% and SNOWY it is 0-3200% for individual processes.","title":"4.5) Why might the numbers disagree?"},{"location":"tutorials/hpc/hpc/#46-what-command-line-flag-instructs-top-to-sort-results-by-mem","text":"Can you think of a reason that this might be useful? Hint Use the *top* manpage. Additional Hint *\"m is for memory!\"* Answer **Answer**: *top -m* will cause *top* to sort the processes by memory usage. **Can you think of a reason that this might be useful?** Your program might be using a lot of memory and you want to know how much; by sorting by memory will cause your program to stay at the top.","title":"4.6) What command-line flag instructs top to sort results by %MEM?"},{"location":"tutorials/hpc/hpc/#47-run-top-c-what-does-it-do-how-might-this-be-helpful","text":"Hint Use the *top* manpage. Additional Hint *\"c is for complete!\"* *\"c is also for command!\"* which is another name for program Answer **What does it do?** It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. **How might this be helpful?** Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the *-c* flag to show the complete command. **NOTE**: If *top* is running you can press the *c* key to toggle show/hide complete command","title":"4.7) Run \"top -c\".  What does it do?  How might this be helpful?"},{"location":"tutorials/hpc/hpc/#48-how-can-you-get-top-to-only-show-your-processes-why-might-this-be-useful","text":"Hint Use the *top* manpage. Additional Hint *\"u is for user[name]!\"* Answer **How can you get *top* to only show your processes?** **Answer 1**: *top -u YOURUSERNAME* **Answer 2**: while running *top* press the *u* key, type YOURUSERNAME and press key **Why might this be useful?** When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours.","title":"4.8) How can you get top to only show your processes?  Why might this be useful?"},{"location":"tutorials/hpc/hpc/#topic-5-all-together","text":"This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job","title":"Topic 5: All Together"},{"location":"tutorials/hpc/hpc/#task-1-write-a-job-script","text":"Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : main Time : 5 mins Memory : 1 GB (remember to specify it in MB) Reservation : training","title":"Task 1: Write a job script"},{"location":"tutorials/hpc/hpc/#task-2-loaduse-software-module","text":"Edit your job script so that it: Loads the training-gcc/1.0 module Runs the fakejob command with your name as the first parameter. FYI: fakejob is a command that was made to demonstrate what real commands might do in terms of CPU and Memory usage. It does not perform any useful task; if you must know, it just calculates prime numbers for 5 minutes and consumes some memory NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script.","title":"Task 2: Load/use software module"},{"location":"tutorials/hpc/hpc/#task-3-submit-job","text":"NOTE : Task 4 is time dependent on task 3; you need to do it within 2 or 3 minutes of running step 3.1 so it might be a good idea to read ahead before hand. Don't stress if you don't complete it in time, you can simply run 3.1 again. Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task).","title":"Task 3: Submit job"},{"location":"tutorials/hpc/hpc/#task-4-monitor-the-job","text":"Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top won't be able to see the job. To be able to use top, you will first need to login to the compute node that is running your job. To login: $ ssh barcooXXX Where XXX is the actual node number you were allocated (See task 3.4). You are now connected from your computer to barcoo which is connected to barcooXXX. +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | BARCOO | -- SSH --> | BARCOOXXX | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt [USERNAME@barcoo USERNAME]$ Changes to: [USERNAME@barcooXXX USERNAME]$ Once logged in to the relevent compute node you can run top to view your job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs.","title":"Task 4: Monitor the job"},{"location":"tutorials/hpc/hpc/#how-does-the-cpu-and-memory-usage-change-over-time","text":"Hint It should vary (within the limits you set in the job script) Answer The *fakejob* program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory. The percentage that it shows is based on the total memory of the node that runs your job; check Topic 4, Question 4.2 to remember how to find the total memory.","title":"How does the CPU and Memory usage change over time?"},{"location":"tutorials/hpc/hpc/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don't forget to complete the training survey and return it to the workshop facilitators.","title":"Finished"},{"location":"tutorials/hpc/intro/","text":"What is an HPC? An HPC is simply a large collection of server-grade computers working together to solve large problems. Big : HPCs typically have lots of CPUs and Memory and consequently large jobs. Shared : There are usually lots of users making use of it at one time. Coordinated : There is a coordinator program to ensure fair-use between its users. Compute Collection : HPCs use a number of computers at once to solve lots of large jobs. Figure : The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. Why use HPCs? The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. Many CPUs : HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. Large Memory : Hundreds of GBs to multiple TBs of RAM are typical for each node. Efficient use : Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. Software Modules There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. Packaged : to avoid conflicts between software, each is packaged up into a module and only used on demand. Loadable : before using a software module you need to load it. Versions : given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. Job Submission Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. SLURM : this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. Queues (Partition) : when a job is submitted it is added to a work queue; in SLURM this is called a Partition. Batch : HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen. Resources So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. CPUs : most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. Memory : you need to estimate (or guess) how much memory (RAM) your program needs. Nodes : most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. Time : like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. Job Types There are two types of jobs that you can submit: Shared : a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. Exclusive : an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Intro"},{"location":"tutorials/hpc/intro/#what-is-an-hpc","text":"An HPC is simply a large collection of server-grade computers working together to solve large problems. Big : HPCs typically have lots of CPUs and Memory and consequently large jobs. Shared : There are usually lots of users making use of it at one time. Coordinated : There is a coordinator program to ensure fair-use between its users. Compute Collection : HPCs use a number of computers at once to solve lots of large jobs. Figure : The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available.","title":"What is an HPC?"},{"location":"tutorials/hpc/intro/#why-use-hpcs","text":"The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. Many CPUs : HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. Large Memory : Hundreds of GBs to multiple TBs of RAM are typical for each node. Efficient use : Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later.","title":"Why use HPCs?"},{"location":"tutorials/hpc/intro/#software-modules","text":"There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. Packaged : to avoid conflicts between software, each is packaged up into a module and only used on demand. Loadable : before using a software module you need to load it. Versions : given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control.","title":"Software Modules"},{"location":"tutorials/hpc/intro/#job-submission","text":"Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. SLURM : this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. Queues (Partition) : when a job is submitted it is added to a work queue; in SLURM this is called a Partition. Batch : HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen.","title":"Job Submission"},{"location":"tutorials/hpc/intro/#resources","text":"So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. CPUs : most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. Memory : you need to estimate (or guess) how much memory (RAM) your program needs. Nodes : most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. Time : like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards.","title":"Resources"},{"location":"tutorials/hpc/intro/#job-types","text":"There are two types of jobs that you can submit: Shared : a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. Exclusive : an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Job Types"},{"location":"tutorials/hpc/robinson-hpc-link/","text":"High Performance Computing Please see the link here .","title":"High Performance Computing"},{"location":"tutorials/hpc/robinson-hpc-link/#high-performance-computing","text":"Please see the link here .","title":"High Performance Computing"},{"location":"tutorials/molecular_dynamics_101/","text":"PR reviewers and advice: Thomas Coudrat Current slides: https://drive.google.com/open?id=1tm2UjKIBikFb9daYBI0z_53Agh4ktCt9lyNr0SH6GrY Other slides: None yet","title":"Home"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/","text":"Molecular Dynamics Tutorial - Introduction to cluster computing Overview In the following tutorials we will be logging on to a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing (this tutorial) : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD. Learning Objectives At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results Requirements This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select 'Software agreement', 'Add software', 'NAMD'. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat. 1 - Overview The aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs. The program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the Theoretical and Computational Biophysics Group at Illinois University at Urbana Champaign . It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output. 2 - Basic introduction to cluster computing a) Logging in to the cluster Using a terminal on your local computer and your username and password, login to the HPC cluster. ssh <username>@snowy.vlsci.unimelb.edu.au You should see a welcome screen and a command line prompt. If you type ls at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet! Note : be careful to use the right terminal when you are typing in commands! Sometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example: [<username>@snowy ~]$ <- tells you your terminal is on Snowy b) Copy across files, starting the job We\u2019ll need to copy across the basic example directory to our working directory on Snowy . Do this with: cp -r /vlsci/examples/namd/Namd_simple_example_01 . Note that the dot is important! Change into this directory and launch the job with the command sbatch and the sbatch script. cd Namd_simple_example_01 Then type: sbatch sbatch_namd_start_job Your job has now been submitted to the cluster. Easy hey? Check the job is running with the showq command. (type it at the command line). Too much information? Try: showq -u <username> This particular job is very short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the .dcd suffix. c) Understanding the input files While we wait, let's take a look at the sbatch example script to understand what is going on. Type: less sbatch_namd_start_job less is a Unix file viewer. Press \u201cq\u201d to quit the viewing of the file You should see the lines: #SBATCH --nodes=4 This line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC. Now let us have a look at the NAMD configuration script: less namd_1ubq_example.conf Woah! There is quite a bit of information here, don't worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top: structure 1ubq_example.psf coordinates 1ubq_example.pdb outputName 1ubq_output_01 These are simply defining the input files, (the protein structure file .psf , and the coordinate file, .pdb ) and also the name of the output files. Further down you will see: set Temp 310 temperature $Temp Which is setting the temperature to 310 K (37 C) while below that we have: ## Parameter file paraTypeCharmm on parameters par_all27_prot_na.prm which tells NAMD which parameter file to use. (you'll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file. Somewhere in the middle you will see these lines: ## Periodic Boundary Conditions cellBasisVector1 48. 0. 0. cellBasisVector2 0. 48. 0. cellBasisVector3 0. 0. 48. cellOrigin 0 0 0 wrapAll on wrapWater on This defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other. Near the bottom we have the lines: ## Output files restartfreq 5000 dcdfreq 100 xstFreq 100 outputEnergies 100 outputPressure 100 outputTiming 100 These lines tell us how often to write out to the output files. The most important is the dcdfreq , (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The .dcd output file can become ridiculously HUGE if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs! The last few line in the configuration file: ## Minimize, reinitialize velocities, run dynamics minimize 500 run 10000 tell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. ( This is a very short example! ). Typically you might set \"run\" to 10,000,000 or more. Press \"q\" to quit viewing the configuration file. Check again on the status of your job: showq -u <username> If you don't see anything it probably means the job has finished. List your directory using the command ls -lrt and you should see something like: [mike@snowy Namd_simple_example_01]$ ls -rlt total 17088 -rw-r--r-- 1 mike VR0021 243622 Dec 8 11:40 par_all27_prot_na.prm -rw-r--r-- 1 mike VR0021 655 Dec 8 11:40 sbatch_namd_start_bluegene -rw-r--r-- 1 mike VR0021 3932 Dec 8 11:40 namd_1ubq_example.conf -rw-r--r-- 1 mike VR0021 814960 Dec 8 11:40 1ubq_example.pdb drwxr-xr-x 2 mike VR0021 512 Dec 8 11:40 BUILD_DIR -rw-r--r-- 1 mike VR0021 700 Dec 8 11:40 sbatch_namd_restartjob_bluegene -rw-r--r-- 1 mike VR0021 1182412 Dec 8 11:40 1ubq_example.psf -rw-r--r-- 1 mike VR0021 4508 Dec 8 11:40 namd_1ubq_restart_example.conf -rw-r--r-- 1 mike VR0021 159 Dec 8 11:40 slurm-2746442.out -rw-r--r-- 1 mike VR0021 1371 Dec 8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.coor.old -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.vel.old -rw-r--r-- 1 mike VR0021 216 Dec 8 11:41 1ubq_output.restart.xsc.old -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.coor -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.vel -rw-r--r-- 1 mike VR0021 218 Dec 8 11:42 1ubq_output.restart.xsc -rw-r--r-- 1 mike VR0021 8417 Dec 8 11:42 1ubq_output.xst -rw-r--r-- 1 mike VR0021 13005576 Dec 8 11:42 1ubq_output.dcd -rw-r--r-- 1 mike VR0021 215 Dec 8 11:42 1ubq_output.xsc -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.coor -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.vel -rw-r--r-- 1 mike VR0021 425681 Dec 8 11:43 Namd_1ubq_example_output.txt The highlighted .dcd file is the main output file while the .xsc , .coor , .vel files all have to do with being able to restart the simulation at a later date, while the Namd_1ubq_example_output.txt file contains the text output from the simulation. Congratulations! You have just run a short molecular dynamics simulation on the cluster. Next, we'll copy that information back to your local computer and use VMD to visualize the results. Now on to part 2, visualizing the results with VMD. 3 - Visualizing NAMD results with VMD In this section you will be using the molecular visualization program VMD to look at the trajectory data of the ubiquitin protein you generated in the first part of the tutorial. If you haven't already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux). Tip : the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the Snowy cluster using the program ssh . You can usually tell which computer you are logged into by the terminal command line: for example the terminal command line: [mike@snowy Namd_simple_example_01]$ tells me I am logged into Snowy , in the Namd_simple_example_01 directory. Compared to my local terminal command line: mike@axion:~$ tells me I am on my local machine (called \"axion\" in this case). Download the entire NAMD example directory back to your local computer . For example, in Linux you can type in your local computer terminal: (if you see snowy in the command line prompt you are typing in the wrong terminal!) scp -r <username>@snowy.vlsci.unimelb.edu.au:Namd_simple_example_01 . What to do if your simulations didn\u2019t run. If for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal: scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_simple_example_01_finished . You can now start VMD locally and load up the trajectory data. In a new local terminal type: vmd Note : On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer. Two new windows should pop up. The main panel : The display : And the terminal should turn into the console : a) Reading structure data files into VMD The first file you need to read into VMD is the protein structure file, (1ubq_example.psf in this case). The .psf file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information. From the main panel : File \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load You should see nothing in the display, but an entry in the Main panel. Next load the coordinates from the .pdb file. First, select the 1ubq_example.psf entry in the VMD main panel, then: File \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load Now you should have the model in the display that can be moved around with the mouse. This is the initial starting position of the simulation. Next load in the trajectory data into VMD (again, select the entry in the VMD main panel): File \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load This data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the main panel . (Use the speed scroll bar to the left of that button to slow it down). What you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases. From the main panel you can bring up the graphical representations window to play with more rendering types: - try them out! Graphics \u2192 Representations And this conclude the basic tutorial to running a simple job on a cluster. Wasn\u2019t so scary now was it? Please play around with VMD. Once you feel comfortable, try start the next tutorial: Molecular dynamics - Building input files","title":"Molecular Dynamics - Introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#molecular-dynamics-tutorial-introduction-to-cluster-computing","text":"","title":"Molecular Dynamics Tutorial - Introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#overview","text":"In the following tutorials we will be logging on to a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing (this tutorial) : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.","title":"Overview"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#learning-objectives","text":"At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results","title":"Learning Objectives"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#requirements","text":"This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select 'Software agreement', 'Add software', 'NAMD'. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat.","title":"Requirements"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#1-overview","text":"The aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs. The program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the Theoretical and Computational Biophysics Group at Illinois University at Urbana Champaign . It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output.","title":"1 - Overview"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#2-basic-introduction-to-cluster-computing","text":"","title":"2 - Basic introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-logging-in-to-the-cluster","text":"Using a terminal on your local computer and your username and password, login to the HPC cluster. ssh <username>@snowy.vlsci.unimelb.edu.au You should see a welcome screen and a command line prompt. If you type ls at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet! Note : be careful to use the right terminal when you are typing in commands! Sometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example: [<username>@snowy ~]$ <- tells you your terminal is on Snowy","title":"a) Logging in to the cluster"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#b-copy-across-files-starting-the-job","text":"We\u2019ll need to copy across the basic example directory to our working directory on Snowy . Do this with: cp -r /vlsci/examples/namd/Namd_simple_example_01 . Note that the dot is important! Change into this directory and launch the job with the command sbatch and the sbatch script. cd Namd_simple_example_01 Then type: sbatch sbatch_namd_start_job Your job has now been submitted to the cluster. Easy hey? Check the job is running with the showq command. (type it at the command line). Too much information? Try: showq -u <username> This particular job is very short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the .dcd suffix.","title":"b) Copy across files, starting the job"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#c-understanding-the-input-files","text":"While we wait, let's take a look at the sbatch example script to understand what is going on. Type: less sbatch_namd_start_job less is a Unix file viewer. Press \u201cq\u201d to quit the viewing of the file You should see the lines: #SBATCH --nodes=4 This line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC. Now let us have a look at the NAMD configuration script: less namd_1ubq_example.conf Woah! There is quite a bit of information here, don't worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top: structure 1ubq_example.psf coordinates 1ubq_example.pdb outputName 1ubq_output_01 These are simply defining the input files, (the protein structure file .psf , and the coordinate file, .pdb ) and also the name of the output files. Further down you will see: set Temp 310 temperature $Temp Which is setting the temperature to 310 K (37 C) while below that we have: ## Parameter file paraTypeCharmm on parameters par_all27_prot_na.prm which tells NAMD which parameter file to use. (you'll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file. Somewhere in the middle you will see these lines: ## Periodic Boundary Conditions cellBasisVector1 48. 0. 0. cellBasisVector2 0. 48. 0. cellBasisVector3 0. 0. 48. cellOrigin 0 0 0 wrapAll on wrapWater on This defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other. Near the bottom we have the lines: ## Output files restartfreq 5000 dcdfreq 100 xstFreq 100 outputEnergies 100 outputPressure 100 outputTiming 100 These lines tell us how often to write out to the output files. The most important is the dcdfreq , (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The .dcd output file can become ridiculously HUGE if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs! The last few line in the configuration file: ## Minimize, reinitialize velocities, run dynamics minimize 500 run 10000 tell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. ( This is a very short example! ). Typically you might set \"run\" to 10,000,000 or more. Press \"q\" to quit viewing the configuration file. Check again on the status of your job: showq -u <username> If you don't see anything it probably means the job has finished. List your directory using the command ls -lrt and you should see something like: [mike@snowy Namd_simple_example_01]$ ls -rlt total 17088 -rw-r--r-- 1 mike VR0021 243622 Dec 8 11:40 par_all27_prot_na.prm -rw-r--r-- 1 mike VR0021 655 Dec 8 11:40 sbatch_namd_start_bluegene -rw-r--r-- 1 mike VR0021 3932 Dec 8 11:40 namd_1ubq_example.conf -rw-r--r-- 1 mike VR0021 814960 Dec 8 11:40 1ubq_example.pdb drwxr-xr-x 2 mike VR0021 512 Dec 8 11:40 BUILD_DIR -rw-r--r-- 1 mike VR0021 700 Dec 8 11:40 sbatch_namd_restartjob_bluegene -rw-r--r-- 1 mike VR0021 1182412 Dec 8 11:40 1ubq_example.psf -rw-r--r-- 1 mike VR0021 4508 Dec 8 11:40 namd_1ubq_restart_example.conf -rw-r--r-- 1 mike VR0021 159 Dec 8 11:40 slurm-2746442.out -rw-r--r-- 1 mike VR0021 1371 Dec 8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.coor.old -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.vel.old -rw-r--r-- 1 mike VR0021 216 Dec 8 11:41 1ubq_output.restart.xsc.old -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.coor -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.vel -rw-r--r-- 1 mike VR0021 218 Dec 8 11:42 1ubq_output.restart.xsc -rw-r--r-- 1 mike VR0021 8417 Dec 8 11:42 1ubq_output.xst -rw-r--r-- 1 mike VR0021 13005576 Dec 8 11:42 1ubq_output.dcd -rw-r--r-- 1 mike VR0021 215 Dec 8 11:42 1ubq_output.xsc -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.coor -rw-r--r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.vel -rw-r--r-- 1 mike VR0021 425681 Dec 8 11:43 Namd_1ubq_example_output.txt The highlighted .dcd file is the main output file while the .xsc , .coor , .vel files all have to do with being able to restart the simulation at a later date, while the Namd_1ubq_example_output.txt file contains the text output from the simulation. Congratulations! You have just run a short molecular dynamics simulation on the cluster. Next, we'll copy that information back to your local computer and use VMD to visualize the results. Now on to part 2, visualizing the results with VMD.","title":"c) Understanding the input files"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#3-visualizing-namd-results-with-vmd","text":"In this section you will be using the molecular visualization program VMD to look at the trajectory data of the ubiquitin protein you generated in the first part of the tutorial. If you haven't already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux). Tip : the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the Snowy cluster using the program ssh . You can usually tell which computer you are logged into by the terminal command line: for example the terminal command line: [mike@snowy Namd_simple_example_01]$ tells me I am logged into Snowy , in the Namd_simple_example_01 directory. Compared to my local terminal command line: mike@axion:~$ tells me I am on my local machine (called \"axion\" in this case). Download the entire NAMD example directory back to your local computer . For example, in Linux you can type in your local computer terminal: (if you see snowy in the command line prompt you are typing in the wrong terminal!) scp -r <username>@snowy.vlsci.unimelb.edu.au:Namd_simple_example_01 . What to do if your simulations didn\u2019t run. If for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal: scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_simple_example_01_finished . You can now start VMD locally and load up the trajectory data. In a new local terminal type: vmd Note : On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer. Two new windows should pop up. The main panel : The display : And the terminal should turn into the console :","title":"3 - Visualizing NAMD results with VMD"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-reading-structure-data-files-into-vmd","text":"The first file you need to read into VMD is the protein structure file, (1ubq_example.psf in this case). The .psf file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information. From the main panel : File \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load You should see nothing in the display, but an entry in the Main panel. Next load the coordinates from the .pdb file. First, select the 1ubq_example.psf entry in the VMD main panel, then: File \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load Now you should have the model in the display that can be moved around with the mouse. This is the initial starting position of the simulation. Next load in the trajectory data into VMD (again, select the entry in the VMD main panel): File \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load This data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the main panel . (Use the speed scroll bar to the left of that button to slow it down). What you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases. From the main panel you can bring up the graphical representations window to play with more rendering types: - try them out! Graphics \u2192 Representations And this conclude the basic tutorial to running a simple job on a cluster. Wasn\u2019t so scary now was it? Please play around with VMD. Once you feel comfortable, try start the next tutorial: Molecular dynamics - Building input files","title":"a) Reading structure data files into VMD"},{"location":"tutorials/molecular_dynamics_201/","text":"PR reviewers and advice: Thomas Coudrat Current slides: https://drive.google.com/open?id=1cJoL7WI-GHIr2iMFm_R9lXrYXiM4kdTBujzpUJH8x0c Other slides: None yet","title":"Home"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/","text":"Molecular Dynamics Tutorial - Building input files, visualising the trajectory Overview In the following tutorials we will be logging on a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files (this tutorial) : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD. Learning Objectives At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results Requirements This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select 'Software agreement', 'Add software', 'NAMD'. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat. 1 - Overview The aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in our introductory molecular dynamics tutorial . Tip : in conjunction with this tutorial there are some excellent NAMD tutorials available that are worth working through. 2 - NAMD overview a) Recap: what we've done so far In the previous introductory tutorial we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution. To run a molecular dynamics simulation on a cluster, the minimum files we need are: <filename>.psf - protein structure file . A list of the atoms, masses, charges and connections between atoms. <filename>.pdb - protein database file . The actual starting coordinates of the models. This has to be the same order as the psf file. <filename>.conf - NAMD configuration file . Tells NAMD how to run the job. par_all27_prot_na.par - a parameter file . (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). It is used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms. sbatch_batchfile - a script file to launch the job on the cluster depending on the scheduler used (i.e. PBS or Slurm). Tells the cluster how to run the NAMD job and how many cores to use. In our introductory tutorial all we had to do was launch the job. We will now go through the process of building a NAMD input files from scratch. b) Building NAMD input files overview In order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated! To generate NAMD input files, we will use the psfgen module within VMD, together with pdb and topology files , to generate a new pdb file and psf file. In a flowchart, the process looks something like this: c) The Namd_intermediate_template directory structure Note : one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the introductory tutorial you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect outputs. In the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories. This directory is found on Snowy, under /vlsci/examples/namd/Namd_intermediate_template . It has a particular directory structure as follows: /Namd_intermediate_template sbatch_start \u2190 script to start equilibration phase sbatch_production \u2190 script to start production phase sim_opt.conf \u2190 Namd configuration file for optimization sim_production.conf \u2190 Namd configuration file for production run project_plan.txt \u2190 A guide to thinking about your simulation counter.txt \u2190 File for keeping track of job number max_jobnumber.txt \u2190 Defines maximum number of jobs /BUILD_DIR \u2190 this is where we will build our models /Errors \u2190 errors go here /Examples \u2190 find some example files here /InputFiles \u2190 our input files go here /Parameters /JobLog \u2190 details of our running jobs end up here /LastRestart \u2190 If we need to restart a job /OutputFiles \u2190 Our NAMD output goes here /OutputText \u2190 Text output from the job /RestartFiles \u2190 Generic restart files saved here Rather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching: sbatch sbatch_start (don't do this just yet!) This will launch a job using the sim_opt.conf configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run: sbatch sbatch_production Production runs are designed to run less than 24 hours at a time, at the end of which restart files are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a counter.txt file by 1. Once that reaches the number in the MaxJobNumber.txt file, the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we'd set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure. The script also date stamps and directs the output to the appropriate folders. A complete dcd trajectory can be reconstructed from the ordered files in the /OutputFile directory. More of running a job later. First we need to build input files! 3 - Building a HIV protease model Download a copy of the Namd_intermediate_template to your local computer . We will prepare our files here and then upload them to the cluster once ready. scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template . Note : don't forget the dot at the end Change into the build directory: cd Namd_intermediate_template/BUILD_DIR We are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the RCSB entry 7hvp . Download the pdb file by clicking on \"Download Files\" (right hand corner), PDB Format. On your local computer start VMD and load the newly downloaded 7hvp.pdb file by doing the following in the VMD main panel : File \u2192 New Molecule \u2192 Browse... \u2192 Load In this structure there are 3 \u201cchains\u201d. Chain A and B are the monomers and chain C is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective. Highlight the protein selection in the VMD main panel and then click: File \u2192 save coordinates In the \u201cselected atoms\u201d box of the save trajectory window type: chain A and protein Save the chain as a pdb file with a sensible name and a pdb extension into BUILD_DIR . (e.g. 7hvp_chainA.pdb) Repeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise. We should now have two pdb files in the /BUILD_DIR which will be the basis of our model building, i.e.: 7hvp_chainA.pdb 7hvp_chainB.pdb We now have to use a text editor to change the build_example script to point to these files. Open the build_example file with a text editor. First thing to look for is that we are calling the right topology files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids). package require psfgen topology ../InputFiles/Parameters/top_all27_prot_na.rtf If we were to be building a model with a lipid bilayer we would need to also include the topology file referring to lipids, i.e.: topology ../InputFiles/Parameter/top_all27_prot_lipid.rtf We now need to change and add \u201csegment\u201d lines to point to our new pdb chains. Edit: segment A {pdb model_chainA.pdb} segment B {pdb model_chainB.pdb} to read: segment A {pdb 7hvp_chainA.pdb} segment B {pdb 7hvp_chainB.pdb} We also need to change the \u201ccoordpdb\u201d lines to reflect our new chains. Edit: coordpdb model_chainA.pdb A coordpdb model_chainB.pdb B to read: coordpdb 7hvp_chainA.pdb A coordpdb 7hvp_chainB.pdb B Between the 'segment' and 'coordpdb' lines we can apply patches to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won't make any modifications for this example. Save and close the build_example script. We will now see if we can build the model using VMD from the command line. Type: vmd -dispdev text -e build_example You should see some errors. This is because in the original chains A and B there are some modified alanine residues labeled ABA . Since the residues ABA are not defined in the topology files, vmd psfgen does not know how to build this model. Edit the 7hvp_chainA.pdb and 7hvp_chainB.pdb files and carefully change any occurrence of ABA to ALA . Note : spacing in pdb files is really important so don't mess it up! Re-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory: model_temp_x.psf model_temp_x.pdb Note : here we use a \u201c_x\u201d notation to specify temporary files. Next load these files into VMD. From BUILD_DIR start vmd: vmd model_temp_x.psf model_temp_x.pdb We will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of 80 x 64 x 64 \u00c5. Open the solvation window from the main panel: Extensions \u2192 Modeling \u2192 Add Solvation Box In this window do the following: toggle on the \u201crotate to minimize volume\u201d button. Change the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d untoggle the 'use molecular Dimensions\u201d button. In the Box Size add: min: x: -40 y: -32 z: -32 max: x: 40 y: 32 z: 32 click \u201cSolvate\u201d We now should have two new files, solvate.psf and solvate.pdb , the solvated version of your original input. You should also your newly solvated system in the VMD display . Tip : you can quickly hide \"model_temp_x.psf\" and \"model_temp_x.pdb\" from the VMD display by double-clicking on 'D' (Drawn) next to these in the VMD main panel. This helps visualise the solvate.psf system. Now we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window: Extensions \u2192 Modeling \u2192 add ions In the \u201cAutoionize\u201d window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files: ionized.psf and ionized.pdb . These are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command: mv ionized.psf ../InputFiles/hiv_protease.psf and mv ionized.pdb ../InputFiles/hiv_protease.pdb You can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy. 4 - Preparing the configuration files By now we have prepared two new input files for a NAMD simulation called hiv_protease.psf and hiv_protease.pdb and placed them in the folder /InputFiles . We now need to make changes to the NAMD configuration files to match our new inputs. In the main directory (Namd_intermediate_template) we have two configuration files and two sbatch files: sim_opt.conf sim_production.conf sbatch_start sbatch_production a) Edit the .conf files Let us first edit the .conf files. Open the sim_opt.conf file. You can see that these two lines match our inputs (change them if you used a different name of the psf and pdb files): structure InputFiles/hiv_protease.psf coordinates InputFiles/hiv_protease.pdb The next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine: paraTypeCharmm on parameters InputFiles/Parameters/par_all27_prot_na.prm If we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example: parameters InputFiles/Parameters/par_all27_prot_lipid.prm parameters InputFiles/Parameters/par_for_ligand.prm We also need to make changes to match our periodic boundary conditions (PBC). The way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn't interact with itself should it wander too close to a boundary. Since our box ended up being of dimensions 80 x 64 x 64 \u00c5, this is reflected in the PBC parameters here: cellBasisVector1 80. 0. 0. cellBasisVector2 0. 64. 0. cellBasisVector3 0. 0. 64. cellOrigin 0 0 0 That should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase. The sim_production.conf file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file MaxJobNumber.txt determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time. Open the sim_production.conf file: Since we would like to run a relatively short job segment for this exercise, we will change the line: set NumberSteps 2500 to: set NumberSteps 20000 This segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation. Edit the counter.txt file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use: echo 0 > counter.txt Then edit the MaxJobNumber.txt file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value. echo 5 > MaxJobNumber.txt We can always make this number bigger later and restart the jobs if we want things to go longer. For this short example we will also change more lines in the sim_production.conf file: restartfreq 2500 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 to: restartfreq 25000 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 We don't have to change the periodic boundary conditions in the sim_production.conf file as we read in the restart files from the previous simulation namely: set inputname generic_restartfile binCoordinates $inputname.coor ; # Coordinates from last run (binary) binVelocities $inputname.vel ; # Velocities from last run (binary) extendedSystem $inputname.xsc ; # Cell dimensions from last run There are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don't change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values. Save and close your .conf files. b) Edit the sbatch scripts The sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated than the ones we saw in the introductory tutorial , but most of the details you need to worry about are all in the first few lines. In a sbatch script we need to pass arguments to the Slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line: #SBATCH --nodes=2 \u2190 this works! # SBATCH --nodes=2 \u2190 this doesn't because of the space between \u201c#\u201d and \u201cSBATCH\u201d Note : people often get confused with this as the \u201c # \u201d symbol usually denotes a comment line. PBS scripts work in a similar way, but with the code word \u201c #PBS \u201d Set the number of nodes used for a job on sbatch_start and sbatch_production to 4, as shown below: #SBATCH --nodes=4 Remember, more nodes is not necessarily faster and can be dramatically slower! It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine. To set the production job runtime change this line on sbatch_production : #SBATCH --time=2:0:0 \u2190 (hours:minutes:seconds) The time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus 10%. If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster (but make sure your walltime is appropriate for your configuration file!). 5 - Launching the job on the cluster We are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make. Upload the entire directory to your account. Under Linux this might be: scp -r Namd_intermediate_template <username>@snowy.vlsci.unimelb.edu.au: Log into your account on Snowy and change into the top of the Namd_intermediate_template/ directory: ssh <username>@snowy.vlsci.unimelb.edu.au Launch the start script: sbatch sbatch_start This should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase. This might take an hour or two to complete . All text output is directed to the /OutputText folder. You can take a peek at how your job is going by using the command tail <filename> which prints out the last few lines of <filename> . For the purpose of this exercise , we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file. It will look something like this: slurm-123456.out The number represents the job number on the cluster. Now use scancel to stop that job (i.e. for above you would use: scancel 123456 ). scancel <jobnumber> Now that your job has finished, we will copy across a completed job run. From your top directory on Snowy: cp -r /vlsci/examples/namd/Namd_intermediate_template_finished/* Namd_intermediate_template/ Once the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our local computer for analysis. If you don't have much memory on your laptop, you can do the analysis remotely on the cluster. A smart way to copy files back to your local computer is to use rsync . This way you only copy new or changed files back to your computer. In Linux from the local computer terminal this would be: rsync -avzt <username>@snowy.vlsci.unimelb.edu.au:Namd_intermediate_template . Note : the dot is important! Now that you have your data, we are ready to visualize the results. 6 - Visualization of the MD trajectory Hopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local computer. We will now have a look at the data you generated. Note : if for some reason you didn't manage to run a successful MD simulation, you can copy a directory containing the precomputed data from the folowing Snowy folder: /vlsci/examples/namd/Namd_intermediate_template_finished . You can do this with the command below: scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template_finished . a) Combining the trajectory files When we run segmented jobs as in this template, we end up with a series of output files in /OutputFiles such as: [train01@snowy OutputFiles]$ ls -lrt total 13184 -rwxr-xr-x 1 train01 TRAINING 1477 Mar 21 10:18 create_dcd_loader_script -rw-r--r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd -rw-r--r-- 1 train01 TRAINING 195 Mar 21 11:15 dcd_list.txt -rw-r--r-- 1 train01 TRAINING 742 Mar 21 11:15 combined_dcd_file_loader.vmd The main output files have the .dcd extension. We can see that things went well as the sizes of these files are identical as expected. If you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run: ./create_dcd_loader_script This creates the file: combined_dcd_file_loader.vmd From the main directory on your local computer, we can load in our trajectory using: vmd InputFiles/hiv_protease.psf InputFiles/hiv_protease.pdb then from the main panel: File \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd Click on the \"play\" button at the bottom right hand corner of the VMD main panel and watch the simulation run! Note : it is possible to restart the simulations of any segment as the restart files are saved under /RestartFiles . b) Molecular dynamics trajectory smoothing The MD example presented here has not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult. Luckily, there is an easy way to center and visualize our simulations which we will cover next. Now display only the protein backbone, in the VMD main panel: Graphics \u2192 Representations... In the graphical representations window: Selected Atoms (protein) + Drawing method (NewRibbons) You may notice the protein jiggles around when you play the trajectory. This is Brownian motion, and this is more prominent in longer sampling. The first thing we might try to ease the jiggling is to increase the trajectory smoothing window size . In the VMD Graphical representations window, select your protein representation and toggle the Trajectory tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values. Although this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box. c) Centering the protein for analysis We will now use the RMSD Trajectory Tool to center our protein frames. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 RMSD Trajectory Tool This should open up a new window. Towards the top left we have the selection, for the 'Selection modifiers' tick 'Backbone'. In the top right, click \u201cRMSD\u201d . When you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much your selection drifts through space. At this point nothing has changed in the trajectory yet. Next, click the \u201cALIGN\u201d button. This will translate each frame to minimize the RMSD of the selection based on the first frame (in this case, our original input files). In other words, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis. Click \u201cRMSD\u201d again and you'll see the value becomes much smaller. d) Using Volmap to map ligand density. Now that we have a nicely centered protein dataset we can do something useful like plot the water density. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 VolMap Tool A new VolMap window should open up. In the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d, click \"Create Map\". This will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical representation window and select the new \u201cIsosurface\u201d representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points (try 1.2). What you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don't display waters in simulations for clarity, and often forget that they are there. If all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation. Exercise : see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail. You can also do this sort of view for ligands to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense! So concludes the intermediate tutorial. Note : a more advanced template that can be used to organise the MD simulations ran on HPC clusters called MD_workflow_py was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.","title":"Molecular Dynamics - Building input files, visualising the trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#molecular-dynamics-tutorial-building-input-files-visualising-the-trajectory","text":"","title":"Molecular Dynamics Tutorial - Building input files, visualising the trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#overview","text":"In the following tutorials we will be logging on a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files (this tutorial) : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.","title":"Overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#learning-objectives","text":"At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results","title":"Learning Objectives"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#requirements","text":"This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select 'Software agreement', 'Add software', 'NAMD'. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat.","title":"Requirements"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#1-overview","text":"The aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in our introductory molecular dynamics tutorial . Tip : in conjunction with this tutorial there are some excellent NAMD tutorials available that are worth working through.","title":"1 - Overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#2-namd-overview","text":"","title":"2 - NAMD overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-recap-what-weve-done-so-far","text":"In the previous introductory tutorial we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution. To run a molecular dynamics simulation on a cluster, the minimum files we need are: <filename>.psf - protein structure file . A list of the atoms, masses, charges and connections between atoms. <filename>.pdb - protein database file . The actual starting coordinates of the models. This has to be the same order as the psf file. <filename>.conf - NAMD configuration file . Tells NAMD how to run the job. par_all27_prot_na.par - a parameter file . (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). It is used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms. sbatch_batchfile - a script file to launch the job on the cluster depending on the scheduler used (i.e. PBS or Slurm). Tells the cluster how to run the NAMD job and how many cores to use. In our introductory tutorial all we had to do was launch the job. We will now go through the process of building a NAMD input files from scratch.","title":"a) Recap: what we've done so far"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-building-namd-input-files-overview","text":"In order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated! To generate NAMD input files, we will use the psfgen module within VMD, together with pdb and topology files , to generate a new pdb file and psf file. In a flowchart, the process looks something like this:","title":"b) Building NAMD input files overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#c-the-namd_intermediate_template-directory-structure","text":"Note : one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the introductory tutorial you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect outputs. In the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories. This directory is found on Snowy, under /vlsci/examples/namd/Namd_intermediate_template . It has a particular directory structure as follows: /Namd_intermediate_template sbatch_start \u2190 script to start equilibration phase sbatch_production \u2190 script to start production phase sim_opt.conf \u2190 Namd configuration file for optimization sim_production.conf \u2190 Namd configuration file for production run project_plan.txt \u2190 A guide to thinking about your simulation counter.txt \u2190 File for keeping track of job number max_jobnumber.txt \u2190 Defines maximum number of jobs /BUILD_DIR \u2190 this is where we will build our models /Errors \u2190 errors go here /Examples \u2190 find some example files here /InputFiles \u2190 our input files go here /Parameters /JobLog \u2190 details of our running jobs end up here /LastRestart \u2190 If we need to restart a job /OutputFiles \u2190 Our NAMD output goes here /OutputText \u2190 Text output from the job /RestartFiles \u2190 Generic restart files saved here Rather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching: sbatch sbatch_start (don't do this just yet!) This will launch a job using the sim_opt.conf configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run: sbatch sbatch_production Production runs are designed to run less than 24 hours at a time, at the end of which restart files are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a counter.txt file by 1. Once that reaches the number in the MaxJobNumber.txt file, the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we'd set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure. The script also date stamps and directs the output to the appropriate folders. A complete dcd trajectory can be reconstructed from the ordered files in the /OutputFile directory. More of running a job later. First we need to build input files!","title":"c) The Namd_intermediate_template directory structure"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#3-building-a-hiv-protease-model","text":"Download a copy of the Namd_intermediate_template to your local computer . We will prepare our files here and then upload them to the cluster once ready. scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template . Note : don't forget the dot at the end Change into the build directory: cd Namd_intermediate_template/BUILD_DIR We are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the RCSB entry 7hvp . Download the pdb file by clicking on \"Download Files\" (right hand corner), PDB Format. On your local computer start VMD and load the newly downloaded 7hvp.pdb file by doing the following in the VMD main panel : File \u2192 New Molecule \u2192 Browse... \u2192 Load In this structure there are 3 \u201cchains\u201d. Chain A and B are the monomers and chain C is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective. Highlight the protein selection in the VMD main panel and then click: File \u2192 save coordinates In the \u201cselected atoms\u201d box of the save trajectory window type: chain A and protein Save the chain as a pdb file with a sensible name and a pdb extension into BUILD_DIR . (e.g. 7hvp_chainA.pdb) Repeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise. We should now have two pdb files in the /BUILD_DIR which will be the basis of our model building, i.e.: 7hvp_chainA.pdb 7hvp_chainB.pdb We now have to use a text editor to change the build_example script to point to these files. Open the build_example file with a text editor. First thing to look for is that we are calling the right topology files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids). package require psfgen topology ../InputFiles/Parameters/top_all27_prot_na.rtf If we were to be building a model with a lipid bilayer we would need to also include the topology file referring to lipids, i.e.: topology ../InputFiles/Parameter/top_all27_prot_lipid.rtf We now need to change and add \u201csegment\u201d lines to point to our new pdb chains. Edit: segment A {pdb model_chainA.pdb} segment B {pdb model_chainB.pdb} to read: segment A {pdb 7hvp_chainA.pdb} segment B {pdb 7hvp_chainB.pdb} We also need to change the \u201ccoordpdb\u201d lines to reflect our new chains. Edit: coordpdb model_chainA.pdb A coordpdb model_chainB.pdb B to read: coordpdb 7hvp_chainA.pdb A coordpdb 7hvp_chainB.pdb B Between the 'segment' and 'coordpdb' lines we can apply patches to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won't make any modifications for this example. Save and close the build_example script. We will now see if we can build the model using VMD from the command line. Type: vmd -dispdev text -e build_example You should see some errors. This is because in the original chains A and B there are some modified alanine residues labeled ABA . Since the residues ABA are not defined in the topology files, vmd psfgen does not know how to build this model. Edit the 7hvp_chainA.pdb and 7hvp_chainB.pdb files and carefully change any occurrence of ABA to ALA . Note : spacing in pdb files is really important so don't mess it up! Re-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory: model_temp_x.psf model_temp_x.pdb Note : here we use a \u201c_x\u201d notation to specify temporary files. Next load these files into VMD. From BUILD_DIR start vmd: vmd model_temp_x.psf model_temp_x.pdb We will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of 80 x 64 x 64 \u00c5. Open the solvation window from the main panel: Extensions \u2192 Modeling \u2192 Add Solvation Box In this window do the following: toggle on the \u201crotate to minimize volume\u201d button. Change the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d untoggle the 'use molecular Dimensions\u201d button. In the Box Size add: min: x: -40 y: -32 z: -32 max: x: 40 y: 32 z: 32 click \u201cSolvate\u201d We now should have two new files, solvate.psf and solvate.pdb , the solvated version of your original input. You should also your newly solvated system in the VMD display . Tip : you can quickly hide \"model_temp_x.psf\" and \"model_temp_x.pdb\" from the VMD display by double-clicking on 'D' (Drawn) next to these in the VMD main panel. This helps visualise the solvate.psf system. Now we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window: Extensions \u2192 Modeling \u2192 add ions In the \u201cAutoionize\u201d window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files: ionized.psf and ionized.pdb . These are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command: mv ionized.psf ../InputFiles/hiv_protease.psf and mv ionized.pdb ../InputFiles/hiv_protease.pdb You can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy.","title":"3 - Building a HIV protease model"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#4-preparing-the-configuration-files","text":"By now we have prepared two new input files for a NAMD simulation called hiv_protease.psf and hiv_protease.pdb and placed them in the folder /InputFiles . We now need to make changes to the NAMD configuration files to match our new inputs. In the main directory (Namd_intermediate_template) we have two configuration files and two sbatch files: sim_opt.conf sim_production.conf sbatch_start sbatch_production","title":"4 - Preparing the configuration files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-edit-the-conf-files","text":"Let us first edit the .conf files. Open the sim_opt.conf file. You can see that these two lines match our inputs (change them if you used a different name of the psf and pdb files): structure InputFiles/hiv_protease.psf coordinates InputFiles/hiv_protease.pdb The next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine: paraTypeCharmm on parameters InputFiles/Parameters/par_all27_prot_na.prm If we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example: parameters InputFiles/Parameters/par_all27_prot_lipid.prm parameters InputFiles/Parameters/par_for_ligand.prm We also need to make changes to match our periodic boundary conditions (PBC). The way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn't interact with itself should it wander too close to a boundary. Since our box ended up being of dimensions 80 x 64 x 64 \u00c5, this is reflected in the PBC parameters here: cellBasisVector1 80. 0. 0. cellBasisVector2 0. 64. 0. cellBasisVector3 0. 0. 64. cellOrigin 0 0 0 That should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase. The sim_production.conf file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file MaxJobNumber.txt determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time. Open the sim_production.conf file: Since we would like to run a relatively short job segment for this exercise, we will change the line: set NumberSteps 2500 to: set NumberSteps 20000 This segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation. Edit the counter.txt file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use: echo 0 > counter.txt Then edit the MaxJobNumber.txt file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value. echo 5 > MaxJobNumber.txt We can always make this number bigger later and restart the jobs if we want things to go longer. For this short example we will also change more lines in the sim_production.conf file: restartfreq 2500 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 to: restartfreq 25000 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 We don't have to change the periodic boundary conditions in the sim_production.conf file as we read in the restart files from the previous simulation namely: set inputname generic_restartfile binCoordinates $inputname.coor ; # Coordinates from last run (binary) binVelocities $inputname.vel ; # Velocities from last run (binary) extendedSystem $inputname.xsc ; # Cell dimensions from last run There are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don't change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values. Save and close your .conf files.","title":"a) Edit the .conf files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-edit-the-sbatch-scripts","text":"The sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated than the ones we saw in the introductory tutorial , but most of the details you need to worry about are all in the first few lines. In a sbatch script we need to pass arguments to the Slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line: #SBATCH --nodes=2 \u2190 this works! # SBATCH --nodes=2 \u2190 this doesn't because of the space between \u201c#\u201d and \u201cSBATCH\u201d Note : people often get confused with this as the \u201c # \u201d symbol usually denotes a comment line. PBS scripts work in a similar way, but with the code word \u201c #PBS \u201d Set the number of nodes used for a job on sbatch_start and sbatch_production to 4, as shown below: #SBATCH --nodes=4 Remember, more nodes is not necessarily faster and can be dramatically slower! It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine. To set the production job runtime change this line on sbatch_production : #SBATCH --time=2:0:0 \u2190 (hours:minutes:seconds) The time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus 10%. If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster (but make sure your walltime is appropriate for your configuration file!).","title":"b) Edit the sbatch scripts"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#5-launching-the-job-on-the-cluster","text":"We are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make. Upload the entire directory to your account. Under Linux this might be: scp -r Namd_intermediate_template <username>@snowy.vlsci.unimelb.edu.au: Log into your account on Snowy and change into the top of the Namd_intermediate_template/ directory: ssh <username>@snowy.vlsci.unimelb.edu.au Launch the start script: sbatch sbatch_start This should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase. This might take an hour or two to complete . All text output is directed to the /OutputText folder. You can take a peek at how your job is going by using the command tail <filename> which prints out the last few lines of <filename> . For the purpose of this exercise , we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file. It will look something like this: slurm-123456.out The number represents the job number on the cluster. Now use scancel to stop that job (i.e. for above you would use: scancel 123456 ). scancel <jobnumber> Now that your job has finished, we will copy across a completed job run. From your top directory on Snowy: cp -r /vlsci/examples/namd/Namd_intermediate_template_finished/* Namd_intermediate_template/ Once the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our local computer for analysis. If you don't have much memory on your laptop, you can do the analysis remotely on the cluster. A smart way to copy files back to your local computer is to use rsync . This way you only copy new or changed files back to your computer. In Linux from the local computer terminal this would be: rsync -avzt <username>@snowy.vlsci.unimelb.edu.au:Namd_intermediate_template . Note : the dot is important! Now that you have your data, we are ready to visualize the results.","title":"5 - Launching the job on the cluster"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#6-visualization-of-the-md-trajectory","text":"Hopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local computer. We will now have a look at the data you generated. Note : if for some reason you didn't manage to run a successful MD simulation, you can copy a directory containing the precomputed data from the folowing Snowy folder: /vlsci/examples/namd/Namd_intermediate_template_finished . You can do this with the command below: scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template_finished .","title":"6 - Visualization of the MD trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-combining-the-trajectory-files","text":"When we run segmented jobs as in this template, we end up with a series of output files in /OutputFiles such as: [train01@snowy OutputFiles]$ ls -lrt total 13184 -rwxr-xr-x 1 train01 TRAINING 1477 Mar 21 10:18 create_dcd_loader_script -rw-r--r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd -rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd -rw-r--r-- 1 train01 TRAINING 195 Mar 21 11:15 dcd_list.txt -rw-r--r-- 1 train01 TRAINING 742 Mar 21 11:15 combined_dcd_file_loader.vmd The main output files have the .dcd extension. We can see that things went well as the sizes of these files are identical as expected. If you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run: ./create_dcd_loader_script This creates the file: combined_dcd_file_loader.vmd From the main directory on your local computer, we can load in our trajectory using: vmd InputFiles/hiv_protease.psf InputFiles/hiv_protease.pdb then from the main panel: File \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd Click on the \"play\" button at the bottom right hand corner of the VMD main panel and watch the simulation run! Note : it is possible to restart the simulations of any segment as the restart files are saved under /RestartFiles .","title":"a) Combining the trajectory files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-molecular-dynamics-trajectory-smoothing","text":"The MD example presented here has not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult. Luckily, there is an easy way to center and visualize our simulations which we will cover next. Now display only the protein backbone, in the VMD main panel: Graphics \u2192 Representations... In the graphical representations window: Selected Atoms (protein) + Drawing method (NewRibbons) You may notice the protein jiggles around when you play the trajectory. This is Brownian motion, and this is more prominent in longer sampling. The first thing we might try to ease the jiggling is to increase the trajectory smoothing window size . In the VMD Graphical representations window, select your protein representation and toggle the Trajectory tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values. Although this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box.","title":"b) Molecular dynamics trajectory smoothing"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#c-centering-the-protein-for-analysis","text":"We will now use the RMSD Trajectory Tool to center our protein frames. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 RMSD Trajectory Tool This should open up a new window. Towards the top left we have the selection, for the 'Selection modifiers' tick 'Backbone'. In the top right, click \u201cRMSD\u201d . When you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much your selection drifts through space. At this point nothing has changed in the trajectory yet. Next, click the \u201cALIGN\u201d button. This will translate each frame to minimize the RMSD of the selection based on the first frame (in this case, our original input files). In other words, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis. Click \u201cRMSD\u201d again and you'll see the value becomes much smaller.","title":"c) Centering the protein for analysis"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#d-using-volmap-to-map-ligand-density","text":"Now that we have a nicely centered protein dataset we can do something useful like plot the water density. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 VolMap Tool A new VolMap window should open up. In the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d, click \"Create Map\". This will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical representation window and select the new \u201cIsosurface\u201d representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points (try 1.2). What you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don't display waters in simulations for clarity, and often forget that they are there. If all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation. Exercise : see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail. You can also do this sort of view for ligands to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense! So concludes the intermediate tutorial. Note : a more advanced template that can be used to organise the MD simulations ran on HPC clusters called MD_workflow_py was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.","title":"d) Using Volmap to map ligand density."},{"location":"tutorials/ngs_overview/","text":"PR reviewers and advice: Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/ngs_overview/NGS_Overview/","text":"Next Generation Sequencing Overview Sequencing technologies Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction) Applications Genomic Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a 'metasample') Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target Epigenomic CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment Transcriptomic (RNA-seq) Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis Types of outcome Variation Detection Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements De Novo assembly Concatemers (length?) Comparative gene expression Matrix of gene vs expression level Splicing detection List of alternative splice isoforms Resequencing alignment strategies Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex) Categories of analysis NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - 'base calling'. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis. Sample preparation Fragment Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read Paired-end sequencing Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus 'related' and have extra information about the structure of the sample. The information is that the two reads from the single fragment are 'close' - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn Mate-pair Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn Barcoded fragments Multiple samples are prepared independently. Each sample is 'labelled' with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes. Enrichment for target sequences PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation Potential services in NGS Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data Appendix Paired end sequencing - empirical observations paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, \\~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all. Mate-pair sequencing Illumina refers to \"paired end\" as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the 'internal' ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard 'paired end' sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"NGS Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#next-generation-sequencing-overview","text":"","title":"Next Generation Sequencing Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#sequencing-technologies","text":"Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction)","title":"Sequencing technologies"},{"location":"tutorials/ngs_overview/NGS_Overview/#applications","text":"","title":"Applications"},{"location":"tutorials/ngs_overview/NGS_Overview/#genomic","text":"Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a 'metasample') Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target","title":"Genomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#epigenomic","text":"CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment","title":"Epigenomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#transcriptomic-rna-seq","text":"Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis","title":"Transcriptomic (RNA-seq)"},{"location":"tutorials/ngs_overview/NGS_Overview/#types-of-outcome","text":"","title":"Types of outcome"},{"location":"tutorials/ngs_overview/NGS_Overview/#variation-detection","text":"Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements","title":"Variation Detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#de-novo-assembly","text":"Concatemers (length?)","title":"De Novo assembly"},{"location":"tutorials/ngs_overview/NGS_Overview/#comparative-gene-expression","text":"Matrix of gene vs expression level","title":"Comparative gene expression"},{"location":"tutorials/ngs_overview/NGS_Overview/#splicing-detection","text":"List of alternative splice isoforms","title":"Splicing detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#resequencing-alignment-strategies","text":"Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex)","title":"Resequencing alignment strategies"},{"location":"tutorials/ngs_overview/NGS_Overview/#categories-of-analysis","text":"NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - 'base calling'. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis.","title":"Categories of analysis"},{"location":"tutorials/ngs_overview/NGS_Overview/#sample-preparation","text":"","title":"Sample preparation"},{"location":"tutorials/ngs_overview/NGS_Overview/#fragment","text":"Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read","title":"Fragment"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing","text":"Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus 'related' and have extra information about the structure of the sample. The information is that the two reads from the single fragment are 'close' - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn","title":"Paired-end sequencing"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair","text":"Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn","title":"Mate-pair"},{"location":"tutorials/ngs_overview/NGS_Overview/#barcoded-fragments","text":"Multiple samples are prepared independently. Each sample is 'labelled' with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes.","title":"Barcoded fragments"},{"location":"tutorials/ngs_overview/NGS_Overview/#enrichment-for-target-sequences","text":"PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation","title":"Enrichment for target sequences"},{"location":"tutorials/ngs_overview/NGS_Overview/#potential-services-in-ngs","text":"Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data","title":"Potential services in NGS"},{"location":"tutorials/ngs_overview/NGS_Overview/#appendix","text":"","title":"Appendix"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing-empirical-observations","text":"paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, \\~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all.","title":"Paired end sequencing - empirical observations"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair-sequencing","text":"Illumina refers to \"paired end\" as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the 'internal' ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard 'paired end' sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"Mate-pair sequencing"},{"location":"tutorials/pacbio/","text":"Long read assembly workshop This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data. Overview Simplified version of workflow: 1. Get started Your workshop trainers will provide you with the address of a virtual machine. Mac users Open the Terminal. Type in ssh researcher@[your virtual machine address] Type in the password provided. Windows users If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under \"Host Name (or IP address)\" enter in the address of your virtual machine. Under \"Port\" type in 22 Under \"Connection Type\" select \"SSH\" Click \"Open\" Under \"Login as:\" enter \"researcher\" Type in the password provided. Activate the conda environment Type in: source /mnt/gvl/apps/conda/bin/activate (This points us to some different directories for the software we need). Create a new working directory on your remote computer. Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called \"Workshop\" mkdir Workshop Change to that directory cd Workshop NOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again. Find your current directory by typing: pwd 2. Get data The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the \"Download\" button, and select \"Copy link address\". Back in your terminal, enter wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files. Shorten each of these files names with the mv command: mv R1.fq\\?download\\=1 R1.fq mv R2.fq\\?download\\=1 R2.fq mv pacbio.fq\\?download\\=1 pacbio.fq Type in ls to check the files are present and correctly-named. We should have R1.fq , R2.fq and pacbio.fq . 3. Assemble We will use the assembly software called Canu , version 1.7. Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=0.03m corThreads=3 -pacbio-raw pacbio.fq the first canu tells the program to run -p canu names prefix for output files (\"canu\") -d canu_outdir names output directory (\"canu_outdir\") genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). corThreads=3 sets the number of available threads. Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say \"Finished\" but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row. 4. Check assembly output Move into the canu output folder: cd canu_outdir View the list of files: ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 47997 \"tig00000001\" is the name given to the contig \"47997\" is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: less canu.report \"less\" is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report. Questions How do long- and short-read assembly methods differ? Answer (click to reveal) Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Where can we find out the what the approximate genome size should be for the species being assembled? Answer (click to reveal) Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. In the assembly output, what are the unassembled reads? Answer (click to reveal) Reads and low-coverage contigs that were not used in the assembly. What are the corrected reads? How did canu correct the reads? Answer (click to reveal) Canu builds overlaps between reads. The consensus is used to correct the reads. Where could you view the output .gfa and what would it show? Answer (click to reveal) A useful program is [Bandage](https://rrwick.github.io/Bandage/). If the assembly has multiple contigs, the assembly graph shows how these are connected. 5. Trim and circularise Bacteria have circular chromosomes. Because of sequencing errors, there may be some \"overhang\" in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: cd .. Run Circlator circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \"Circularized x of x contig(s)\". Check the output Move into the Circlator output directory: cd circlator_outdir List the files: ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? less 04.merge.circularise.log \"less\" is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? infoseq 06.fixstart.fasta The contig \"tig00000001\" has a length of 30019. This is about 18,000 bases shorter than before circularisation. This was the \"overhang\" and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: cp 06.fixstart.fasta ../contig1.fasta Move back into the main folder: cd .. Questions Were all the contigs circularised? Answer (click to reveal) In this example, yes, the contig was circularised. Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer (click to reveal) Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this. 6. Find smaller plasmids Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn't map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim. Align Illumina reads to the PacBio contig Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam bwa mem is the alignment tool -t 4 is the number of cores contig1.fasta is the input assembly file R1.fq R2.fq are the Illumina reads | samtools sort pipes the output to samtools to sort > aln.bam sends the alignment to the file aln.bam Extract unmapped Illumina reads Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \"unmapped\" files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq . Assemble the unmapped reads Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \"off\") -o is the output directory Move into the output directory: cd spades_assembly Look at the contigs: infoseq contigs.fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: cp contigs.fasta contig2.fasta Trim the plasmid To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta > contig2.fa.head head -n 10 takes the first ten lines of contig2.fasta > sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl makeblastdb makes a database for the tool Blast This will generate three new files in the directory with suffixes .nhr, .nin and .nsq -in sets the input file as contig2.fasta -dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls blastn is the tool Blast, set as blast n to compare sequences of nucleotides to each other -query sets the input sequence as contig2.fa.head -db sets the database as that of the original sequence contig2.fasta . We don't have to specify the other files that were created when we formatted this file, but they need to present in our current directory. -evalue is the number of hits expected by chance, here set as 1e-3 -dust no turns off the masking of low-complexity regions -out sets the output file as contig2.bls Look at the hits (the matches): less contig2.bls The first hit is at the start, as expected. We can see that \"Query 1\" (the start of the contig) is aligned to \"Sbject 1\" (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows \"Query 1\" (the start of the contig) also matches to \"Sbject 1\" (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: nano contig2.fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won't work.) At the first line, delete the text, which will be something like \">NODE_1_length_2359_cov_3.320333\" Type in \">contig2\" Don't forget the > symbol Press Control-X \"Save modified buffer ?\" - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): samtools faidx contig2.fasta faidx means index the fasta file Trim the contig: samtools faidx contig2.fasta contig2:1-2252 > plasmid.fasta this extracts contig2 from position 1-2252 > plasmid.fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: cp plasmid.fasta ../ Move file back into main folder: cd .. Collect contigs Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): cat contig1.fasta plasmid.fasta > genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 30019 plasmid: 2252 Questions Why is this section so complicated? Answer (click to reveal) Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Why can PacBio sequencing miss small plasmids? Answer (click to reveal) Library prep size selection. We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer (click to reveal) Repeats that have mapped to the PacBio assembly. How do you find a plasmid in a Bandage graph? Answer (click to reveal) It is probably circular, matches the size of a known plasmid, and has a rep gene. Are there easier ways to find plasmids? Answer (click to reveal) Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler 7. Correct the assembly Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly. Make an alignment file Index the fasta file: bwa index genome.fasta Align the Illumina reads: bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) > pilon_aln.bam redirects the sorted bam to this file Index the files: samtools index pilon_aln.bam samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam Run Pilon Run: pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: less pilon1.changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 30059 (net +40 bases) plasmid - 2252 (no change) Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid. Questions Why don't we correct earlier in the assembly process? Answer (click to reveal) We need to circularise the contigs and trim overhangs first. Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer (click to reveal) Illumina reads have higher accuracy. Could we just use PacBio reads to assemble the genome? Answer (click to reveal) Yes, if accuracy adequate. 8. Comparative Genomics In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine a complete bacterial genome. Assemblies This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Does it matter that an assembly is in many contigs? Answer (click to reveal) Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure (*e.g.* the number of plasmids) and the location of genes of interest (*e.g.* gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information). Annotations Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Why are there more CDS identified in the PacBio assembly? Answer (click to reveal) The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Why are there more rRNA identified in the PacBio assembly? Answer (click to reveal) There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly. 9. Summary In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/ and https://galaxy-au-training.github.io/tutorials/","title":"Long read assembly"},{"location":"tutorials/pacbio/#long-read-assembly-workshop","text":"This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data.","title":"Long read assembly workshop"},{"location":"tutorials/pacbio/#overview","text":"Simplified version of workflow:","title":"Overview"},{"location":"tutorials/pacbio/#1-get-started","text":"Your workshop trainers will provide you with the address of a virtual machine.","title":"1. Get started"},{"location":"tutorials/pacbio/#mac-users","text":"Open the Terminal. Type in ssh researcher@[your virtual machine address] Type in the password provided.","title":"Mac users"},{"location":"tutorials/pacbio/#windows-users","text":"If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under \"Host Name (or IP address)\" enter in the address of your virtual machine. Under \"Port\" type in 22 Under \"Connection Type\" select \"SSH\" Click \"Open\" Under \"Login as:\" enter \"researcher\" Type in the password provided.","title":"Windows users"},{"location":"tutorials/pacbio/#activate-the-conda-environment","text":"Type in: source /mnt/gvl/apps/conda/bin/activate (This points us to some different directories for the software we need).","title":"Activate the conda environment"},{"location":"tutorials/pacbio/#create-a-new-working-directory-on-your-remote-computer","text":"Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called \"Workshop\" mkdir Workshop Change to that directory cd Workshop NOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again. Find your current directory by typing: pwd","title":"Create a new working directory on your remote computer."},{"location":"tutorials/pacbio/#2-get-data","text":"The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the \"Download\" button, and select \"Copy link address\". Back in your terminal, enter wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files. Shorten each of these files names with the mv command: mv R1.fq\\?download\\=1 R1.fq mv R2.fq\\?download\\=1 R2.fq mv pacbio.fq\\?download\\=1 pacbio.fq Type in ls to check the files are present and correctly-named. We should have R1.fq , R2.fq and pacbio.fq .","title":"2. Get data"},{"location":"tutorials/pacbio/#3-assemble","text":"We will use the assembly software called Canu , version 1.7. Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=0.03m corThreads=3 -pacbio-raw pacbio.fq the first canu tells the program to run -p canu names prefix for output files (\"canu\") -d canu_outdir names output directory (\"canu_outdir\") genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). corThreads=3 sets the number of available threads. Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say \"Finished\" but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row.","title":"3. Assemble"},{"location":"tutorials/pacbio/#4-check-assembly-output","text":"Move into the canu output folder: cd canu_outdir View the list of files: ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 47997 \"tig00000001\" is the name given to the contig \"47997\" is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: less canu.report \"less\" is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report.","title":"4. Check assembly output"},{"location":"tutorials/pacbio/#questions","text":"How do long- and short-read assembly methods differ? Answer (click to reveal) Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Where can we find out the what the approximate genome size should be for the species being assembled? Answer (click to reveal) Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. In the assembly output, what are the unassembled reads? Answer (click to reveal) Reads and low-coverage contigs that were not used in the assembly. What are the corrected reads? How did canu correct the reads? Answer (click to reveal) Canu builds overlaps between reads. The consensus is used to correct the reads. Where could you view the output .gfa and what would it show? Answer (click to reveal) A useful program is [Bandage](https://rrwick.github.io/Bandage/). If the assembly has multiple contigs, the assembly graph shows how these are connected.","title":"Questions"},{"location":"tutorials/pacbio/#5-trim-and-circularise","text":"Bacteria have circular chromosomes. Because of sequencing errors, there may be some \"overhang\" in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: cd ..","title":"5. Trim and circularise"},{"location":"tutorials/pacbio/#run-circlator","text":"circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \"Circularized x of x contig(s)\".","title":"Run Circlator"},{"location":"tutorials/pacbio/#check-the-output","text":"Move into the Circlator output directory: cd circlator_outdir List the files: ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? less 04.merge.circularise.log \"less\" is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? infoseq 06.fixstart.fasta The contig \"tig00000001\" has a length of 30019. This is about 18,000 bases shorter than before circularisation. This was the \"overhang\" and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: cp 06.fixstart.fasta ../contig1.fasta Move back into the main folder: cd ..","title":"Check the output"},{"location":"tutorials/pacbio/#questions_1","text":"Were all the contigs circularised? Answer (click to reveal) In this example, yes, the contig was circularised. Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer (click to reveal) Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.","title":"Questions"},{"location":"tutorials/pacbio/#6-find-smaller-plasmids","text":"Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn't map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim.","title":"6. Find smaller plasmids"},{"location":"tutorials/pacbio/#align-illumina-reads-to-the-pacbio-contig","text":"Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam bwa mem is the alignment tool -t 4 is the number of cores contig1.fasta is the input assembly file R1.fq R2.fq are the Illumina reads | samtools sort pipes the output to samtools to sort > aln.bam sends the alignment to the file aln.bam","title":"Align Illumina reads to the PacBio contig"},{"location":"tutorials/pacbio/#extract-unmapped-illumina-reads","text":"Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \"unmapped\" files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq .","title":"Extract unmapped Illumina reads"},{"location":"tutorials/pacbio/#assemble-the-unmapped-reads","text":"Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \"off\") -o is the output directory Move into the output directory: cd spades_assembly Look at the contigs: infoseq contigs.fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: cp contigs.fasta contig2.fasta","title":"Assemble the unmapped reads"},{"location":"tutorials/pacbio/#trim-the-plasmid","text":"To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta > contig2.fa.head head -n 10 takes the first ten lines of contig2.fasta > sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl makeblastdb makes a database for the tool Blast This will generate three new files in the directory with suffixes .nhr, .nin and .nsq -in sets the input file as contig2.fasta -dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls blastn is the tool Blast, set as blast n to compare sequences of nucleotides to each other -query sets the input sequence as contig2.fa.head -db sets the database as that of the original sequence contig2.fasta . We don't have to specify the other files that were created when we formatted this file, but they need to present in our current directory. -evalue is the number of hits expected by chance, here set as 1e-3 -dust no turns off the masking of low-complexity regions -out sets the output file as contig2.bls Look at the hits (the matches): less contig2.bls The first hit is at the start, as expected. We can see that \"Query 1\" (the start of the contig) is aligned to \"Sbject 1\" (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows \"Query 1\" (the start of the contig) also matches to \"Sbject 1\" (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: nano contig2.fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won't work.) At the first line, delete the text, which will be something like \">NODE_1_length_2359_cov_3.320333\" Type in \">contig2\" Don't forget the > symbol Press Control-X \"Save modified buffer ?\" - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): samtools faidx contig2.fasta faidx means index the fasta file Trim the contig: samtools faidx contig2.fasta contig2:1-2252 > plasmid.fasta this extracts contig2 from position 1-2252 > plasmid.fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: cp plasmid.fasta ../ Move file back into main folder: cd ..","title":"Trim the plasmid"},{"location":"tutorials/pacbio/#collect-contigs","text":"Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): cat contig1.fasta plasmid.fasta > genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 30019 plasmid: 2252","title":"Collect contigs"},{"location":"tutorials/pacbio/#questions_2","text":"Why is this section so complicated? Answer (click to reveal) Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Why can PacBio sequencing miss small plasmids? Answer (click to reveal) Library prep size selection. We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer (click to reveal) Repeats that have mapped to the PacBio assembly. How do you find a plasmid in a Bandage graph? Answer (click to reveal) It is probably circular, matches the size of a known plasmid, and has a rep gene. Are there easier ways to find plasmids? Answer (click to reveal) Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler","title":"Questions"},{"location":"tutorials/pacbio/#7-correct-the-assembly","text":"Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly.","title":"7. Correct the assembly"},{"location":"tutorials/pacbio/#make-an-alignment-file","text":"Index the fasta file: bwa index genome.fasta Align the Illumina reads: bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) > pilon_aln.bam redirects the sorted bam to this file Index the files: samtools index pilon_aln.bam samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam","title":"Make an alignment file"},{"location":"tutorials/pacbio/#run-pilon","text":"Run: pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: less pilon1.changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 30059 (net +40 bases) plasmid - 2252 (no change) Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.","title":"Run Pilon"},{"location":"tutorials/pacbio/#questions_3","text":"Why don't we correct earlier in the assembly process? Answer (click to reveal) We need to circularise the contigs and trim overhangs first. Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer (click to reveal) Illumina reads have higher accuracy. Could we just use PacBio reads to assemble the genome? Answer (click to reveal) Yes, if accuracy adequate.","title":"Questions"},{"location":"tutorials/pacbio/#8-comparative-genomics","text":"In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine a complete bacterial genome.","title":"8. Comparative Genomics"},{"location":"tutorials/pacbio/#assemblies","text":"This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Does it matter that an assembly is in many contigs? Answer (click to reveal) Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure (*e.g.* the number of plasmids) and the location of genes of interest (*e.g.* gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information).","title":"Assemblies"},{"location":"tutorials/pacbio/#annotations","text":"Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Why are there more CDS identified in the PacBio assembly? Answer (click to reveal) The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Why are there more rRNA identified in the PacBio assembly? Answer (click to reveal) There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly.","title":"Annotations"},{"location":"tutorials/pacbio/#9-summary","text":"In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/ and https://galaxy-au-training.github.io/tutorials/","title":"9. Summary"},{"location":"tutorials/proteomics_basic/","text":"PR reviewers and advice: Ira Cooke Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/proteomics_basic/background_data_formats/","text":"Data Formats and Pre-processing Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats Background"},{"location":"tutorials/proteomics_basic/background_data_formats/#data-formats-and-pre-processing","text":"Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats and Pre-processing"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/","text":"Create a workflow Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create find count items workflow"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/#create-a-workflow","text":"Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create a workflow"},{"location":"tutorials/proteomics_basic/background_galaxy/","text":"How to use Galaxy This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki . Create a new History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \"Unnamed History\". Click its title to rename it. Rename a history item Locate the item in your history and click its pencil icon Enter a new name in the Name: field and click Save Find someone's exact username Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username Share a History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user's full username Import a History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name Copy Datasets Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories Multi File Inputs Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected. Saved Histories To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"Galaxy Background"},{"location":"tutorials/proteomics_basic/background_galaxy/#how-to-use-galaxy","text":"This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki .","title":"How to use Galaxy"},{"location":"tutorials/proteomics_basic/background_galaxy/#create-a-new-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \"Unnamed History\". Click its title to rename it.","title":"Create a new History"},{"location":"tutorials/proteomics_basic/background_galaxy/#rename-a-history-item","text":"Locate the item in your history and click its pencil icon Enter a new name in the Name: field and click Save","title":"Rename a history item"},{"location":"tutorials/proteomics_basic/background_galaxy/#find-someones-exact-username","text":"Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username","title":"Find someone's exact username"},{"location":"tutorials/proteomics_basic/background_galaxy/#share-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user's full username","title":"Share a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#import-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name","title":"Import a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#copy-datasets","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories","title":"Copy Datasets"},{"location":"tutorials/proteomics_basic/background_galaxy/#multi-file-inputs","text":"Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected.","title":"Multi File Inputs"},{"location":"tutorials/proteomics_basic/background_galaxy/#saved-histories","text":"To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"Saved Histories"},{"location":"tutorials/proteomics_basic/background_protein_databases/","text":"Protein Databases In a perfect experiment we would obtain fragment ions for all the b,y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database. Large vs Small Database Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue. Typical sources of data for search databases Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome. Should I include decoys? Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to 'pin down' the negative distribution.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#protein-databases","text":"In a perfect experiment we would obtain fragment ions for all the b,y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#large-vs-small-database","text":"Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue.","title":"Large vs Small Database"},{"location":"tutorials/proteomics_basic/background_protein_databases/#typical-sources-of-data-for-search-databases","text":"Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome.","title":"Typical sources of data for search databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#should-i-include-decoys","text":"Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to 'pin down' the negative distribution.","title":"Should I include decoys?"},{"location":"tutorials/proteomics_basic/background_protein_prophet/","text":"Protein Prophet The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It's citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let's look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1.0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr|Q3UN47|Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp|O08600|NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1.0 ) but probabilities group members are different. The first member of the group has a high probability 0.99 but all other members have probabilities of 0.0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_protein_prophet/#protein-prophet","text":"The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It's citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let's look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1.0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr|Q3UN47|Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp|O08600|NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1.0 ) but probabilities group members are different. The first member of the group has a high probability 0.99 but all other members have probabilities of 0.0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_search_engines/","text":"How Search Engines Work When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it's old, I recommend this presentation by Brian Searle","title":"Search Engines"},{"location":"tutorials/proteomics_basic/background_search_engines/#how-search-engines-work","text":"When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it's old, I recommend this presentation by Brian Searle","title":"How Search Engines Work"},{"location":"tutorials/proteomics_basic/proteomics_basic/","text":"Identifying proteins from mass spec data Overview This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together Login to Galaxy Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server Import mass spec data Create a new history in Galaxy and name it \"Organelle Tutorial\" Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \"Edit attributes\". This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \"/\" Your item should then be named OrganelleSample.mzML Dont forget to click \"Save\" Basic properties of the data Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\". There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \"MS1 spectrum\" in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); if ($(\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate data formats Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); if ($(\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Obtain a Search Database Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; sp|Q9CQV8|1433B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Run a search using X!Tandem A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716.fasta MSMS File OrganelleSample.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0.5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine's work . Convert Results to tabular format Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Sort tabular outputs Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys. Convert raw scores to probabilities Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to Peptide Prophet is Percolator . If you're curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \"greater than or equal to\" in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. c11>=0.95 To answer the second question use the Select tool on the filtered table to select lines matching \"decoy_\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); if ($(\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); if ($(\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Perform Protein Inference Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we've viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings. Functional enrichment analysis This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \"pipes\" that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Tutorial"},{"location":"tutorials/proteomics_basic/proteomics_basic/#identifying-proteins-from-mass-spec-data","text":"","title":"Identifying proteins from mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#overview","text":"This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together","title":"Overview"},{"location":"tutorials/proteomics_basic/proteomics_basic/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server","title":"Login to Galaxy"},{"location":"tutorials/proteomics_basic/proteomics_basic/#import-mass-spec-data","text":"Create a new history in Galaxy and name it \"Organelle Tutorial\" Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \"Edit attributes\". This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \"/\" Your item should then be named OrganelleSample.mzML Dont forget to click \"Save\"","title":"Import mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#basic-properties-of-the-data","text":"Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\". There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \"MS1 spectrum\" in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); if ($(\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Basic properties of the data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#alternate-data-formats","text":"Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); if ($(\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Alternate data formats"},{"location":"tutorials/proteomics_basic/proteomics_basic/#obtain-a-search-database","text":"Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v1/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; sp|Q9CQV8|1433B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Obtain a Search Database"},{"location":"tutorials/proteomics_basic/proteomics_basic/#run-a-search-using-xtandem","text":"A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716.fasta MSMS File OrganelleSample.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0.5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine's work .","title":"Run a search using X!Tandem"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-results-to-tabular-format","text":"Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert Results to tabular format"},{"location":"tutorials/proteomics_basic/proteomics_basic/#sort-tabular-outputs","text":"Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys.","title":"Sort tabular outputs"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-raw-scores-to-probabilities","text":"Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to Peptide Prophet is Percolator . If you're curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \"greater than or equal to\" in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. c11>=0.95 To answer the second question use the Select tool on the filtered table to select lines matching \"decoy_\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); if ($(\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); if ($(\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert raw scores to probabilities"},{"location":"tutorials/proteomics_basic/proteomics_basic/#perform-protein-inference","text":"Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we've viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.","title":"Perform Protein Inference"},{"location":"tutorials/proteomics_basic/proteomics_basic/#functional-enrichment-analysis","text":"This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \"pipes\" that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Functional enrichment analysis"},{"location":"tutorials/python_overview/","text":"PR reviewers and advice: Bernard Pope Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/python_overview/python_overview/","text":"Authors: Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne General information Python modules are stored in files containing a \".py\" suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used. Python 2 versus Python 3 Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary. Indentation for grouping code blocks Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code. Style Guide A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: # Compute factorial of n, # assuming n >= 0 def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result print(factorial(10)) C program for computing factorial: #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial(int n) { int result = 1; while (n > 0) { result *= n; n -= 1; } return result; } int main(void) { printf(\"%d\\n\", factorial(10)); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer). Comments Program comments start with a hash character \"#\" and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. ''' Running a Python program There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented. Objects and types Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \".\" operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type(x) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): >>> # Create a list, assign to the variable x >>> x = [3, 1, 2, 3] >>> # Ask for the type of the value assigned to x >>> type(x) <class 'list'> >>> # Ask for the type of the first item in the list (an integer) >>> type(x[0]) <class 'int'> >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x.count(3) 2 >>> # Sort the contents of the list in-place. >>> # Note that this mutates the list object! >>> # Also note that Python does not print the result in this case. >>> x.sort() >>> # Ask Python to show the value of the list >>> # assigned to the variable x (note it is now sorted) >>> x [1, 2, 3, 3] >>> # Assign x to an object of a different type (a float) >>> x = 3.142 >>> type(x) <class 'float'> Booleans Represent truth values Values: True , False Type: bool Operators: and , or , not bool(x) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0.0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: >>> not True False >>> not False True >>> not () True >>> not [1,2,3] False >>> True and False False >>> True and () () Conditional Statements Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: if expression: statement-block elif expression: statement-block ... else: statement-block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: >>> if []: ... print(\"Was considered True\") ... else: ... print(\"Was considered False\") ... Was considered False Numbers and basic mathematics Integers Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means (4 * 10) + 2 ) Hexadecimal literals start with 0x , octal literals start with 0o , binary literals start with 0b . int(x) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0x10 16 >>> 0b10 2 >>> -0 == 0 True >>> int(\"123\") 123 >>> int(\"3.142\") Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ValueError: invalid literal for int() with base 10: '3.142' Floating Point Numbers Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys.float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3.142 Exponential: 314.2e-2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float(x) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: >>> type(3.142) <class 'float'> >>> type(12) <class 'int'> >>> 3.142 + 12 15.142 >>> 3.142 == 314.2e-2 True >>> 3. == 3.0 True >>> 1/0 Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ZeroDivisionError: division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3.3333333333333335 >>> float(\"123\") 123.0 >>> float(\"3.142\") 3.142 Complex Numbers Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real +/- imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: >>> 5j + 3j 8j >>> 2-5j (2-5j) >>> 2-5j + 3j (2-2j) Numeric Operators Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): >>> 3 + 4 * 5 23 >>> (3 + 4) * 5 35 >>> 10 / 3 3.3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> (2 ** 3) ** 4 4096 Strings Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn't going to work though' File \"<stdin>\", line 1 'this isn't going to work though' ^ SyntaxError: invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\n newline \\t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s[5] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): >>> type(\"hello\") <class 'str'> >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\".upper() 'BONJOUR' >>> len(\"bonjour\") 7 >>> \"bonjour\".startswith(\"b\") True >>> \"cat,sat,flat\".split(\",\") ['cat', 'sat', 'flat'] >>> # Print the first 5 Chinese unicode characters >>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04') \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x[0] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: # Prompt the user to input a string: input = raw_input(\"Enter string: \") # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input: if char in vowels: count += 1 # Print the count to the standard output print(count) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels.py : python vowels.py Enter string: abracadabra 5 Lists Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [1, 2, 3] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x[3] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: >>> type([1, 2, 3]) <class 'list'> >>> x = [] >>> len(x) 0 >>> x.append(\"hello\") >>> x ['hello'] >>> len(x) 1 >>> x[0] 'hello' >>> x.insert(0, True) >>> x [True, 'hello'] >>> del x[1] >>> x [True] >>> x += [42, \"Newton\", 3.142] >>> x [True, 42, 'Newton', 3.142] Dictionaries Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. {12: \"XII\", 6: \"VI\"} Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: >>> type({12: \"XII\", 6: \"VI\"}) <class 'dict'> >>> friends = {} >>> friends['Fred'] = ['Barney', 'Dino'] >>> friends {'Fred': ['Barney', 'Dino']} >>> friends['Fred'] ['Barney', 'Dino'] >>> friends['Barney'] Traceback (most recent call last): File \"\\<stdin\\>\", line 1, in \\<module\\> KeyError: 'Barney' >>> friends['Wilma'] = ['Betty'] >>> friends {'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']} >>> friends.keys() dict_keys(['Fred', 'Wilma']) >>> friends.values() dict_values([['Barney', 'Dino'], ['Betty']]) >>> 'Dino' in friends False Example program: # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys.stdin: # Parse the next input as an integer next_integer = int(line) # Update the histogram accordingly if next_integer in histogram: # We've seen this integer before histogram[next_integer] += 1 else: # First occurrence of this integer in the input histogram[next_integer] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted(histogram): print(\"{} {}\".format(key, histogram[key]))s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo.py : python histo.py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 3 43 12 19 3 12 12 43 Program prints its output: 3 2 12 3 19 1 43 2 Tuples Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: (1, 2, 3) The can be used as keys in dictionaries (unlike lists). Loops While loops Iterate until condition is False Syntax: while expression: statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result For loops Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: for variable in expression: statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range() function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: def factorial(n): result = 1 for item in range(n + 1): result *= item return result Break and continue Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly. Functions Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: def variable(parameter_list): statement_block Anonymous function syntax: lambda parameter_list: expression Example: def is_leap_year(year): if year % 4 == 0 and year % 100 != 0: return True else: return year % 400 == 0 for year in range(2000, 2100 + 1): result = is_leap_year(year) print(\"{} {}\".format(year, result)) Anonymous function example: >>> squared = lambda x: x ** 2 >>> squared(2) 4 >>> list(map(lambda x: x + 1, [1, 2, 3])) [2, 3, 4] Input and output The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys.argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys.argv[1] # Open the file file = open(filename) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file: num_lines += 1 num_words += len(line.split()) file.close() print(\"Number of lines and words in {}: {} {}\" \\ .format(filename, num_lines, num_words)) Advanced Topics Classes Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: class variable(superclass_list): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: class Vector(object): def __init__(self, x=0, y=0, z=0): self.x = x self.y = y self.z = z def magnitude(self): return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) def normalise(self): magnitude = self.magnitude() if magnitude == 0: # Somehow we have a degenerate vector. return self else: return self / self.magnitude() def angle(self, other): dp = self.dot_product(other) return acos(dp / self.magnitude() * other.magnitude()) def dot_product(self, other): return self.x * other.x + self.y * other.y + self.z * other.z Exceptions Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: try: statement_block except exception_type as variable: statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: # alternative version of the histogram code from the section on # dictionaries for line in sys.stdin: next_integer = int(line) try: histogram[next_integer] += 1 except KeyError: histogram[next_integer] = 1 Modules A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: >>> import math >>> math.sqrt(100) 10.0 >>> sqrt(100) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NameError: name 'sqrt' is not defined >>> from math import sqrt >>> sqrt(100) 10.0 >>> import math as m >>> m.sqrt(100) 10.0 Packages A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \"sandboxed\" Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Python Overview"},{"location":"tutorials/python_overview/python_overview/#authors","text":"Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne","title":"Authors:"},{"location":"tutorials/python_overview/python_overview/#general-information","text":"Python modules are stored in files containing a \".py\" suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used.","title":"General information"},{"location":"tutorials/python_overview/python_overview/#python-2-versus-python-3","text":"Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary.","title":"Python 2 versus Python 3"},{"location":"tutorials/python_overview/python_overview/#indentation-for-grouping-code-blocks","text":"Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.","title":"Indentation for grouping code blocks"},{"location":"tutorials/python_overview/python_overview/#style-guide","text":"A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: # Compute factorial of n, # assuming n >= 0 def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result print(factorial(10)) C program for computing factorial: #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial(int n) { int result = 1; while (n > 0) { result *= n; n -= 1; } return result; } int main(void) { printf(\"%d\\n\", factorial(10)); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer).","title":"Style Guide"},{"location":"tutorials/python_overview/python_overview/#comments","text":"Program comments start with a hash character \"#\" and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. '''","title":"Comments"},{"location":"tutorials/python_overview/python_overview/#running-a-python-program","text":"There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented.","title":"Running a Python program"},{"location":"tutorials/python_overview/python_overview/#objects-and-types","text":"Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \".\" operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type(x) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): >>> # Create a list, assign to the variable x >>> x = [3, 1, 2, 3] >>> # Ask for the type of the value assigned to x >>> type(x) <class 'list'> >>> # Ask for the type of the first item in the list (an integer) >>> type(x[0]) <class 'int'> >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x.count(3) 2 >>> # Sort the contents of the list in-place. >>> # Note that this mutates the list object! >>> # Also note that Python does not print the result in this case. >>> x.sort() >>> # Ask Python to show the value of the list >>> # assigned to the variable x (note it is now sorted) >>> x [1, 2, 3, 3] >>> # Assign x to an object of a different type (a float) >>> x = 3.142 >>> type(x) <class 'float'>","title":"Objects and types"},{"location":"tutorials/python_overview/python_overview/#booleans","text":"Represent truth values Values: True , False Type: bool Operators: and , or , not bool(x) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0.0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: >>> not True False >>> not False True >>> not () True >>> not [1,2,3] False >>> True and False False >>> True and () ()","title":"Booleans"},{"location":"tutorials/python_overview/python_overview/#conditional-statements","text":"Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: if expression: statement-block elif expression: statement-block ... else: statement-block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: >>> if []: ... print(\"Was considered True\") ... else: ... print(\"Was considered False\") ... Was considered False","title":"Conditional Statements"},{"location":"tutorials/python_overview/python_overview/#numbers-and-basic-mathematics","text":"","title":"Numbers and basic mathematics"},{"location":"tutorials/python_overview/python_overview/#integers","text":"Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means (4 * 10) + 2 ) Hexadecimal literals start with 0x , octal literals start with 0o , binary literals start with 0b . int(x) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0x10 16 >>> 0b10 2 >>> -0 == 0 True >>> int(\"123\") 123 >>> int(\"3.142\") Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ValueError: invalid literal for int() with base 10: '3.142'","title":"Integers"},{"location":"tutorials/python_overview/python_overview/#floating-point-numbers","text":"Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys.float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3.142 Exponential: 314.2e-2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float(x) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: >>> type(3.142) <class 'float'> >>> type(12) <class 'int'> >>> 3.142 + 12 15.142 >>> 3.142 == 314.2e-2 True >>> 3. == 3.0 True >>> 1/0 Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ZeroDivisionError: division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3.3333333333333335 >>> float(\"123\") 123.0 >>> float(\"3.142\") 3.142","title":"Floating Point Numbers"},{"location":"tutorials/python_overview/python_overview/#complex-numbers","text":"Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real +/- imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: >>> 5j + 3j 8j >>> 2-5j (2-5j) >>> 2-5j + 3j (2-2j)","title":"Complex Numbers"},{"location":"tutorials/python_overview/python_overview/#numeric-operators","text":"Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): >>> 3 + 4 * 5 23 >>> (3 + 4) * 5 35 >>> 10 / 3 3.3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> (2 ** 3) ** 4 4096","title":"Numeric Operators"},{"location":"tutorials/python_overview/python_overview/#strings","text":"Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn't going to work though' File \"<stdin>\", line 1 'this isn't going to work though' ^ SyntaxError: invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\n newline \\t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s[5] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): >>> type(\"hello\") <class 'str'> >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\".upper() 'BONJOUR' >>> len(\"bonjour\") 7 >>> \"bonjour\".startswith(\"b\") True >>> \"cat,sat,flat\".split(\",\") ['cat', 'sat', 'flat'] >>> # Print the first 5 Chinese unicode characters >>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04') \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x[0] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: # Prompt the user to input a string: input = raw_input(\"Enter string: \") # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input: if char in vowels: count += 1 # Print the count to the standard output print(count) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels.py : python vowels.py Enter string: abracadabra 5","title":"Strings"},{"location":"tutorials/python_overview/python_overview/#lists","text":"Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [1, 2, 3] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x[3] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: >>> type([1, 2, 3]) <class 'list'> >>> x = [] >>> len(x) 0 >>> x.append(\"hello\") >>> x ['hello'] >>> len(x) 1 >>> x[0] 'hello' >>> x.insert(0, True) >>> x [True, 'hello'] >>> del x[1] >>> x [True] >>> x += [42, \"Newton\", 3.142] >>> x [True, 42, 'Newton', 3.142]","title":"Lists"},{"location":"tutorials/python_overview/python_overview/#dictionaries","text":"Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. {12: \"XII\", 6: \"VI\"} Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: >>> type({12: \"XII\", 6: \"VI\"}) <class 'dict'> >>> friends = {} >>> friends['Fred'] = ['Barney', 'Dino'] >>> friends {'Fred': ['Barney', 'Dino']} >>> friends['Fred'] ['Barney', 'Dino'] >>> friends['Barney'] Traceback (most recent call last): File \"\\<stdin\\>\", line 1, in \\<module\\> KeyError: 'Barney' >>> friends['Wilma'] = ['Betty'] >>> friends {'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']} >>> friends.keys() dict_keys(['Fred', 'Wilma']) >>> friends.values() dict_values([['Barney', 'Dino'], ['Betty']]) >>> 'Dino' in friends False Example program: # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys.stdin: # Parse the next input as an integer next_integer = int(line) # Update the histogram accordingly if next_integer in histogram: # We've seen this integer before histogram[next_integer] += 1 else: # First occurrence of this integer in the input histogram[next_integer] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted(histogram): print(\"{} {}\".format(key, histogram[key]))s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo.py : python histo.py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 3 43 12 19 3 12 12 43 Program prints its output: 3 2 12 3 19 1 43 2","title":"Dictionaries"},{"location":"tutorials/python_overview/python_overview/#tuples","text":"Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: (1, 2, 3) The can be used as keys in dictionaries (unlike lists).","title":"Tuples"},{"location":"tutorials/python_overview/python_overview/#loops","text":"","title":"Loops"},{"location":"tutorials/python_overview/python_overview/#while-loops","text":"Iterate until condition is False Syntax: while expression: statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result","title":"While loops"},{"location":"tutorials/python_overview/python_overview/#for-loops","text":"Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: for variable in expression: statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range() function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: def factorial(n): result = 1 for item in range(n + 1): result *= item return result","title":"For loops"},{"location":"tutorials/python_overview/python_overview/#break-and-continue","text":"Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly.","title":"Break and continue"},{"location":"tutorials/python_overview/python_overview/#functions","text":"Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: def variable(parameter_list): statement_block Anonymous function syntax: lambda parameter_list: expression Example: def is_leap_year(year): if year % 4 == 0 and year % 100 != 0: return True else: return year % 400 == 0 for year in range(2000, 2100 + 1): result = is_leap_year(year) print(\"{} {}\".format(year, result)) Anonymous function example: >>> squared = lambda x: x ** 2 >>> squared(2) 4 >>> list(map(lambda x: x + 1, [1, 2, 3])) [2, 3, 4]","title":"Functions"},{"location":"tutorials/python_overview/python_overview/#input-and-output","text":"The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys.argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys.argv[1] # Open the file file = open(filename) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file: num_lines += 1 num_words += len(line.split()) file.close() print(\"Number of lines and words in {}: {} {}\" \\ .format(filename, num_lines, num_words))","title":"Input and output"},{"location":"tutorials/python_overview/python_overview/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"tutorials/python_overview/python_overview/#classes","text":"Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: class variable(superclass_list): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: class Vector(object): def __init__(self, x=0, y=0, z=0): self.x = x self.y = y self.z = z def magnitude(self): return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) def normalise(self): magnitude = self.magnitude() if magnitude == 0: # Somehow we have a degenerate vector. return self else: return self / self.magnitude() def angle(self, other): dp = self.dot_product(other) return acos(dp / self.magnitude() * other.magnitude()) def dot_product(self, other): return self.x * other.x + self.y * other.y + self.z * other.z","title":"Classes"},{"location":"tutorials/python_overview/python_overview/#exceptions","text":"Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: try: statement_block except exception_type as variable: statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: # alternative version of the histogram code from the section on # dictionaries for line in sys.stdin: next_integer = int(line) try: histogram[next_integer] += 1 except KeyError: histogram[next_integer] = 1","title":"Exceptions"},{"location":"tutorials/python_overview/python_overview/#modules","text":"A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: >>> import math >>> math.sqrt(100) 10.0 >>> sqrt(100) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NameError: name 'sqrt' is not defined >>> from math import sqrt >>> sqrt(100) 10.0 >>> import math as m >>> m.sqrt(100) 10.0","title":"Modules"},{"location":"tutorials/python_overview/python_overview/#packages","text":"A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \"sandboxed\" Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Packages"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/","text":"Overview This workshop covers practical approaches for handling data in Python. We will use the Python library Pandas. This workshop is a recommended prerequisite for the Data Visualisation workshop. In order to do effective data analysis or visualisation, we usually need to have our data cleaned and in a consistent format. We will cover the concept of \"tidy\", long-form, and wide-form data, and hands-on approaches for manipulating data and fixing common problems. This workshop concentrates on tabular data, like that found in spreadsheets or databases. Learning Objectives At the end of this workshop you will be able to: read tabular data into Python using Pandas, and manipulate it identify problems in datasets that will hinder analysis use Python to fix common problems understand and convert between different data layouts such as wide-form and \"tidy\" as appropriate for the problem being solved Requirements This workshop is designed for participants with a basic knowledge of Python, but is also appropriate for attendees who do not know Python but have significant experience using another programming language. If you are new to Python, you will probably want to refer to a Python primer. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren't able to install it prior to the workshop, we can work around this, but please contact us beforehand. Notebooks and Data This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Pandas_and_tidying.ipynb notebook.","title":"Data tidying with Python and Pandas"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#overview","text":"This workshop covers practical approaches for handling data in Python. We will use the Python library Pandas. This workshop is a recommended prerequisite for the Data Visualisation workshop. In order to do effective data analysis or visualisation, we usually need to have our data cleaned and in a consistent format. We will cover the concept of \"tidy\", long-form, and wide-form data, and hands-on approaches for manipulating data and fixing common problems. This workshop concentrates on tabular data, like that found in spreadsheets or databases.","title":"Overview"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#learning-objectives","text":"At the end of this workshop you will be able to: read tabular data into Python using Pandas, and manipulate it identify problems in datasets that will hinder analysis use Python to fix common problems understand and convert between different data layouts such as wide-form and \"tidy\" as appropriate for the problem being solved","title":"Learning Objectives"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#requirements","text":"This workshop is designed for participants with a basic knowledge of Python, but is also appropriate for attendees who do not know Python but have significant experience using another programming language. If you are new to Python, you will probably want to refer to a Python primer. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren't able to install it prior to the workshop, we can work around this, but please contact us beforehand.","title":"Requirements"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#notebooks-and-data","text":"This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Pandas_and_tidying.ipynb notebook.","title":"Notebooks and Data"},{"location":"tutorials/python_viz/python_viz/","text":"Overview Python has a wide range of libraries for plotting and visualising data. Many of these are excellent, but it can be hard for a newcomer to know where to start. We will introduce the range of options available, then do hands-on visualisation exercises with some popular libraries: Matplotlib, Seaborn, and Altair. Seaborn builds on Matplotlib to easily create beautiful statistical visualisations. Altair is intended for interactive visualisation and makes it easy to create complex responsive visualisations. Learning Objectives At the end of this workshop you should be able to: be aware of the landscape of visualisation libraries create visualisations of data in Matplotlib, Seaborn and Altair know how to search the documentation for further visualisation functions Requirements This workshop is designed for participants with a basic knowledge of Python. The \"Data tidying with Python and Pandas\" workshop is recommended as a prerequisite. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren't able to install it prior to the workshop, we can work around this, but please contact us beforehand. Notebooks and Data This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Seaborn_Matplotlib.ipynb and Altair.ipynb notebooks.","title":"Data visualisation with Python"},{"location":"tutorials/python_viz/python_viz/#overview","text":"Python has a wide range of libraries for plotting and visualising data. Many of these are excellent, but it can be hard for a newcomer to know where to start. We will introduce the range of options available, then do hands-on visualisation exercises with some popular libraries: Matplotlib, Seaborn, and Altair. Seaborn builds on Matplotlib to easily create beautiful statistical visualisations. Altair is intended for interactive visualisation and makes it easy to create complex responsive visualisations.","title":"Overview"},{"location":"tutorials/python_viz/python_viz/#learning-objectives","text":"At the end of this workshop you should be able to: be aware of the landscape of visualisation libraries create visualisations of data in Matplotlib, Seaborn and Altair know how to search the documentation for further visualisation functions","title":"Learning Objectives"},{"location":"tutorials/python_viz/python_viz/#requirements","text":"This workshop is designed for participants with a basic knowledge of Python. The \"Data tidying with Python and Pandas\" workshop is recommended as a prerequisite. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren't able to install it prior to the workshop, we can work around this, but please contact us beforehand.","title":"Requirements"},{"location":"tutorials/python_viz/python_viz/#notebooks-and-data","text":"This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Seaborn_Matplotlib.ipynb and Altair.ipynb notebooks.","title":"Notebooks and Data"},{"location":"tutorials/rna_seq_dge_advanced/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Advanced Tutorial Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial. Background [15 min] Where does the data in this tutorial come from? The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total-- two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8. Section 1: Preparation [15 min] 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Yeast RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed. Convert the GTF to a GFF file This is needed for downstream analysis. In the tools panel, search for \"GTF\" and click on \"GTF-to-GFF converter\". Select the GTF file and click \"Execute\". Section 2: Alignment [30 mins] In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Map/align the reads with Tophat to the S. cerevisiae reference In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is. 2. Rename the output files You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. 'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam') by using the pen icon next to the file. 3. Visualise the aligned reads with JBrowse In the tool panel search bar, search for \"JBrowse\" and click on it. For \"Reference genome to display\": Use a built-in genome \"Select a reference genome\": Yeast: sacCer2 \"Genetic Code\": 1. The Standard Code Set up a track for the mapped reads: Insert Track Group Insert Annotation Track Track Type: Bam Pileups Select two bam files, one from each condition, e.g. batch1_acceptedhits.bam and chem1_acceptedhits.bam Set up a track for the annotations: Insert Track Group Insert Annotation Track Select the GFF file (that we converted from the GTF file earlier on) Execute When the file is ready, click the eye icon. Select chr1 in the drop down box. Tick all the track names in the left hand side. Zoom in and out with the plus and minus buttons. Go to position 87500 to see a splice junction. Section 3. Cuffdiff [40 min] The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes. 1. Run Cuffdiff to identify differentially expressed genes and transcripts In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Note: This step may take a while, depending on how busy the server is. 2. Explore the Cuffdiff output files There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 43, data 38, and others\". We'll mostly be interested in the file ending with 'gene differential expression testing' which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Cuffdiff_Significant_DE_Genes\". Section 4. Count reads in features [30 min] HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu search bar, search for \"count matrix\". Click on SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes. Section 5: edgeR [30 min] edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models. 1. Generate a list of differentially expressed genes using edgeR In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute 2. Examine the outputs from the previous step Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user's guide at Bioconductor . 3. Extract the significant differentially expressed genes. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_edgeR_topTable_edgeR.xls\" With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"edgeR_Significant_DE_Genes\". Section 6. DESeq2 [30 min] DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity. 1. Generate a list of differentially expressed genes using DESeq2 In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2 2. Examine the outputs the previous step Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Filter out the significant differentially expressed genes. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_DESeq2_topTable_DESeq2.xls\" With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see the first few differentially expressed genes are similar to the ones identified by EdgeR. Section 7: How much concordance is there between methods? We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like 'Cuffdiff_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like 'edgeR_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like 'DESeq2_gene_list' Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_common_gene_list' Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list' We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools. Section 8: Gene set enrichment analysis The biological question being asked in the original paper is essentially: \"What is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\" We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As... Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one. Optional extension: Degust Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ . 1. Load count data into Degust In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\" generated in Section 4 using the disk icon . Go to http://degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data. 3. Explore your data Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. References [1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"RNA-seq DGE Advanced"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#rna-seq-differential-gene-expression-advanced-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Advanced Tutorial"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#tutorial-overview","text":"In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#background-15-min","text":"","title":"Background [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total-- two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account","text":"Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Yeast RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#convert-the-gtf-to-a-gff-file","text":"This is needed for downstream analysis. In the tools panel, search for \"GTF\" and click on \"GTF-to-GFF converter\". Select the GTF file and click \"Execute\".","title":"Convert the GTF to a GFF file"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-2-alignment-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Alignment [30 mins]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-mapalign-the-reads-with-tophat-to-the-s-cerevisiae-reference","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Map/align the reads with Tophat to the S. cerevisiae reference"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-rename-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. 'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam') by using the pen icon next to the file.","title":"2.  Rename the output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-visualise-the-aligned-reads-with-jbrowse","text":"In the tool panel search bar, search for \"JBrowse\" and click on it. For \"Reference genome to display\": Use a built-in genome \"Select a reference genome\": Yeast: sacCer2 \"Genetic Code\": 1. The Standard Code Set up a track for the mapped reads: Insert Track Group Insert Annotation Track Track Type: Bam Pileups Select two bam files, one from each condition, e.g. batch1_acceptedhits.bam and chem1_acceptedhits.bam Set up a track for the annotations: Insert Track Group Insert Annotation Track Select the GFF file (that we converted from the GTF file earlier on) Execute When the file is ready, click the eye icon. Select chr1 in the drop down box. Tick all the track names in the left hand side. Zoom in and out with the plus and minus buttons. Go to position 87500 to see a splice junction.","title":"3.  Visualise the aligned reads with JBrowse"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-3-cuffdiff-40-min","text":"The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes.","title":"Section 3. Cuffdiff [40 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Note: This step may take a while, depending on how busy the server is.","title":"1.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 43, data 38, and others\". We'll mostly be interested in the file ending with 'gene differential expression testing' which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Cuffdiff_Significant_DE_Genes\".","title":"2.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-4-count-reads-in-features-30-min","text":"HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu search bar, search for \"count matrix\". Click on SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes.","title":"Section 4. Count reads in features [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-5-edger-30-min","text":"edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models.","title":"Section 5: edgeR  [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-edger","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute","title":"1.  Generate a list of differentially expressed genes using edgeR"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-from-the-previous-step","text":"Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user's guide at Bioconductor .","title":"2.  Examine the outputs from the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-extract-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_edgeR_topTable_edgeR.xls\" With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".","title":"3.  Extract the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-6-deseq2-30-min","text":"DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity.","title":"Section 6. DESeq2 [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-deseq2","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2","title":"1.  Generate a list of differentially expressed genes using DESeq2"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-the-previous-step","text":"Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Examine the outputs the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-filter-out-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_DESeq2_topTable_DESeq2.xls\" With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see the first few differentially expressed genes are similar to the ones identified by EdgeR.","title":"3.  Filter out the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-7-how-much-concordance-is-there-between-methods","text":"We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like 'Cuffdiff_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like 'edgeR_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like 'DESeq2_gene_list' Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_common_gene_list' Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list' We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools.","title":"Section 7: How much concordance is there between methods?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-8-gene-set-enrichment-analysis","text":"The biological question being asked in the original paper is essentially: \"What is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\" We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As... Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one.","title":"Section 8: Gene set enrichment analysis"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#optional-extension-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ .","title":"Optional extension: Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-load-count-data-into-degust","text":"In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\" generated in Section 4 using the disk icon . Go to http://degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts.","title":"1. Load count data into Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-configure-your-uploaded-data","text":"Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data.","title":"2. Configure your uploaded data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-explore-your-data","text":"Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right.","title":"3. Explore your data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#4-explore-the-demo-data","text":"Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"4. Explore the demo data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#references","text":"[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"References"},{"location":"tutorials/rna_seq_dge_basic/","text":"PR reviewers and advice: Jessica Chung, Clare Sloggett Current slides (Jessica's slides from March 2018): - Overview: https://docs.google.com/presentation/d/1HovpEc5xzB_plxlIpWJAoTpb4309ujfiqOcLKGCDWYw - Workshop: https://docs.google.com/presentation/d/1YmJl8ks7tCg9UOYcjOg1rzr97Qrid5HsS4FqCqxgqq4 Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/","text":"Background Introduction to RNA-seq RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites The Galaxy workflow platform What is Galaxy? Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right. Differential gene expression analysis using Tophat and Cufflinks Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome. The Alternate protocol The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes. Figure 2: General workflow for testing expression differences between two experimental conditions Tophat Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cuffdiff The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences The full Tuxedo Protocol Figure 3: Full Tuxedo protocol workflow Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots. Protocols recommendations Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult. Limitations of the protocols Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques. References Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#introduction-to-rna-seq","text":"RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites","title":"Introduction to RNA-seq"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here","title":"What is Galaxy?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-1-the-galaxy-interface","text":"Tools on the left, data in the middle, analysis workflow on the right.","title":"Figure 1: The Galaxy interface"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#differential-gene-expression-analysis-using-tophat-and-cufflinks","text":"Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.","title":"Differential gene expression analysis using Tophat and Cufflinks"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-alternate-protocol","text":"The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.","title":"The Alternate protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-2-general-workflow-for-testing-expression-differences-between-two-experimental-conditions","text":"","title":"Figure 2: General workflow for testing expression differences between two experimental conditions"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#tophat","text":"Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file)","title":"Tophat"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#cuffdiff","text":"The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences","title":"Cuffdiff"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-full-tuxedo-protocol","text":"","title":"The full Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-3-full-tuxedo-protocol-workflow","text":"Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.","title":"Figure 3: Full Tuxedo protocol workflow"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#protocols-recommendations","text":"Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.","title":"Protocols recommendations"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#limitations-of-the-protocols","text":"Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.","title":"Limitations of the protocols"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"References"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 2px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.extra { color: #444444; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq - Differential Gene Expression Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie Tutorial Overview In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. Here, we'll be using a subset of the data from a published experiment by Hateley et. al. in 2016. In practice, full-sized datasets would be much larger and take longer to run. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner, HISAT2 visualise RNA-seq alignment data with IGV or JBrowse use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data Learning Objectives At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Requirements Participants with no previous Galaxy experience are strongly recommended to attend the \"Introduction to Galaxy\" workshop first. Attendees are required to bring their own laptop computers. The data The sequencing data you will be working with is from Drosophila melanogaster pupae from the study, Transcriptomic response of Drosophila melanogaster pupae developed in hypergravity . The experiment has two conditions, g3 where pupae underwent development in three times Earth's gravity (i.e. 3 g ), and g1 , the control, where pupae developed in the standard gravitational acceleration felt on the surface of Earth (i.e. 1 g ). There are three samples in each condition and the sequencing data is paired-end so you will have two files for each of the six samples. Your aim will be to find differentially expressed genes in g1 vs g3 . Section 1: Preparation 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Data for RNA-Seq tutorial - Hypergravity . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the g1 group. Keep the type as \"Auto-detect\" when uploading these files. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R2.fastq.gz These six files are three paired-end samples from the g3 group. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R2.fastq.gz Then, upload this file of gene definitions. Keep the type as \"Auto-detect\" when this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the 3 samples that developed in 1 g : g1_01_R1.fastq.gz g1_01_R2.fastq.gz g1_02_R1.fastq.gz g1_02_R2.fastq.gz g1_03_R1.fastq.gz g1_03_R2.fastq.gz 6 files containing paired-ended reads for the 3 samples that developed in 3 g : g3_01_R1.fastq.gz g3_01_R2.fastq.gz g3_02_R1.fastq.gz g3_02_R2.fastq.gz g3_03_R1.fastq.gz g3_03_R2.fastq.gz And 1 gene annotation file for chromosome 4 of the Drosophila genome: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: \".fastq.gz\". If you are not familiar with the FASTQ format, click here for an overview . The \".gz\" extension indicates these files have been compressed by gzip . FASTQ files are typically stored as compressed files to save disk space as they are usually gigabytes in size. Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: The reads are paired-end, i.e. g1_01_R1.fastq.gz and g1_01_R2.fastq.gz are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here . Section 2: Alignment with HISAT2 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT2 to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *_R1.fastq.gz. This should be correspond to every second dataset (e.g. 1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) g1_01_R1.fastq.gz g1_02_R1.fastq.gz g1_03_R1.fastq.gz g3_01_R1.fastq.gz g3_02_R1.fastq.gz g3_03_R1.fastq.gz Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *_R2.fastq.gz.) g1_01_R2.fastq.gz g1_02_R2.fastq.gz g1_03_R2.fastq.gz g3_01_R2.fastq.gz g3_02_R2.fastq.gz g3_03_R2.fastq.gz Use defaults for the other fields Execute Your tool interface panel will look similar to this: Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'g1_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view when you click on the eye icon. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say g1_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 50000 reads; of these: 50000 (100.00%) were paired; of these: 321 (0.64%) aligned concordantly 0 times 45766 (91.53%) aligned concordantly exactly 1 time 3913 (7.83%) aligned concordantly >1 times ---- 321 pairs aligned concordantly 0 times; of these: 0 (0.00%) aligned discordantly 1 time ---- 321 pairs aligned 0 times concordantly or discordantly; of these: 642 mates make up the pairs; of these: 529 (82.40%) aligned 0 times 77 (11.99%) aligned exactly 1 time 36 (5.61%) aligned >1 times 99.47% overall alignment rate Here we see we have a very high alignment rate, which is expected since the reads in this dataset have been pre-selected to align to chromosome 4. Section 3: Visualise the aligned reads The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse and IGV. JBrowse and IGV are both interactive tools that can visualise BAM files. You can pick either one to use in this section. JBrowse is run on Galaxy which means you can view your BAM file in your browser, but it takes a while to run the job (~30 mins). IGV is a separate application you'll need to download to your computer and run locally. Viewing in JBrowse Before using JBrowse, you'll need to convert your GTF file to a GFF file. In the left tool panel menu, select the \"GTF-to-GFF converter\" tool, then provide your GTF file and click \"execute\". GTF and GFF are similar representations of the same information, but JBrowse requires the annotation information to be in GFF format. To visualise the alignment data: search for \"JBrowse\" in the tool panel search bar for \"Reference genome to display\": Use a built-in genome for \"Select a reference genome\": Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": BAM pileups for \"BAM Track Data\": select the multiple datasets icon, then select a bam from each condition, e.g., g1_01.bam and g3_01.bam (your files may be named differently) for \"Autogenerate SNP Track\": Yes Set up a track for the annotated genome: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": GFF/GFF3/BED/GBK Features for \"GFF/GFF3/BED Track Data\": select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \"Available Tracks\" on the left are ticked. Zoom in and out with the plus and minus buttons. Select \"Chr4\". Viewing in IGV An alternative to JBrowse is IGV. If you don't already have IGV installed on your computer, download and install it now. You will also need Java installed to run IGV. To visualise the alignment data: Open the IGV application (this may take a few seconds). Once opened, use the top left drop-down menu bar to select the correct Drosophila genome (dm3). You may need to select the \"More...\" option, and select \"D. melanogaster (dm3)\". In Galaxy, click on one of the BAM files, for example 'g1_01.bam', to expand the available options. Click on \"display with IGV local \" and the BAM file should be loaded into IGV. Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) Open another BAM file from the other condition (e.g. 'g3_01.bam') by clicking on the dataset in Galaxy and clicking on \"display with IGV local \". Section 4. Quantification HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) g1_01.bam g1_02.bam g1_03.bam g3_01.bam g3_02.bam g3_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as g1_01, g1_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) g1_01 g1_02 g1_03 g3_01 g3_02 g3_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between g1 samples and g3 samples. Section 5. Degust Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: g1 and g3. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 0.5 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. 5. Explore the full dataset The FASTQ files we started with is only a small proportion of the full dataset. If you wish, you can download the full count matrix here , upload it to Degust, and explore the results. Section 5. DESeq2 In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: gravity 1: Factor level: Specify a factor level: g1 (Select the three g1 htseq-count files.) g1_01 g1_02 g1_03 2: Factor level: Specify a factor level: g3 (Select the three g3 htseq-count files.) g3_01 g3_02 g3_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there? Section 6. The importance of replicates Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. CG1674 is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like CG1674 was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifying a gene as differentially expressed when it is, in reality, not. Optional extension Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#rna-seq-differential-gene-expression","text":"Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie","title":"RNA-Seq - Differential Gene Expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. Here, we'll be using a subset of the data from a published experiment by Hateley et. al. in 2016. In practice, full-sized datasets would be much larger and take longer to run. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner, HISAT2 visualise RNA-seq alignment data with IGV or JBrowse use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#learning-objectives","text":"At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#requirements","text":"Participants with no previous Galaxy experience are strongly recommended to attend the \"Introduction to Galaxy\" workshop first. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#the-data","text":"The sequencing data you will be working with is from Drosophila melanogaster pupae from the study, Transcriptomic response of Drosophila melanogaster pupae developed in hypergravity . The experiment has two conditions, g3 where pupae underwent development in three times Earth's gravity (i.e. 3 g ), and g1 , the control, where pupae developed in the standard gravitational acceleration felt on the surface of Earth (i.e. 1 g ). There are three samples in each condition and the sequencing data is paired-end so you will have two files for each of the six samples. Your aim will be to find differentially expressed genes in g1 vs g3 .","title":"The data"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-1-preparation","text":"1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Data for RNA-Seq tutorial - Hypergravity . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the g1 group. Keep the type as \"Auto-detect\" when uploading these files. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R2.fastq.gz These six files are three paired-end samples from the g3 group. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R2.fastq.gz Then, upload this file of gene definitions. Keep the type as \"Auto-detect\" when this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the 3 samples that developed in 1 g : g1_01_R1.fastq.gz g1_01_R2.fastq.gz g1_02_R1.fastq.gz g1_02_R2.fastq.gz g1_03_R1.fastq.gz g1_03_R2.fastq.gz 6 files containing paired-ended reads for the 3 samples that developed in 3 g : g3_01_R1.fastq.gz g3_01_R2.fastq.gz g3_02_R1.fastq.gz g3_02_R2.fastq.gz g3_03_R1.fastq.gz g3_03_R2.fastq.gz And 1 gene annotation file for chromosome 4 of the Drosophila genome: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: \".fastq.gz\". If you are not familiar with the FASTQ format, click here for an overview . The \".gz\" extension indicates these files have been compressed by gzip . FASTQ files are typically stored as compressed files to save disk space as they are usually gigabytes in size. Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: The reads are paired-end, i.e. g1_01_R1.fastq.gz and g1_01_R2.fastq.gz are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here .","title":"Section 1: Preparation"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-2-alignment-with-hisat2","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT2 to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *_R1.fastq.gz. This should be correspond to every second dataset (e.g. 1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) g1_01_R1.fastq.gz g1_02_R1.fastq.gz g1_03_R1.fastq.gz g3_01_R1.fastq.gz g3_02_R1.fastq.gz g3_03_R1.fastq.gz Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *_R2.fastq.gz.) g1_01_R2.fastq.gz g1_02_R2.fastq.gz g1_03_R2.fastq.gz g3_01_R2.fastq.gz g3_02_R2.fastq.gz g3_03_R2.fastq.gz Use defaults for the other fields Execute Your tool interface panel will look similar to this: Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'g1_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view when you click on the eye icon. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say g1_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 50000 reads; of these: 50000 (100.00%) were paired; of these: 321 (0.64%) aligned concordantly 0 times 45766 (91.53%) aligned concordantly exactly 1 time 3913 (7.83%) aligned concordantly >1 times ---- 321 pairs aligned concordantly 0 times; of these: 0 (0.00%) aligned discordantly 1 time ---- 321 pairs aligned 0 times concordantly or discordantly; of these: 642 mates make up the pairs; of these: 529 (82.40%) aligned 0 times 77 (11.99%) aligned exactly 1 time 36 (5.61%) aligned >1 times 99.47% overall alignment rate Here we see we have a very high alignment rate, which is expected since the reads in this dataset have been pre-selected to align to chromosome 4.","title":"Section 2: Alignment with HISAT2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-3-visualise-the-aligned-reads","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse and IGV. JBrowse and IGV are both interactive tools that can visualise BAM files. You can pick either one to use in this section. JBrowse is run on Galaxy which means you can view your BAM file in your browser, but it takes a while to run the job (~30 mins). IGV is a separate application you'll need to download to your computer and run locally.","title":"Section 3: Visualise the aligned reads"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#viewing-in-jbrowse","text":"Before using JBrowse, you'll need to convert your GTF file to a GFF file. In the left tool panel menu, select the \"GTF-to-GFF converter\" tool, then provide your GTF file and click \"execute\". GTF and GFF are similar representations of the same information, but JBrowse requires the annotation information to be in GFF format. To visualise the alignment data: search for \"JBrowse\" in the tool panel search bar for \"Reference genome to display\": Use a built-in genome for \"Select a reference genome\": Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": BAM pileups for \"BAM Track Data\": select the multiple datasets icon, then select a bam from each condition, e.g., g1_01.bam and g3_01.bam (your files may be named differently) for \"Autogenerate SNP Track\": Yes Set up a track for the annotated genome: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": GFF/GFF3/BED/GBK Features for \"GFF/GFF3/BED Track Data\": select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \"Available Tracks\" on the left are ticked. Zoom in and out with the plus and minus buttons. Select \"Chr4\".","title":"Viewing in JBrowse"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#viewing-in-igv","text":"An alternative to JBrowse is IGV. If you don't already have IGV installed on your computer, download and install it now. You will also need Java installed to run IGV. To visualise the alignment data: Open the IGV application (this may take a few seconds). Once opened, use the top left drop-down menu bar to select the correct Drosophila genome (dm3). You may need to select the \"More...\" option, and select \"D. melanogaster (dm3)\". In Galaxy, click on one of the BAM files, for example 'g1_01.bam', to expand the available options. Click on \"display with IGV local \" and the BAM file should be loaded into IGV. Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) Open another BAM file from the other condition (e.g. 'g3_01.bam') by clicking on the dataset in Galaxy and clicking on \"display with IGV local \".","title":"Viewing in IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-4-quantification","text":"HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) g1_01.bam g1_02.bam g1_03.bam g3_01.bam g3_02.bam g3_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as g1_01, g1_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) g1_01 g1_02 g1_03 g3_01 g3_02 g3_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between g1 samples and g3 samples.","title":"Section 4. Quantification"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: g1 and g3. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 0.5 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. 5. Explore the full dataset The FASTQ files we started with is only a small proportion of the full dataset. If you wish, you can download the full count matrix here , upload it to Degust, and explore the results.","title":"Section 5. Degust"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-deseq2","text":"In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: gravity 1: Factor level: Specify a factor level: g1 (Select the three g1 htseq-count files.) g1_01 g1_02 g1_03 2: Factor level: Specify a factor level: g3 (Select the three g3 htseq-count files.) g3_01 g3_02 g3_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there?","title":"Section 5. DESeq2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-6-the-importance-of-replicates","text":"Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. CG1674 is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like CG1674 was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifying a gene as differentially expressed when it is, in reality, not.","title":"Section 6. The importance of replicates"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#optional-extension","text":"Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Optional extension"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 2px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.extra { color: #444444; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq - Differential Gene Expression Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie Tutorial Overview In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Requirements Participants with no previous Galaxy experience are strongly recommended to attend the \"Introduction to Galaxy\" workshop first. Attendees are required to bring their own laptop computers. The data The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditions, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO. Section 1: Preparation 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Fly RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here . Convert the GTF to a GFF file This is needed for downstream analysis. In the tools panel, search for \"GTF\" and click on \"GTF-to-GFF converter\". Select the GTF file and click \"Execute\". Section 2: Alignment with HISAT2 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Use defaults for the other fields Execute Your tool interface panel will look similar to this (although the options may be in a different order): Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view, when you click on the eye icon. In section 3, we'll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 16046 reads; of these: 16046 (100.00%) were paired; of these: 104 (0.65%) aligned concordantly 0 times 13558 (84.49%) aligned concordantly exactly 1 time 2384 (14.86%) aligned concordantly >1 times ---- 104 pairs aligned concordantly 0 times; of these: 1 (0.96%) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly; of these: 206 mates make up the pairs; of these: 106 (51.46%) aligned 0 times 91 (44.17%) aligned exactly 1 time 9 (4.37%) aligned >1 times 99.67% overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination. Section 3: Visualise the aligned reads The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse. Viewing in JBrowse To visualise the alignment data: search for \"JBrowse\" in the tool panel search bar for \"Reference genome to display\": Use a built-in genome for \"Select a reference genome\": Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": BAM pileups for \"BAM Track Data\": select the multiple datasets icon, then select a bam from each condition, e.g., WT_01.bam and KO_01.bam (your files may be named differently) for \"Autogenerate SNP Track\": Yes Set up a track for the annotated genome: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": GFF/GFF3/BED/GBK Features for \"GFF/GFF3/BED Track Data\": select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \"Available Tracks\" on the left are ticked. Zoom in and out with the plus and minus buttons. Select \"Chr4\". The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. Viewing in IGV To visualise the alignment data: Click on one of the BAM files, for example 'WT_01.bam'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select 'KO_02.bam' and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. Section 4. Quantification HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples. Section 5. Degust Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. Section 5. DESeq2 In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there? Section 6. The importance of replicates Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. Optional extension Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Rna seq basic tutorial 2018"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#rna-seq-differential-gene-expression","text":"Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie","title":"RNA-Seq - Differential Gene Expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#learning-objectives","text":"At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#requirements","text":"Participants with no previous Galaxy experience are strongly recommended to attend the \"Introduction to Galaxy\" workshop first. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#the-data","text":"The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditions, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO.","title":"The data"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-1-preparation","text":"1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Fly RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here .","title":"Section 1: Preparation"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#convert-the-gtf-to-a-gff-file","text":"This is needed for downstream analysis. In the tools panel, search for \"GTF\" and click on \"GTF-to-GFF converter\". Select the GTF file and click \"Execute\".","title":"Convert the GTF to a GFF file"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-2-alignment-with-hisat2","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Use defaults for the other fields Execute Your tool interface panel will look similar to this (although the options may be in a different order): Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view, when you click on the eye icon. In section 3, we'll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 16046 reads; of these: 16046 (100.00%) were paired; of these: 104 (0.65%) aligned concordantly 0 times 13558 (84.49%) aligned concordantly exactly 1 time 2384 (14.86%) aligned concordantly >1 times ---- 104 pairs aligned concordantly 0 times; of these: 1 (0.96%) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly; of these: 206 mates make up the pairs; of these: 106 (51.46%) aligned 0 times 91 (44.17%) aligned exactly 1 time 9 (4.37%) aligned >1 times 99.67% overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination.","title":"Section 2: Alignment with HISAT2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-3-visualise-the-aligned-reads","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse.","title":"Section 3: Visualise the aligned reads"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#viewing-in-jbrowse","text":"To visualise the alignment data: search for \"JBrowse\" in the tool panel search bar for \"Reference genome to display\": Use a built-in genome for \"Select a reference genome\": Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": BAM pileups for \"BAM Track Data\": select the multiple datasets icon, then select a bam from each condition, e.g., WT_01.bam and KO_01.bam (your files may be named differently) for \"Autogenerate SNP Track\": Yes Set up a track for the annotated genome: click \"Insert Track Group\" and then \"Insert Annotation Track\" for \"Track Type\": GFF/GFF3/BED/GBK Features for \"GFF/GFF3/BED Track Data\": select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \"Available Tracks\" on the left are ticked. Zoom in and out with the plus and minus buttons. Select \"Chr4\". The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"Viewing in JBrowse"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#viewing-in-igv","text":"To visualise the alignment data: Click on one of the BAM files, for example 'WT_01.bam'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select 'KO_02.bam' and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"Viewing in IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-4-quantification","text":"HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples.","title":"Section 4. Quantification"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-5-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"Section 5. Degust"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-5-deseq2","text":"In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there?","title":"Section 5. DESeq2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-6-the-importance-of-replicates","text":"Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not.","title":"Section 6. The importance of replicates"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#optional-extension","text":"Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Optional extension"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Basic Tutorial Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Background Where does the data in this tutorial come from? The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \"Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\" (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions. The Tuxedo Protocol The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here . Section 1: Preparation [15 min] 1. Register as a new user in Galaxy if you don\u2019t already have an account ( what is Galaxy? ) Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as 'fastqsanger' and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 Section 2: Align reads with Tophat [30 mins] In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Align the RNA-seq short reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is. 2. Examine the output files You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. 3. Visualise the aligned reads with IGV The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example 'Tophat on data 1: accepted_hits'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as 'TopHat on data 1: splice junctions'. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select 'TopHat on data 4: accepted_hits' (this is the accepted hits alignment file from first replicate of condition C2) and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. 4. [Optional] Visualise the aligned reads in Trackster We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar. Section 3: Test differential expression with Cuffdiff [45 min] The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here . 1. Examine the reference transcriptome Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites. 2. Run Cuffdiff to identify differentially expressed genes and transcripts In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Explore the Cuffdiff output files There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 37, data 32, and others\". FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM tracking\" by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene differential expression testing\" by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Significant_DE_Genes\". Examine the sorted list of differentially expressed genes. Click on the eye icon next to \"Significant_DE_Genes\" to view the data. How many genes are in the Significant_DE_Genes file? What are their names? Two genes have been identified as differentially expressed between conditions C1 and C2: Ank located at chr4:137014-150378, and CG2177 located at chr4:331557-334534 Both genes have q-values of 0.00175. \"CG2177\" located at chr4:331557-334534 was the gene that we intuitively (with IGV) saw to be differentially expressed in the previous section, in the broader region of chr4:325197-341887. Section 4. Repeat without replicates [20 min] In this section, we will run Cuffdiff with fewer replicates. Stop and think: Why do we need replicates for an RNA-seq differential gene expression experiment? What do you expect to happen if we only use one sample from each condition for our analysis? Repeat the differential gene expression testing from section 2, but this time only use one replicate from each condition group (C1 and C2). From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_C1_R1_vs_C2_R1\" Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R1. You should get no differentially expressed genes at statistical significance of 0.05. The \"Ank\" gene and the \"CG1277\", which were found to be significantly differentially expressed in our first analysis, are not identified as differentially expressed when we only use one sample for each condition. Repeat this no-replicates analysis, but this time specify a different set of samples. From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 5: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_C1_R1_vs_C2_R2\" Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R2. We now see \"CG2177\" appear again in the list as significantly differentially expressed, but not \"Ank\". How can we interpret the difference in results from using different replicates? There is a larger absolute difference in CG1277 expression between samples 1 (C1_R1) and 5 (C2_R2) than samples 1 (C1_R1) and 4 (C2_R1), hence Cuffdiff identifies CG1277 as differentially expressed between C1_R1 and C2_R2, but not between C1_R1 and C2_R1. On the other hand, differences in level of expression of Ank is much smaller between samples, so we need to see it consistently across multiple replicates for Cuffdiff to be confident it actually exists. One replicate is not enough. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. If we say that genes Ank and CG2177 are truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. [Optional step] Repeat this analysis, specifying groups of two replicates each. What do you get? How many replicates do we need to identify Ank as differentially expressed? Section 5. Optional Extension [20 min] Extension on the Tuxedo Protocol The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn't have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_using_Cufflinks_Assembly\" Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177). Transcript-level differential expression One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \"unit of aggregation\" to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation. References Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"Tuxedo Protocol Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#rna-seq-differential-gene-expression-basic-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Basic Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#learning-objectives","text":"At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \"Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\" (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#the-tuxedo-protocol","text":"The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here .","title":"The Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account-what-is-galaxy","text":"Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account (what is Galaxy?)"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as 'fastqsanger' and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-view-and-have-an-understanding-of-the-files-involved-in-rna-seq-analysis","text":"You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019","title":"3.  View and have an understanding of the files involved in RNA-seq analysis."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-2-align-reads-with-tophat-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Align reads with Tophat [30 mins]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-align-the-rna-seq-short-reads-to-a-reference-genome","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Align the RNA-seq short reads to a reference genome."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-examine-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history.","title":"2.  Examine the output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-visualise-the-aligned-reads-with-igv","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example 'Tophat on data 1: accepted_hits'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as 'TopHat on data 1: splice junctions'. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select 'TopHat on data 4: accepted_hits' (this is the accepted hits alignment file from first replicate of condition C2) and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"3.  Visualise the aligned reads with IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#4-optional-visualise-the-aligned-reads-in-trackster","text":"We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar.","title":"4.  [Optional] Visualise the aligned reads in Trackster"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-3-test-differential-expression-with-cuffdiff-45-min","text":"The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here .","title":"Section 3: Test differential expression with Cuffdiff [45 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-examine-the-reference-transcriptome","text":"Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites.","title":"1.  Examine the reference transcriptome"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 37, data 32, and others\". FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM tracking\" by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene differential expression testing\" by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Significant_DE_Genes\". Examine the sorted list of differentially expressed genes. Click on the eye icon next to \"Significant_DE_Genes\" to view the data.","title":"3.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-4-repeat-without-replicates-20-min","text":"In this section, we will run Cuffdiff with fewer replicates.","title":"Section 4. Repeat without replicates [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-5-optional-extension-20-min","text":"","title":"Section 5. Optional Extension [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#extension-on-the-tuxedo-protocol","text":"The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn't have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_using_Cufflinks_Assembly\" Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177).","title":"Extension on the Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#transcript-level-differential-expression","text":"One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \"unit of aggregation\" to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation.","title":"Transcript-level differential expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"References"},{"location":"tutorials/rna_seq_exp_design/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/","text":"RNA-Seq Experimental Design What is RNA-seq? RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes. Why is a good experimental design vital? An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you're planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage. Terminology Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth. The importance of replicates to estimate variance When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can't be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis. How many replicates and how many reads do I need? Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions. Sequencing options to consider How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3' end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility. Summary A good experimental design is vital for a successful experiment. If you're planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"RNA-seq DGE Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#rna-seq-experimental-design","text":"","title":"RNA-Seq Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#what-is-rna-seq","text":"RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes.","title":"What is RNA-seq?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#why-is-a-good-experimental-design-vital","text":"An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you're planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage.","title":"Why is a good experimental design vital?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#terminology","text":"Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth.","title":"Terminology"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#the-importance-of-replicates-to-estimate-variance","text":"When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can't be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis.","title":"The importance of replicates to estimate variance"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#how-many-replicates-and-how-many-reads-do-i-need","text":"Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions.","title":"How many replicates and how many reads do I need?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#sequencing-options-to-consider","text":"How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3' end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility.","title":"Sequencing options to consider"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#summary","text":"A good experimental design is vital for a successful experiment. If you're planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"Summary"},{"location":"tutorials/singlecell/","text":"10X single-cell RNA-seq analysis in R Overview In this workshop, you will be learning how to analyse 10X Chromium single-cell RNA-seq profiles using R. This will include reading the count data into R, quality control, normalisation, dimensionality reduction, cell clustering and finding marker genes. The main part of the workflow uses the package. You will learn how to generate common plots for visualising single-cell data, such as t-SNE plots and heatmaps. This workshop is aimed at biologists interested in learning how to explore single-cell RNA-seq data generated by the 10X platform. This workshop is presented by Dr Yunshun Chen from Walter Eliza Hall Institute of Medical Research (WEHI). Dr Chen is a statistical bioinformatician and a senior post-doc at WEHI Bioinformatics Division. He is one of the authors and the main maintainer of the edgeR package. He has extensive experience in RNA-seq gene expression and single-cell RNA-seq analyses. Requirements The course is aimed at advanced PhD students, postdoctoral researchers and principal investigators. Some basic R knowledge is assumed \u2013 this is not an introduction to R course. If you are not familiar with the R statistical programming language it is compulsory that you work through an introductory R course before you attend this workshop. This workshop will cover single-cell RNA-seq analysis and assumes you have some familiarity with the more common analysis of bulk RNA-seq data. If you have no experience in analysing bulk RNA-seq data, we strongly recommend you also attend our RNA-seq Differential Gene Expression analysis in R workshop. Participants must bring a laptop with a Mac, Linux, or Windows operating system (not a tablet, Chromebook, etc.) that they have administrative privileges on. Bringing your laptop charger is advised. R and a few specific software packages should be installed in advance. The 10X data to be analysed, the gene annotation file and the gene signature RData file should also be downloaded prior to the workshop to allow us time to troubleshoot any problems you may have. Downloads: Download R from https://cran.r-project.org/ . The lastest version is recommended (version 3.6.0 for Windows and Mac OS X 10.11 and higher). Install R packages by opening R and copying the following commands into your R console if (!requireNamespace(\u201cBiocManager\u201d, quietly = TRUE)) install.packages(\u201cBiocManager\u201d) BiocManager::install(c(\u201cscran\u201d, \u201cmonocle\u201d, \u201cvcd\u201d)) Download the 10X single-cell RNA-seq data from GEO (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2510617). The three data files required for the workshop are: GSM2510617_P7-barcodes.tsv.gz, GSM2510617_P7-genes.tsv.gz and GSM2510617_P7-matrix.mtx.gz, of which the download links are available at the bottom of the web page. Unzip the three data files and store them in the working directory (or a sub-folder under the working directory). Download the mouse gene annotation file from http://bioinf.wehi.edu.au/edgeR/Mus_musculus.gene_info.gz and save it under the working directory. Note there is no need to unzip the file. Download gene signatures from http://bioinf.wehi.edu.au/edgeR/BulkSignatures.RData and save it under the working directory. Check downloads are correct by looking for any error messages when running the commands: library(edgeR) library(scater) library(scran) library(monocle) library(vcd) read.delim(\u201cP7-genes.tsv\u201d, nrow=5) read.delim(\u201cMus_musculus.gene_info.gz\u201d, nrow=5) load(\u201cBulkSignatures.RData\u201d) Important: If you have any trouble installing the software or packages, please contact us prior to the workshop. Workshop syllabus This workshop will follow this analysis guide . Additional single-cell analysis links An overview and comparison of protocols and analysis methods: Tian, L. et al, Biorxiv, scRNA-seq mixology: towards better benchmarking of single cell RNA-seq protocols and analysis methods Single cell RNA-seq data analysis: A course by CSC, Finland: Single cell data analysis A course by the Hemberg Lab: scRNA-seq analysis List of software packages for single-cell data analysis: Davis, S. et al: Zenodo link ; Github link","title":"10X single-cell RNA-seq analysis in R"},{"location":"tutorials/singlecell/#10x-single-cell-rna-seq-analysis-in-r","text":"","title":"10X single-cell RNA-seq analysis in R"},{"location":"tutorials/singlecell/#overview","text":"In this workshop, you will be learning how to analyse 10X Chromium single-cell RNA-seq profiles using R. This will include reading the count data into R, quality control, normalisation, dimensionality reduction, cell clustering and finding marker genes. The main part of the workflow uses the package. You will learn how to generate common plots for visualising single-cell data, such as t-SNE plots and heatmaps. This workshop is aimed at biologists interested in learning how to explore single-cell RNA-seq data generated by the 10X platform. This workshop is presented by Dr Yunshun Chen from Walter Eliza Hall Institute of Medical Research (WEHI). Dr Chen is a statistical bioinformatician and a senior post-doc at WEHI Bioinformatics Division. He is one of the authors and the main maintainer of the edgeR package. He has extensive experience in RNA-seq gene expression and single-cell RNA-seq analyses.","title":"Overview"},{"location":"tutorials/singlecell/#requirements","text":"The course is aimed at advanced PhD students, postdoctoral researchers and principal investigators. Some basic R knowledge is assumed \u2013 this is not an introduction to R course. If you are not familiar with the R statistical programming language it is compulsory that you work through an introductory R course before you attend this workshop. This workshop will cover single-cell RNA-seq analysis and assumes you have some familiarity with the more common analysis of bulk RNA-seq data. If you have no experience in analysing bulk RNA-seq data, we strongly recommend you also attend our RNA-seq Differential Gene Expression analysis in R workshop. Participants must bring a laptop with a Mac, Linux, or Windows operating system (not a tablet, Chromebook, etc.) that they have administrative privileges on. Bringing your laptop charger is advised. R and a few specific software packages should be installed in advance. The 10X data to be analysed, the gene annotation file and the gene signature RData file should also be downloaded prior to the workshop to allow us time to troubleshoot any problems you may have. Downloads: Download R from https://cran.r-project.org/ . The lastest version is recommended (version 3.6.0 for Windows and Mac OS X 10.11 and higher). Install R packages by opening R and copying the following commands into your R console if (!requireNamespace(\u201cBiocManager\u201d, quietly = TRUE)) install.packages(\u201cBiocManager\u201d) BiocManager::install(c(\u201cscran\u201d, \u201cmonocle\u201d, \u201cvcd\u201d)) Download the 10X single-cell RNA-seq data from GEO (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2510617). The three data files required for the workshop are: GSM2510617_P7-barcodes.tsv.gz, GSM2510617_P7-genes.tsv.gz and GSM2510617_P7-matrix.mtx.gz, of which the download links are available at the bottom of the web page. Unzip the three data files and store them in the working directory (or a sub-folder under the working directory). Download the mouse gene annotation file from http://bioinf.wehi.edu.au/edgeR/Mus_musculus.gene_info.gz and save it under the working directory. Note there is no need to unzip the file. Download gene signatures from http://bioinf.wehi.edu.au/edgeR/BulkSignatures.RData and save it under the working directory. Check downloads are correct by looking for any error messages when running the commands: library(edgeR) library(scater) library(scran) library(monocle) library(vcd) read.delim(\u201cP7-genes.tsv\u201d, nrow=5) read.delim(\u201cMus_musculus.gene_info.gz\u201d, nrow=5) load(\u201cBulkSignatures.RData\u201d) Important: If you have any trouble installing the software or packages, please contact us prior to the workshop.","title":"Requirements"},{"location":"tutorials/singlecell/#workshop-syllabus","text":"This workshop will follow this analysis guide .","title":"Workshop syllabus"},{"location":"tutorials/singlecell/#additional-single-cell-analysis-links","text":"An overview and comparison of protocols and analysis methods: Tian, L. et al, Biorxiv, scRNA-seq mixology: towards better benchmarking of single cell RNA-seq protocols and analysis methods Single cell RNA-seq data analysis: A course by CSC, Finland: Single cell data analysis A course by the Hemberg Lab: scRNA-seq analysis List of software packages for single-cell data analysis: Davis, S. et al: Zenodo link ; Github link","title":"Additional single-cell analysis links"},{"location":"tutorials/unix/","text":"PR reviewers and advice: Andrew Robinson, Peter Georgeson, Chol-hee Jung, Ben Moran Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/unix/intro/","text":"Why use unix Powerful : Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. Big data : Unix programs are designed to handle large data sets Flexible : Small programs that can be arranged in many ways to solve your problems Automation : Scripting allows you to do many tasks in one step and repeat steps many times Pipelines : Unix programs are designed to be 'chained' together to form long multi-step pipelines Science Software : Lots of Scientific software is designed to run in a Unix environment User interface The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. Command prompt The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: Time : the time (when the last command finished) Username : the username that you are logged in as Hostname : the name of the computer that your are connected to Current working directory : the current position within the file system that your are working. More to follow Prompt : this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign. Command line Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): Command : this is the name of the program (command) that you want to run Flag : these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. Long flag : same as flag except they are generally two dashes (--) followed by a word (or two) Option : set the value of a configurable option. They are a flag (or long flag) followed by a value Anonymous options : these are one or more options that are specified in the required order Quoted value : if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value. File system The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. Absolute file names Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. Absolute file name : /settings Absolute file name : /home/user1/file01.txt Absolute file name : /home/user2 Note : the final slash is not needed (but generally doesn't hurt if it is present). Current working directory The current working directory is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. Relative file names Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to /home you can leave this part from the beginning of the filename. Relative file name : user1/muscle.fq (Note: the absence of the leading slash) Special file names : There are a few further short cuts for typing relative file names: ~ (Tilde): is a short cut to your home directory . (dot): is a short cut for the current directory .. (2x dot): means the parent (or one directory up) from current directory ... (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. ../.. Note : the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to /home/user2 the relative path to muscle.fq is different. Relative file name : ../user1/muscle.fq","title":"Intro"},{"location":"tutorials/unix/intro/#why-use-unix","text":"Powerful : Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. Big data : Unix programs are designed to handle large data sets Flexible : Small programs that can be arranged in many ways to solve your problems Automation : Scripting allows you to do many tasks in one step and repeat steps many times Pipelines : Unix programs are designed to be 'chained' together to form long multi-step pipelines Science Software : Lots of Scientific software is designed to run in a Unix environment","title":"Why use unix"},{"location":"tutorials/unix/intro/#user-interface","text":"The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment.","title":"User interface"},{"location":"tutorials/unix/intro/#command-prompt","text":"The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: Time : the time (when the last command finished) Username : the username that you are logged in as Hostname : the name of the computer that your are connected to Current working directory : the current position within the file system that your are working. More to follow Prompt : this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign.","title":"Command prompt"},{"location":"tutorials/unix/intro/#command-line","text":"Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): Command : this is the name of the program (command) that you want to run Flag : these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. Long flag : same as flag except they are generally two dashes (--) followed by a word (or two) Option : set the value of a configurable option. They are a flag (or long flag) followed by a value Anonymous options : these are one or more options that are specified in the required order Quoted value : if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value.","title":"Command line"},{"location":"tutorials/unix/intro/#file-system","text":"The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on.","title":"File system"},{"location":"tutorials/unix/intro/#absolute-file-names","text":"Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. Absolute file name : /settings Absolute file name : /home/user1/file01.txt Absolute file name : /home/user2 Note : the final slash is not needed (but generally doesn't hurt if it is present).","title":"Absolute file names"},{"location":"tutorials/unix/intro/#current-working-directory","text":"The current working directory is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt.","title":"Current working directory"},{"location":"tutorials/unix/intro/#relative-file-names","text":"Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to /home you can leave this part from the beginning of the filename. Relative file name : user1/muscle.fq (Note: the absence of the leading slash) Special file names : There are a few further short cuts for typing relative file names: ~ (Tilde): is a short cut to your home directory . (dot): is a short cut for the current directory .. (2x dot): means the parent (or one directory up) from current directory ... (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. ../.. Note : the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to /home/user2 the relative path to muscle.fq is different. Relative file name : ../user1/muscle.fq","title":"Relative file names"},{"location":"tutorials/unix/robinson-unix-link/","text":"Introduction to Unix Please see the link here .","title":"Introduction to Unix"},{"location":"tutorials/unix/robinson-unix-link/#introduction-to-unix","text":"Please see the link here .","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/","text":"em {font-style: normal; font-family: courier new;} Introduction to Unix A hands-on workshop covering the basics of the Unix/Linux command line interface. Overview Knowledge of the Unix operating system is fundamental to being productive on HPC systems. This workshop will introduce you to the fundamental Unix concepts by way of a series of hands-on exercises. The workshop is facilitated by experienced Unix users who will be able to guide you through the exercises and offer assistance where needed. Learning Objectives At the end of the course, you will be able to: Log into a Unix machine remotely Organise your files into directories Change file permissions to improve security and safety Create and edit files with a text editor Copy files between directories Use command line programs to manipulate files Automate your workflow using shell scripts Requirements The workshop is intended for beginners with no prior experience in Unix. Attendees are required to bring their own laptop computers. Introduction Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the Unix concepts. The slides are available if you would like. Additionally the following reference material is available for later use. Reference Material ### Why use unix * **Powerful**: Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. * **Big data**: Unix programs are designed to handle large data sets * **Flexible**: Small programs that can be arranged in many ways to solve your problems * **Automation**: Scripting allows you to do many tasks in one step and repeat steps many times * **Pipelines**: Unix programs are designed to be 'chained' together to form long multi-step pipelines * **Science Software**: Lots of Scientific software is designed to run in a Unix environment ### User interface The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. ### Command prompt The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: * **Time**: the time (when the last command finished) * **Username**: the username that you are logged in as * **Hostname**: the name of the computer that your are connected to * **Current working directory**: the current position within the file system that your are working. More to follow * **Prompt**: this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign. ### Command line Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): * **Command**: this is the name of the program (command) that you want to run * **Flag**: these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. * **Long flag**: same as flag except they are generally two dashes (--) followed by a word (or two) * **Option**: set the value of a configurable option. They are a flag (or long flag) followed by a value * **Anonymous options**: these are one or more options that are specified in the required order * **Quoted value**: if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value. ### File system The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. #### Absolute file names Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. **Absolute file name**: */settings* **Absolute file name**: */home/user1/file01.txt* **Absolute file name**: */home/user2* **Note**: the final slash is not needed (but generally doesn't hurt if it is present). #### Current working directory The *current working directory* is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. #### Relative file names Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to */home* you can leave this part from the beginning of the filename. **Relative file name**: *user1/muscle.fq* (Note: the absence of the leading slash) **Special file names**: There are a few further short cuts for typing relative file names: * *~* (Tilde): is a short cut to your home directory * *.* (dot): is a short cut for the current directory * *..* (2x dot): means the parent (or one directory up) from current directory * *...* (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. *../..* **Note**: the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to */home/user2* the relative path to muscle.fq is different. **Relative file name**: *../user1/muscle.fq* Topic 1: Remote log in In this topic we will learn how to connect to a Unix computer via a program called ssh and run a few basic commands. Connecting to a Unix computer To begin this workshop you will need to connect to an HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC's tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. barcoo is a high performance computer for Melbourne Bioinformatics users. Logging in connects your local computer (e.g. laptop) to barcoo, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on barcoo for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of barcoo, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into barcoo over ssh. Exercises 1.1) When you've logged into the Unix server, run the following commands and see what they do: who whoami date cal hostname /vlsci/TRAINING/shared/Intro_to_Unix/hi Answer * **who**: displays a list of the users who are currently using this Unix computer. * **whoami**: displays your username (i.e. they person currently logged in). * **date**: displays the current date and time. * **cal**: displays a calendar on the terminal. It can be configured to display more than just the current month. * **hostname**: displays the name of the computer we are logged in to. * **/vlsci/TRAINING/shared/Intro_to_Unix/hi**: displays the text \"Hello World\" Topic 2: Exploring your home directory In this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer. 2.1) Use the ls command to list the files in your home directory. How many files are there? Hint Literally, type *ls* and press the *ENTER* key. Answer $ ls exp01 file01 muscle.fq When running the *ls* command with no options it will list files in your current working directory. The place where you start when you first login is your *HOME* directory. **Answer**: 3 (exp01, file01 and muscle.fq) The above answer is not quite correct. There are a number of hidden files in your home directory as well. 2.2) What flag might you use to display all files with the ls command? How many files are really there? Hint Take the *all* quite literally. Additional Hint Type *ls --all* and press the *ENTER* key. Answer **Answer 1**: *--all* (or *-a*) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely *.* and *..* which are not real files but instead, shortcuts. *.* is a shortcut for the current directory and *..* a shortcut for the directory above the current one. **Answer 2**: 10 files (don't count *.* and *..*) 2.3) What is the full path name of your home directory? Hint Remember your *Current Working Directory* starts in your *home* directory. Additional Hint Try a shortened version of *print working directory* Answer You can find out the full path name of the current working directory with the *pwd* command. Your home directory will look something like this: $ pwd /home/trainingXX **Answer**: */vlsci/TRAINING/trainXX* where *XX* is replaced by some 2 digit sequence. **Alternate method**: You can also find out the name of your home directory by printing the value of the *$HOME* shell variable: echo $HOME 2.4) Run ls using the long flag ( -l ), how did the output change? Hint Run *ls -l* Answer **Answer**: it changed the output to place 1 file/directory per line. It also added some extra information about each. $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11:28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq **Details**: drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 \\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/ permission | username group size date name /---^---\\ linkcount Where: * **permissions**: 4 parts, file type, user perms, group perms and other perms * *filetype*: 1 character, *d* = directory and *-* regular file * *user* permissions: 3 characters, *r* = read, *w* = write, *x* = execute and *-* no permission * *group* permissions: same as user except for users within the owner group * *other* permissions: same as user except for users that are not in either user *or* *group* * **username**: the user who *owns* this file/directory * **group**: the group name who *owns* this file/directory * **size**: the number of bytes this file/directory takes to store on disk * **date**: the date and time when this file/directory was *last edited* * **name**: name of the file * **linkcount**: technical detail which represents the number of links this file has in the file system (safe to ignore) 2.5) What type of file is exp01 and muscle.fq ? Hint Check the output from the *ls -l*. Answer **Answer**: * *exp01*: Directory (given the 'd' as the first letter of its permissions) * *muscle.fq*: Regular File (given the '-') 2.6) Who has permission to read , write and execute your home directory? Hint You can also give *ls* a filename as the first option. Additional Hint *ls -l* will show you the contents of the *CWD*; how might you see the contents of the *parent* directory? (remember the slides) Answer If you pass the *-l* flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. **Method 1**: given we know the *CWD* is our home directory. $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... The *..* refers to the parent directory. **Method 2**: using $HOME. This works no matter what our *CWD* is set to. You could list the permissions of all files and directories in the parent directory of your home: $ ls -l $HOME/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... In this case we use the shell variable to refer to our home directory. **Method 3**: using *~* (tilde) shortcut You may also refer to your home directory using the *~* (tilde) character: $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like *trainXX*. Where *XX* is replaced by a two digit string. **Altername**: using the *-a* flag and looking at the *.* (dot) special file. $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 . ... **Answer**: *drwxr-x---* * **You**: read (see filenames), write (add, delete files), execute (change your CWD to this directory). * **Training users**: read, execute * **Everyone else**: No access **Discussion on Permissions**: The permission string is *\"drwxr-x---\"*. The *d* means it is a directory. The *rwx* means that the owner of the directory (your user account) can *read*, *write* and *execute* the directory. Execute permissions on a directory means that you can *cd* into the directory. The *r-x* means that anyone in the same user group as *training* can read or execute the directory. The *---* means that nobody else (other users on the system) can do anything with the directory. man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q . 2.7) Use the man command to find out what the -h flag does for ls Hint Give *ls* as an option to *man* command. Additional Hint *man ls* Answer Use the following command to view the *man* page for *ls*: $ man ls **Answer**: You should discover that the *-h* option prints file sizes in human readable format -h, --human-readable with -l, print sizes in human readable format (e.g., 1K 234M 2G) 2.8) Use the -h , how did the output change of muscle.fq ? Hint Don't forget the *-l* option too. Additional Hint Run *ls -lh* Answer $ ls -lh ... -rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq **Answer**: it changed the output so the *filesize* of *muscle.fq* is now *2.5K* instead of *2461* Topic 3: Exploring the file system In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file 3.1) Print the value of your current working directory. Answer The *pwd* command prints the value of your current working directory. $ pwd /home/training01 3.2) List the contents of the root directory, called ' / ' (forward slash). Hint *ls* expects one or more anonymous options which are the files/directories to list. Answer $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that *ls* can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory. 3.3) Use the cd command to change your working directory to the root directory. Did your prompt change? Hint *cd* expects a single option which is the directory to change to Answer The *cd* command changes the value of your current working directory. To change to the root directory use the following command: $ cd / **Answer**: Yes, it now says the CWD is */* instead of *~*. Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen. 3.4) List the contents of the CWD and verify it matches the list in 3.2 Hint *ls* Answer Assuming you have changed to the root directory then this can be achieved with *ls*, or *ls -a* (for all files) or *ls -la* for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys **Answer**: Yes, we got the same output as exercise 3.2 3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system? Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory Additional Hint *$HOME*, *~*, */vlsci/TRAINING/trainXX* are all methods to name your home directory. Yet there is a simpler method; the answer is buried in *man cd* however *cd* doesn't have its own manpage so you will need to search for it. Answer Use the *cd* command to change your working directory to your home directory. There are a number of ways to refer to your home directory: cd $HOME is equivalent to: cd ~ The simplest way to change your current working directory to your home directory is to run the *cd* command with no arguments: **Answer**: the simplest for is cd with NO options. cd This is a special-case behaviour which is built into *cd* for convenience. 3.6) Change your working directory to the following directory: /vlsci/TRAINING/shared/Intro_to_Unix Answer **Answer**: *cd /vlsci/TRAINING/shared/Intro_to_Unix* 3.7) List the contents of that directory. How many files does it contain? Hint *ls* Answer You can do this with *ls* $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy **Answer**: 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy) 3.8) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/sleepy ? Hint Take the word *file* quite literally. Additional Hint *file sleepy* Answer Use the *file* command to get extra information about the contents of a file: Assuming your current working directory is */vlsci/TRAINING/shared/Intro_to_Unix* $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: $ file /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Bourne-Again shell script text executable **Answer**: Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The *file* command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: man file 3.9) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/hi ? Hint Take the word *file* quite literally. Answer Use the file command again. If you are in the same directory as *hi* then: $ file hi ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.9, not stripped **Answer**: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called *hi* contains a binary executable program (raw instructions that the computer can execute directly). 3.10) What are the file permissions of the following file and what do they mean? /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Hint Remember the *ls* command, and don't forget the *-l* flag Answer You can find the permissions of *sleepy* using the *ls* command with the *-l* flag. If you are in the same directory as *sleepy* then: $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16:36 sleepy **Answer**: The Answer is dependent on the computer you are connected too however will follow something like above. We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and write-able only to arobinson. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the *ln* command. 3.11) Change your working directory back to your home directory ready for the next topic. Hint *cd* Answer You should know how to do this with the cd command: cd Topic 4: Working with files and directories In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections. 4.1) In your home directory make a sub-directory called test. Hint You are trying to *make a directory*, which of the above commands looks like a shortened version of this? Additional Hint *mkdir* Answer Make sure you are in your home directory first. If not *cd* to your home directory. Use the *mkdir* command to make new directories: $ mkdir test Use the *ls* command to check that the new directory was created. $ ls exp01 file01 muscle.fq test 4.2) Copy all the files from the following directory into the newly created test directory: /vlsci/TRAINING/shared/Intro_to_Unix Hint You are trying to *copy*, which of the above commands looks like a shortened version of this? Additional Hint $ man cp ... SYNOPSIS cp [OPTION]... [-T] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. which means *cp* expects zero or more flags, a SOURCE file followed by a DEST file or directory Answer Use the *cp* command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to *cp*. You only need to perform one of these ways, but we show multiple ones for your reference. **Answer 1**: From your home directory: $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test **Answer 2**: Change to the test directory and then copy (assuming you started in your home directory): $ cd test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* . In the example above the '*.*' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. **Answer 3**: Change to the \\end{UNIX_TRAINING_FILES_PATH} directory and then copy: cd /vlsci/TRAINING/shared/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. Note : This exercise assumes that the copy command from the previous exercise was successful. 4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to. Hint Remember *ls* can show you the file size (with one of its flags) Additional Hint *ls -l* Answer Use *ls -l* to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: $ ls -l /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. **Answer**: They should each be *1033773* bytes **Alternate**: Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the *-h* option for ls: $ ls -lh /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as *1010K*. Larger files are reported in megabytes, gigabytes etcetera. Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test 4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to. Hint What is the opposite of *same*? Additional Hint *diff*erence Answer Use the *diff* command to compare the contents of two files. $ diff /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the *diff* command will NOT produce any output) **Answer**: Yes, they are the same since no output was given. 4.5) How many lines, words and characters are in expectations.txt? Hint Initialisms are key Additional Hint *w*ord *c*ount Answer Use the *wc* (for \"word count\") to count the number of characters, lines and words in a file: $ wc expectations.txt 20415 187465 1033773 expectations.txt **Answer**: There are *20415* lines, *187465* words and *1033773* characters in expectations.txt. To get just the line, word or character count: $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt 4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano . Hint *nano FILENAME* Once *nano* is open it displays some command hints along the bottom of the screen. Additional Hint *^O* means hold the *Control* (or CTRL) key while pressing the *o*. Despite what it displays, you need to type the lower-case letter that follows the *^* character. WriteOut is another name for Save. Answer Take some time to play around with the *nano* text editor. *Nano* is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as *vim* and *emacs*, however they take a substantial amount of time to learn. 4.7) Did the changes you made to ~/test/expectations.txt have any effect on /vlsci/TRAINING/shared/Intro_to_Unix ? How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 Additional Hint Use *diff* Answer Use *diff* to check that the two files are different after you have made the change to the copy of *expectations.txt* in your *~/test* directory. diff ~/test/expectations.txt \\ /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt You could also use *ls* to check that the files have different sizes. 4.8) In your test subdirectory, rename expectations.txt to foo.txt . Hint Another way to think of it is *moving* it from *expectations.txt* to *foo.txt* Additional Hint *mv* Use *man mv* if you need to work out how to use it. Answer Use the *mv* command to rename the file: $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy 4.9) Rename foo.txt back to expectations.txt. Answer Use the *mv* command to rename the file: $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use *ls* to check that the file is in fact renamed. 4.10) Remove the file expectations.txt from your test directory. Hint We are trying to *remove* a file, check the commands at the top of this topic. Additional Hint *rm* Answer Use the *rm* command to remove files (carefully): $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy 4.11) Remove the entire test directory and all the files within it. Hint We are trying to *remove a directory*. Additional Hint You could use *rmdir* but there is an easier way using just *rm* and a flag. Answer You could use the *rm* command to remove each file individually, and then use the *rmdir* command to remove the directory. Note that *rmdir* will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the *-r* (for recursive) flag to *rm* to remove all the files and the directory in one go: **Logical Answer**: cd ~ rm test/* rmdir test **Easier Answer**: cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies. 4.12) Recreate the test directory in your home directory and copy all the files from /vlsci/TRAINING/shared/Intro_to_Unix back into the test directory. Hint See exercises 4.1 and 4.2 Answer Repeat exercises 4.1 and 4.2. $ cd ~ $ mkdir test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test 4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c Hint Use *man* if you can't guess how it might work. Answer $ cd ~/test $ cat hello.c #include <stdio.h> int main(void) { printf (\"Hello World\\n\"); return 0; } *hello.c* contains the source code of a C program. The compiled executable version of this code is in the file called *hi*, which you can run like so: $ ./hi Hello World 4.14) Use the head command to view the first 20 lines of the file sample_1.fastq Hint Remember your *best* friend! Additional Hint Use *man* to find out what option you need to add to display a given number of *lines*. Answer $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa`a```^aaaaa`_]aaa`aaa__a_X]`` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa 4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq Hint It's very much like *head*. Answer tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab]abbaa`babaaabbb`bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 `aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__ 4.16) Use the grep command to find out all the lines in moby.txt that contain the word \"Ahab\" Hint One might say we are 'looking for the *pattern* \"Ahab\"' Additional Hint $ man grep ... SYNOPSIS grep [OPTIONS] PATTERN [FILE...] ... Answer $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the *wc -l* command: $ grep Ahab moby.txt | wc -l 491 which shows that there are *491* lines in *moby.txt* that contain the word Ahab. 4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera). Hint One might say we are *ignoring case*. Additional Hint $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. (-i is specified by POSIX.) ... Answer Use the *-i* flag to *grep* to make it perform case insensitive search: $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [Project Gutenberg Editor's Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to *wc -l* to count the number of lines: $ grep -i the expectations.txt | wc -l 8165 4.18) Use the gzip command to compress the file sample_1.fastq . Use gunzip to decompress it back to the original contents. Hint Use the above commands along with *man* and *ls* to see what happens to the file. Answer Check the file size of sample_1.fastq before compressing it: # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq You will see that when it was compressed it is *26997595* bytes in size, making it about *0.3* times the size of the original file. **Note**: in the above section the lines starting with *#* are comments so don't need to be copied but if you do then they wont do anything. Topic 5: Pipes, output redirection and shell scripts In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \"tip of the iceberg\". Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut 5.1) How many reads are contained in the file sample_1.fastq ? Hint Examine some of the file to work out how many lines each *read* takes up. Additional Hint Count the number of lines Answer We can answer this question by counting the number of lines in the file and dividing by 4: $ wc -l sample_1.fastq 3000000 **Answer**: There are *3000000* lines in the file representing *750000* reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called *bc*: $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. *bc* is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera). 5.2) How many reads in sample_1.fastq contain the sequence GATTACA ? Hint Check out exercise 4.16 Answer Use *grep* to find all the lines that contain *GATTACA* and \"pipe\" the output to *wc -l* to count them: $ grep GATTACA sample_1.fastq | wc -l 1119 **Answer**: *1119* If you are unsure about the possibility of upper and lower case characters then consider using the *-i* (ignore case option for grep). 5.3) On what line numbers do the sequences containing GATTACA occur? Hint We are looking for the *line numbers*. Additional Hint Check out the manpage for *grep* and/or *nl* Answer You can use the *-n* flag to grep to make it prefix each line with a line number: **Answer 1**: $ grep -n GATTACA sample_1.fastq 5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Answer 2**: Or you can use the *nl* command to number each line of sample_1.fastq and then search for *GATTACA* in the numbered lines: $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Just the line numbers**: If you just want to see the line numbers then you can \"pipe\" the output of the above command into *cut -f 1*: $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... *cut* will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the *-f 1* option) $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... 5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning. Hint Check answer to 5.3. Answer $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with *control-c* (hold the *control* key down and simultaneously press the \"*c*\" character). 5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl . Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and ' / ' to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, *> FILENAME* is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" Answer $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \"*>*\" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the *head* command like so: $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... The *less* command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the '*/*' followed by the search term. You can quit by pressing \"*q*\". Note that the *less* command is used by default to display man pages. $ less sample_1.fastq.nl 5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq into TSV format: $ cat sample_1.fastq | paste - - - - > sample_1.tsv Answer The *'-'* (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all will support it. The *paste* command is useful for merging multiple files together line-by-line, such that the *Nth* line from each file is joined together into one line in the output, separated by default with a *tab* character. In the above example we give paste 4 copies of the contents of *sample_1.fastq*, which causes it to join consecutive groups of 4 lines from the file into one line of output. 5.7) Do you expect the output of the following command to produce the same output as above? and why? $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) Hint Use *less* to examine it. Answer **Answer**: No, in the second instance we get 4 copies of each line. **Why**: In the first command *paste* will use the input file (standard input) 4 times since the *cat* command will only give one copy of the file to *paste*, where as, in the second command *paste* will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point. 5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file. Hint Remember the *wc* command. Answer We can count the number of lines in *sample_1.tsv* using *wc*: $ wc -l sample_1.tsv The output should be *750000* as expected (1/4 of the number of lines in sample_1.fastq). To view the first *20* lines of *sample_1.tsv* use the *head* command: $ head -20 sample_1.tsv 5.9) Use the cut command to print out the second column of sample_1.tsv . Redirect the output to a file called sample_1.dna.txt . Hint See exercise 5.3 (for cut) and 5.5 (redirection) Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using *head* or *less*. 5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt . Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As. Hint Use *man* (sort) and see exercise 5.5 (redirection) Answer $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running *head* on the output file reveals that there are duplicate DNA sequences in the input FASTQ file. 5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt , redirect the result to sample_1.dna.uniq.txt . Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt . Hint I am pretty sure you have already used *man* (or just guessed how to use *uniq*). You're also a gun at redirection now. Answer $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of *sample_1.dna.uniq.txt* to check that the duplicate DNA sequences have been removed. 5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt ? Hint Checkout the *uniq* manpage Additional Hint Look at the man page for uniq. Answer Use the *-d* flag to *uniq* to print out only the duplicated lines from the file: $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt 5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq. Hint That is, *piping* most of the commands you used above instead of redirecting to file Additional Hint i.e. 6 commands (*cat*, *paste*, *cut*, *sort*, *uniq*, *wc*) Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: **Answer**: $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have *56079* lines. 5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq. Hint Check out the *sleepy* file (with *cat* or *nano*); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out *chmod* command). Answer Put the answer to *5.13* into a file called *sample_1_dups.sh* (or whatever you want). Use *nano* to create the file. **Answer**: the contents of the file will look like this: #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: $ chmod +x sample_1_dups.sh You can run the script like so: $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13. 5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter. Hint Shell scripts can refer to command line arguments by their position using special variables called *$0*, *$1*, *$2* and so on. Additional Hint *$0* refers to the name of the script as it was called on the command line. *$1* refers to the first command line argument, and so on. Answer Copy the shell script from *5.14* into a new file: $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: $ ./fastq_dups.sh sample_1.fastq In the above example the script takes *sample_1.fastq* as input and prints the number of duplicated sequences as output. **A better Answer**: Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: #!/bin/bash if [ $# -eq 1 ]; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The '*if ...; then*' line means: do the following line(s) ONLY if the *...* (called condition) bit is true. The '*else*' line means: otherwise do the following line(s) instead. Note: it is optional. The '*fi*' line means: this marks the end of the current *if* or *else* section. The '*[ $# -eq 1 ]*' part is the condition: * *$#*: is a special shell variable that indicates how many command line arguments were given. * *-eq*: checks if the numbers on either side of it are equal. * *1*: is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you. 5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file. Answer We can add a loop to our script to accept multiple input FASTQ files: #!/bin/bash for file in $@; do dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l) echo \"$file $dups\" done There's a lot going on in this script. The *$@* is a sequence of all command line arguments. The '*for ...; do*' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called *file*. The *$(...)* allow us to capture the output of another command (in-place of the *...*). In this case we capture the output of the pipeline and save it to the variable called *dups*. If you had multiple FASTQ files available you could run the script like so: ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY Finished Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don't forget to complete the training survey and give it back to the workshop facilitators.","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/#introduction-to-unix","text":"A hands-on workshop covering the basics of the Unix/Linux command line interface.","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/#overview","text":"Knowledge of the Unix operating system is fundamental to being productive on HPC systems. This workshop will introduce you to the fundamental Unix concepts by way of a series of hands-on exercises. The workshop is facilitated by experienced Unix users who will be able to guide you through the exercises and offer assistance where needed.","title":"Overview"},{"location":"tutorials/unix/unix/#learning-objectives","text":"At the end of the course, you will be able to: Log into a Unix machine remotely Organise your files into directories Change file permissions to improve security and safety Create and edit files with a text editor Copy files between directories Use command line programs to manipulate files Automate your workflow using shell scripts","title":"Learning Objectives"},{"location":"tutorials/unix/unix/#requirements","text":"The workshop is intended for beginners with no prior experience in Unix. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/unix/unix/#introduction","text":"Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the Unix concepts. The slides are available if you would like. Additionally the following reference material is available for later use. Reference Material ### Why use unix * **Powerful**: Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. * **Big data**: Unix programs are designed to handle large data sets * **Flexible**: Small programs that can be arranged in many ways to solve your problems * **Automation**: Scripting allows you to do many tasks in one step and repeat steps many times * **Pipelines**: Unix programs are designed to be 'chained' together to form long multi-step pipelines * **Science Software**: Lots of Scientific software is designed to run in a Unix environment ### User interface The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. ### Command prompt The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: * **Time**: the time (when the last command finished) * **Username**: the username that you are logged in as * **Hostname**: the name of the computer that your are connected to * **Current working directory**: the current position within the file system that your are working. More to follow * **Prompt**: this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign. ### Command line Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): * **Command**: this is the name of the program (command) that you want to run * **Flag**: these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. * **Long flag**: same as flag except they are generally two dashes (--) followed by a word (or two) * **Option**: set the value of a configurable option. They are a flag (or long flag) followed by a value * **Anonymous options**: these are one or more options that are specified in the required order * **Quoted value**: if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value. ### File system The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. #### Absolute file names Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. **Absolute file name**: */settings* **Absolute file name**: */home/user1/file01.txt* **Absolute file name**: */home/user2* **Note**: the final slash is not needed (but generally doesn't hurt if it is present). #### Current working directory The *current working directory* is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. #### Relative file names Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to */home* you can leave this part from the beginning of the filename. **Relative file name**: *user1/muscle.fq* (Note: the absence of the leading slash) **Special file names**: There are a few further short cuts for typing relative file names: * *~* (Tilde): is a short cut to your home directory * *.* (dot): is a short cut for the current directory * *..* (2x dot): means the parent (or one directory up) from current directory * *...* (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. *../..* **Note**: the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to */home/user2* the relative path to muscle.fq is different. **Relative file name**: *../user1/muscle.fq*","title":"Introduction"},{"location":"tutorials/unix/unix/#topic-1-remote-log-in","text":"In this topic we will learn how to connect to a Unix computer via a program called ssh and run a few basic commands.","title":"Topic 1: Remote log in"},{"location":"tutorials/unix/unix/#connecting-to-a-unix-computer","text":"To begin this workshop you will need to connect to an HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC's tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. barcoo is a high performance computer for Melbourne Bioinformatics users. Logging in connects your local computer (e.g. laptop) to barcoo, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on barcoo for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of barcoo, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into barcoo over ssh.","title":"Connecting to a Unix computer"},{"location":"tutorials/unix/unix/#exercises","text":"","title":"Exercises"},{"location":"tutorials/unix/unix/#11-when-youve-logged-into-the-unix-server-run-the-following-commands-and-see-what-they-do","text":"who whoami date cal hostname /vlsci/TRAINING/shared/Intro_to_Unix/hi Answer * **who**: displays a list of the users who are currently using this Unix computer. * **whoami**: displays your username (i.e. they person currently logged in). * **date**: displays the current date and time. * **cal**: displays a calendar on the terminal. It can be configured to display more than just the current month. * **hostname**: displays the name of the computer we are logged in to. * **/vlsci/TRAINING/shared/Intro_to_Unix/hi**: displays the text \"Hello World\"","title":"1.1) When you've logged into the Unix server, run the following commands and see what they do:"},{"location":"tutorials/unix/unix/#topic-2-exploring-your-home-directory","text":"In this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer.","title":"Topic 2: Exploring your home directory"},{"location":"tutorials/unix/unix/#21-use-the-ls-command-to-list-the-files-in-your-home-directory-how-many-files-are-there","text":"Hint Literally, type *ls* and press the *ENTER* key. Answer $ ls exp01 file01 muscle.fq When running the *ls* command with no options it will list files in your current working directory. The place where you start when you first login is your *HOME* directory. **Answer**: 3 (exp01, file01 and muscle.fq) The above answer is not quite correct. There are a number of hidden files in your home directory as well.","title":"2.1) Use the ls command to list the files in your home directory.  How many files are there?"},{"location":"tutorials/unix/unix/#22-what-flag-might-you-use-to-display-all-files-with-the-ls-command-how-many-files-are-really-there","text":"Hint Take the *all* quite literally. Additional Hint Type *ls --all* and press the *ENTER* key. Answer **Answer 1**: *--all* (or *-a*) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely *.* and *..* which are not real files but instead, shortcuts. *.* is a shortcut for the current directory and *..* a shortcut for the directory above the current one. **Answer 2**: 10 files (don't count *.* and *..*)","title":"2.2) What flag might you use to display all files with the ls command?  How many files are really there?"},{"location":"tutorials/unix/unix/#23-what-is-the-full-path-name-of-your-home-directory","text":"Hint Remember your *Current Working Directory* starts in your *home* directory. Additional Hint Try a shortened version of *print working directory* Answer You can find out the full path name of the current working directory with the *pwd* command. Your home directory will look something like this: $ pwd /home/trainingXX **Answer**: */vlsci/TRAINING/trainXX* where *XX* is replaced by some 2 digit sequence. **Alternate method**: You can also find out the name of your home directory by printing the value of the *$HOME* shell variable: echo $HOME","title":"2.3) What is the full path name of your home directory?"},{"location":"tutorials/unix/unix/#24-run-ls-using-the-long-flag-l-how-did-the-output-change","text":"Hint Run *ls -l* Answer **Answer**: it changed the output to place 1 file/directory per line. It also added some extra information about each. $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11:28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq **Details**: drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 \\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/ permission | username group size date name /---^---\\ linkcount Where: * **permissions**: 4 parts, file type, user perms, group perms and other perms * *filetype*: 1 character, *d* = directory and *-* regular file * *user* permissions: 3 characters, *r* = read, *w* = write, *x* = execute and *-* no permission * *group* permissions: same as user except for users within the owner group * *other* permissions: same as user except for users that are not in either user *or* *group* * **username**: the user who *owns* this file/directory * **group**: the group name who *owns* this file/directory * **size**: the number of bytes this file/directory takes to store on disk * **date**: the date and time when this file/directory was *last edited* * **name**: name of the file * **linkcount**: technical detail which represents the number of links this file has in the file system (safe to ignore)","title":"2.4) Run ls using the long flag (-l), how did the output change?"},{"location":"tutorials/unix/unix/#25-what-type-of-file-is-exp01-and-musclefq","text":"Hint Check the output from the *ls -l*. Answer **Answer**: * *exp01*: Directory (given the 'd' as the first letter of its permissions) * *muscle.fq*: Regular File (given the '-')","title":"2.5) What type of file is exp01 and muscle.fq?"},{"location":"tutorials/unix/unix/#26-who-has-permission-to-read-write-and-execute-your-home-directory","text":"Hint You can also give *ls* a filename as the first option. Additional Hint *ls -l* will show you the contents of the *CWD*; how might you see the contents of the *parent* directory? (remember the slides) Answer If you pass the *-l* flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. **Method 1**: given we know the *CWD* is our home directory. $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... The *..* refers to the parent directory. **Method 2**: using $HOME. This works no matter what our *CWD* is set to. You could list the permissions of all files and directories in the parent directory of your home: $ ls -l $HOME/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... In this case we use the shell variable to refer to our home directory. **Method 3**: using *~* (tilde) shortcut You may also refer to your home directory using the *~* (tilde) character: $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like *trainXX*. Where *XX* is replaced by a two digit string. **Altername**: using the *-a* flag and looking at the *.* (dot) special file. $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 . ... **Answer**: *drwxr-x---* * **You**: read (see filenames), write (add, delete files), execute (change your CWD to this directory). * **Training users**: read, execute * **Everyone else**: No access **Discussion on Permissions**: The permission string is *\"drwxr-x---\"*. The *d* means it is a directory. The *rwx* means that the owner of the directory (your user account) can *read*, *write* and *execute* the directory. Execute permissions on a directory means that you can *cd* into the directory. The *r-x* means that anyone in the same user group as *training* can read or execute the directory. The *---* means that nobody else (other users on the system) can do anything with the directory. man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q .","title":"2.6) Who has permission to read, write and execute your home directory?"},{"location":"tutorials/unix/unix/#27-use-the-man-command-to-find-out-what-the-h-flag-does-for-ls","text":"Hint Give *ls* as an option to *man* command. Additional Hint *man ls* Answer Use the following command to view the *man* page for *ls*: $ man ls **Answer**: You should discover that the *-h* option prints file sizes in human readable format -h, --human-readable with -l, print sizes in human readable format (e.g., 1K 234M 2G)","title":"2.7) Use the man command to find out what the -h flag does for ls"},{"location":"tutorials/unix/unix/#28-use-the-h-how-did-the-output-change-of-musclefq","text":"Hint Don't forget the *-l* option too. Additional Hint Run *ls -lh* Answer $ ls -lh ... -rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq **Answer**: it changed the output so the *filesize* of *muscle.fq* is now *2.5K* instead of *2461*","title":"2.8) Use the -h, how did the output change of muscle.fq?"},{"location":"tutorials/unix/unix/#topic-3-exploring-the-file-system","text":"In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file","title":"Topic 3: Exploring the file system"},{"location":"tutorials/unix/unix/#31-print-the-value-of-your-current-working-directory","text":"Answer The *pwd* command prints the value of your current working directory. $ pwd /home/training01","title":"3.1) Print the value of your current working directory."},{"location":"tutorials/unix/unix/#32-list-the-contents-of-the-root-directory-called-forward-slash","text":"Hint *ls* expects one or more anonymous options which are the files/directories to list. Answer $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that *ls* can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory.","title":"3.2) List the contents of the root directory, called '/' (forward slash)."},{"location":"tutorials/unix/unix/#33-use-the-cd-command-to-change-your-working-directory-to-the-root-directory-did-your-prompt-change","text":"Hint *cd* expects a single option which is the directory to change to Answer The *cd* command changes the value of your current working directory. To change to the root directory use the following command: $ cd / **Answer**: Yes, it now says the CWD is */* instead of *~*. Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen.","title":"3.3) Use the cd command to change your working directory to the root directory.  Did your prompt change?"},{"location":"tutorials/unix/unix/#34-list-the-contents-of-the-cwd-and-verify-it-matches-the-list-in-32","text":"Hint *ls* Answer Assuming you have changed to the root directory then this can be achieved with *ls*, or *ls -a* (for all files) or *ls -la* for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys **Answer**: Yes, we got the same output as exercise 3.2","title":"3.4) List the contents of the CWD and verify it matches the list in 3.2"},{"location":"tutorials/unix/unix/#35-change-your-current-working-directory-back-to-your-home-directory-what-is-the-simplest-unix-command-that-will-get-you-back-to-your-home-directory-from-anywhere-else-in-the-file-system","text":"Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory Additional Hint *$HOME*, *~*, */vlsci/TRAINING/trainXX* are all methods to name your home directory. Yet there is a simpler method; the answer is buried in *man cd* however *cd* doesn't have its own manpage so you will need to search for it. Answer Use the *cd* command to change your working directory to your home directory. There are a number of ways to refer to your home directory: cd $HOME is equivalent to: cd ~ The simplest way to change your current working directory to your home directory is to run the *cd* command with no arguments: **Answer**: the simplest for is cd with NO options. cd This is a special-case behaviour which is built into *cd* for convenience.","title":"3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system?"},{"location":"tutorials/unix/unix/#36-change-your-working-directory-to-the-following-directory","text":"/vlsci/TRAINING/shared/Intro_to_Unix Answer **Answer**: *cd /vlsci/TRAINING/shared/Intro_to_Unix*","title":"3.6) Change your working directory to the following directory:"},{"location":"tutorials/unix/unix/#37-list-the-contents-of-that-directory-how-many-files-does-it-contain","text":"Hint *ls* Answer You can do this with *ls* $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy **Answer**: 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy)","title":"3.7) List the contents of that directory. How many files does it contain?"},{"location":"tutorials/unix/unix/#38-what-kind-of-file-is-vlscitrainingsharedintro_to_unixsleepy","text":"Hint Take the word *file* quite literally. Additional Hint *file sleepy* Answer Use the *file* command to get extra information about the contents of a file: Assuming your current working directory is */vlsci/TRAINING/shared/Intro_to_Unix* $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: $ file /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Bourne-Again shell script text executable **Answer**: Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The *file* command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: man file 3.9) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/hi ? Hint Take the word *file* quite literally. Answer Use the file command again. If you are in the same directory as *hi* then: $ file hi ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.9, not stripped **Answer**: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called *hi* contains a binary executable program (raw instructions that the computer can execute directly).","title":"3.8) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/sleepy?"},{"location":"tutorials/unix/unix/#310-what-are-the-file-permissions-of-the-following-file-and-what-do-they-mean","text":"/vlsci/TRAINING/shared/Intro_to_Unix/sleepy Hint Remember the *ls* command, and don't forget the *-l* flag Answer You can find the permissions of *sleepy* using the *ls* command with the *-l* flag. If you are in the same directory as *sleepy* then: $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16:36 sleepy **Answer**: The Answer is dependent on the computer you are connected too however will follow something like above. We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and write-able only to arobinson. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the *ln* command.","title":"3.10) What are the file permissions of the following file and what do they mean?"},{"location":"tutorials/unix/unix/#311-change-your-working-directory-back-to-your-home-directory-ready-for-the-next-topic","text":"Hint *cd* Answer You should know how to do this with the cd command: cd","title":"3.11) Change your working directory back to your home directory ready for the next topic."},{"location":"tutorials/unix/unix/#topic-4-working-with-files-and-directories","text":"In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections.","title":"Topic 4: Working with files and directories"},{"location":"tutorials/unix/unix/#41-in-your-home-directory-make-a-sub-directory-called-test","text":"Hint You are trying to *make a directory*, which of the above commands looks like a shortened version of this? Additional Hint *mkdir* Answer Make sure you are in your home directory first. If not *cd* to your home directory. Use the *mkdir* command to make new directories: $ mkdir test Use the *ls* command to check that the new directory was created. $ ls exp01 file01 muscle.fq test","title":"4.1) In your home directory make a sub-directory called test."},{"location":"tutorials/unix/unix/#42-copy-all-the-files-from-the-following-directory-into-the-newly-created-test-directory","text":"/vlsci/TRAINING/shared/Intro_to_Unix Hint You are trying to *copy*, which of the above commands looks like a shortened version of this? Additional Hint $ man cp ... SYNOPSIS cp [OPTION]... [-T] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. which means *cp* expects zero or more flags, a SOURCE file followed by a DEST file or directory Answer Use the *cp* command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to *cp*. You only need to perform one of these ways, but we show multiple ones for your reference. **Answer 1**: From your home directory: $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test **Answer 2**: Change to the test directory and then copy (assuming you started in your home directory): $ cd test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* . In the example above the '*.*' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. **Answer 3**: Change to the \\end{UNIX_TRAINING_FILES_PATH} directory and then copy: cd /vlsci/TRAINING/shared/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. Note : This exercise assumes that the copy command from the previous exercise was successful.","title":"4.2) Copy all the files from the following directory into the newly created test directory:"},{"location":"tutorials/unix/unix/#43-check-that-the-file-size-of-expectationstxt-is-the-same-in-both-the-directory-that-you-copied-it-from-and-the-directory-that-you-copied-it-to","text":"Hint Remember *ls* can show you the file size (with one of its flags) Additional Hint *ls -l* Answer Use *ls -l* to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: $ ls -l /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. **Answer**: They should each be *1033773* bytes **Alternate**: Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the *-h* option for ls: $ ls -lh /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as *1010K*. Larger files are reported in megabytes, gigabytes etcetera. Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test","title":"4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to."},{"location":"tutorials/unix/unix/#44-check-that-the-contents-of-expectationstxt-are-the-same-in-both-the-directory-that-you-copied-it-from-and-the-directory-that-you-copied-it-to","text":"Hint What is the opposite of *same*? Additional Hint *diff*erence Answer Use the *diff* command to compare the contents of two files. $ diff /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the *diff* command will NOT produce any output) **Answer**: Yes, they are the same since no output was given.","title":"4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to."},{"location":"tutorials/unix/unix/#45-how-many-lines-words-and-characters-are-in-expectationstxt","text":"Hint Initialisms are key Additional Hint *w*ord *c*ount Answer Use the *wc* (for \"word count\") to count the number of characters, lines and words in a file: $ wc expectations.txt 20415 187465 1033773 expectations.txt **Answer**: There are *20415* lines, *187465* words and *1033773* characters in expectations.txt. To get just the line, word or character count: $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt","title":"4.5) How many lines, words and characters are in expectations.txt?"},{"location":"tutorials/unix/unix/#46-open-testexpectationstxt-in-the-nano-text-editor-delete-the-first-line-of-text-and-save-your-changes-to-the-file-exit-nano","text":"Hint *nano FILENAME* Once *nano* is open it displays some command hints along the bottom of the screen. Additional Hint *^O* means hold the *Control* (or CTRL) key while pressing the *o*. Despite what it displays, you need to type the lower-case letter that follows the *^* character. WriteOut is another name for Save. Answer Take some time to play around with the *nano* text editor. *Nano* is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as *vim* and *emacs*, however they take a substantial amount of time to learn.","title":"4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano."},{"location":"tutorials/unix/unix/#47-did-the-changes-you-made-to-testexpectationstxt-have-any-effect-on-vlscitrainingsharedintro_to_unix","text":"How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 Additional Hint Use *diff* Answer Use *diff* to check that the two files are different after you have made the change to the copy of *expectations.txt* in your *~/test* directory. diff ~/test/expectations.txt \\ /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt You could also use *ls* to check that the files have different sizes.","title":"4.7) Did the changes you made to ~/test/expectations.txt have any effect on /vlsci/TRAINING/shared/Intro_to_Unix?"},{"location":"tutorials/unix/unix/#48-in-your-test-subdirectory-rename-expectationstxt-to-footxt","text":"Hint Another way to think of it is *moving* it from *expectations.txt* to *foo.txt* Additional Hint *mv* Use *man mv* if you need to work out how to use it. Answer Use the *mv* command to rename the file: $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy","title":"4.8) In your test subdirectory, rename expectations.txt to foo.txt."},{"location":"tutorials/unix/unix/#49-rename-footxt-back-to-expectationstxt","text":"Answer Use the *mv* command to rename the file: $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use *ls* to check that the file is in fact renamed.","title":"4.9) Rename foo.txt back to expectations.txt."},{"location":"tutorials/unix/unix/#410-remove-the-file-expectationstxt-from-your-test-directory","text":"Hint We are trying to *remove* a file, check the commands at the top of this topic. Additional Hint *rm* Answer Use the *rm* command to remove files (carefully): $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy","title":"4.10) Remove the file expectations.txt from your test directory."},{"location":"tutorials/unix/unix/#411-remove-the-entire-test-directory-and-all-the-files-within-it","text":"Hint We are trying to *remove a directory*. Additional Hint You could use *rmdir* but there is an easier way using just *rm* and a flag. Answer You could use the *rm* command to remove each file individually, and then use the *rmdir* command to remove the directory. Note that *rmdir* will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the *-r* (for recursive) flag to *rm* to remove all the files and the directory in one go: **Logical Answer**: cd ~ rm test/* rmdir test **Easier Answer**: cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies.","title":"4.11) Remove the entire test directory and all the files within it."},{"location":"tutorials/unix/unix/#412-recreate-the-test-directory-in-your-home-directory-and-copy-all-the-files-from-vlscitrainingsharedintro_to_unix-back-into-the-test-directory","text":"Hint See exercises 4.1 and 4.2 Answer Repeat exercises 4.1 and 4.2. $ cd ~ $ mkdir test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test","title":"4.12) Recreate the test directory in your home directory and copy all the files from /vlsci/TRAINING/shared/Intro_to_Unix back into the test directory."},{"location":"tutorials/unix/unix/#413-change-directories-to-test-and-use-the-cat-command-to-display-the-entire-contents-of-the-file-helloc","text":"Hint Use *man* if you can't guess how it might work. Answer $ cd ~/test $ cat hello.c #include <stdio.h> int main(void) { printf (\"Hello World\\n\"); return 0; } *hello.c* contains the source code of a C program. The compiled executable version of this code is in the file called *hi*, which you can run like so: $ ./hi Hello World","title":"4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c"},{"location":"tutorials/unix/unix/#414-use-the-head-command-to-view-the-first-20-lines-of-the-file-sample_1fastq","text":"Hint Remember your *best* friend! Additional Hint Use *man* to find out what option you need to add to display a given number of *lines*. Answer $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa`a```^aaaaa`_]aaa`aaa__a_X]`` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa","title":"4.14) Use the head command to view the first 20 lines of the file sample_1.fastq"},{"location":"tutorials/unix/unix/#415-use-the-tail-command-to-view-the-last-8-lines-of-the-file-sample_1fastq","text":"Hint It's very much like *head*. Answer tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab]abbaa`babaaabbb`bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 `aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__","title":"4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq"},{"location":"tutorials/unix/unix/#416-use-the-grep-command-to-find-out-all-the-lines-in-mobytxt-that-contain-the-word-ahab","text":"Hint One might say we are 'looking for the *pattern* \"Ahab\"' Additional Hint $ man grep ... SYNOPSIS grep [OPTIONS] PATTERN [FILE...] ... Answer $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the *wc -l* command: $ grep Ahab moby.txt | wc -l 491 which shows that there are *491* lines in *moby.txt* that contain the word Ahab.","title":"4.16) Use the grep command to find out all the lines in moby.txt that contain the word \"Ahab\""},{"location":"tutorials/unix/unix/#417-use-the-grep-command-to-find-out-all-the-lines-in-expectationstxt-that-contain-the-word-the-with-a-case-insensitive-search-it-should-count-the-the-the-the-etcetera","text":"Hint One might say we are *ignoring case*. Additional Hint $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. (-i is specified by POSIX.) ... Answer Use the *-i* flag to *grep* to make it perform case insensitive search: $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [Project Gutenberg Editor's Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to *wc -l* to count the number of lines: $ grep -i the expectations.txt | wc -l 8165","title":"4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera)."},{"location":"tutorials/unix/unix/#418-use-the-gzip-command-to-compress-the-file-sample_1fastq-use-gunzip-to-decompress-it-back-to-the-original-contents","text":"Hint Use the above commands along with *man* and *ls* to see what happens to the file. Answer Check the file size of sample_1.fastq before compressing it: # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq You will see that when it was compressed it is *26997595* bytes in size, making it about *0.3* times the size of the original file. **Note**: in the above section the lines starting with *#* are comments so don't need to be copied but if you do then they wont do anything.","title":"4.18) Use the gzip command to compress the file sample_1.fastq. Use gunzip to decompress it back to the original contents."},{"location":"tutorials/unix/unix/#topic-5-pipes-output-redirection-and-shell-scripts","text":"In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \"tip of the iceberg\". Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut","title":"Topic 5: Pipes, output redirection and shell scripts"},{"location":"tutorials/unix/unix/#51-how-many-reads-are-contained-in-the-file-sample_1fastq","text":"Hint Examine some of the file to work out how many lines each *read* takes up. Additional Hint Count the number of lines Answer We can answer this question by counting the number of lines in the file and dividing by 4: $ wc -l sample_1.fastq 3000000 **Answer**: There are *3000000* lines in the file representing *750000* reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called *bc*: $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. *bc* is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera).","title":"5.1) How many reads are contained in the file sample_1.fastq?"},{"location":"tutorials/unix/unix/#52-how-many-reads-in-sample_1fastq-contain-the-sequence-gattaca","text":"Hint Check out exercise 4.16 Answer Use *grep* to find all the lines that contain *GATTACA* and \"pipe\" the output to *wc -l* to count them: $ grep GATTACA sample_1.fastq | wc -l 1119 **Answer**: *1119* If you are unsure about the possibility of upper and lower case characters then consider using the *-i* (ignore case option for grep).","title":"5.2) How many reads in sample_1.fastq contain the sequence GATTACA?"},{"location":"tutorials/unix/unix/#53-on-what-line-numbers-do-the-sequences-containing-gattaca-occur","text":"Hint We are looking for the *line numbers*. Additional Hint Check out the manpage for *grep* and/or *nl* Answer You can use the *-n* flag to grep to make it prefix each line with a line number: **Answer 1**: $ grep -n GATTACA sample_1.fastq 5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Answer 2**: Or you can use the *nl* command to number each line of sample_1.fastq and then search for *GATTACA* in the numbered lines: $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Just the line numbers**: If you just want to see the line numbers then you can \"pipe\" the output of the above command into *cut -f 1*: $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... *cut* will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the *-f 1* option) $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ...","title":"5.3) On what line numbers do the sequences containing GATTACA occur?"},{"location":"tutorials/unix/unix/#54-use-the-nl-command-to-print-each-line-of-sample_1fastq-with-its-corresponding-line-number-at-the-beginning","text":"Hint Check answer to 5.3. Answer $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with *control-c* (hold the *control* key down and simultaneously press the \"*c*\" character).","title":"5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning."},{"location":"tutorials/unix/unix/#55-redirect-the-output-of-the-previous-command-to-a-file-called-sample_1fastqnl","text":"Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and ' / ' to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, *> FILENAME* is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" Answer $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \"*>*\" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the *head* command like so: $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... The *less* command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the '*/*' followed by the search term. You can quit by pressing \"*q*\". Note that the *less* command is used by default to display man pages. $ less sample_1.fastq.nl","title":"5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl."},{"location":"tutorials/unix/unix/#56-the-four-lines-per-read-format-of-fastq-is-cumbersome-to-deal-with-often-it-would-be-preferable-if-we-could-convert-it-to-tab-separated-value-tsv-format-such-that-each-read-appears-on-a-single-line-with-each-of-its-fields-separated-by-tabs-use-the-following-command-to-convert-sample_1fastq-into-tsv-format","text":"$ cat sample_1.fastq | paste - - - - > sample_1.tsv Answer The *'-'* (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all will support it. The *paste* command is useful for merging multiple files together line-by-line, such that the *Nth* line from each file is joined together into one line in the output, separated by default with a *tab* character. In the above example we give paste 4 copies of the contents of *sample_1.fastq*, which causes it to join consecutive groups of 4 lines from the file into one line of output.","title":"5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq into TSV format:"},{"location":"tutorials/unix/unix/#57-do-you-expect-the-output-of-the-following-command-to-produce-the-same-output-as-above-and-why","text":"$ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) Hint Use *less* to examine it. Answer **Answer**: No, in the second instance we get 4 copies of each line. **Why**: In the first command *paste* will use the input file (standard input) 4 times since the *cat* command will only give one copy of the file to *paste*, where as, in the second command *paste* will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point.","title":"5.7) Do you expect the output of the following command to produce the same output as above? and why?"},{"location":"tutorials/unix/unix/#58-check-that-sample_1tsv-has-the-correct-number-of-lines-use-the-head-command-to-view-the-first-20-lines-of-the-file","text":"Hint Remember the *wc* command. Answer We can count the number of lines in *sample_1.tsv* using *wc*: $ wc -l sample_1.tsv The output should be *750000* as expected (1/4 of the number of lines in sample_1.fastq). To view the first *20* lines of *sample_1.tsv* use the *head* command: $ head -20 sample_1.tsv","title":"5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file."},{"location":"tutorials/unix/unix/#59-use-the-cut-command-to-print-out-the-second-column-of-sample_1tsv-redirect-the-output-to-a-file-called-sample_1dnatxt","text":"Hint See exercise 5.3 (for cut) and 5.5 (redirection) Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using *head* or *less*.","title":"5.9) Use the cut command to print out the second column of sample_1.tsv. Redirect the output to a file called sample_1.dna.txt."},{"location":"tutorials/unix/unix/#510-use-the-sort-command-to-sort-the-lines-of-sample_1dnatxt-and-redirect-the-output-to-sample_1dnasortedtxt-use-head-to-look-at-the-first-few-lines-of-the-output-file-you-should-see-a-lot-of-repeated-sequences-of-as","text":"Hint Use *man* (sort) and see exercise 5.5 (redirection) Answer $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running *head* on the output file reveals that there are duplicate DNA sequences in the input FASTQ file.","title":"5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt. Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As."},{"location":"tutorials/unix/unix/#511-use-the-uniq-command-to-remove-duplicate-consecutive-lines-from-sample_1dnasortedtxt-redirect-the-result-to-sample_1dnauniqtxt-compare-the-number-of-lines-in-sample1_dnatxt-to-the-number-of-lines-in-sample_1dnauniqtxt","text":"Hint I am pretty sure you have already used *man* (or just guessed how to use *uniq*). You're also a gun at redirection now. Answer $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of *sample_1.dna.uniq.txt* to check that the duplicate DNA sequences have been removed.","title":"5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt, redirect the result to sample_1.dna.uniq.txt. Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt."},{"location":"tutorials/unix/unix/#512-can-you-modify-the-command-from-above-to-produce-only-those-sequences-of-dna-which-were-duplicated-in-sample_1dnasortedtxt","text":"Hint Checkout the *uniq* manpage Additional Hint Look at the man page for uniq. Answer Use the *-d* flag to *uniq* to print out only the duplicated lines from the file: $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt","title":"5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt?"},{"location":"tutorials/unix/unix/#513-write-a-shell-pipeline-which-will-print-the-number-of-duplicated-dna-sequences-in-sample_1fastq","text":"Hint That is, *piping* most of the commands you used above instead of redirecting to file Additional Hint i.e. 6 commands (*cat*, *paste*, *cut*, *sort*, *uniq*, *wc*) Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: **Answer**: $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have *56079* lines.","title":"5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq."},{"location":"tutorials/unix/unix/#514-advanced-write-a-shell-script-which-will-print-the-number-of-duplicated-dna-sequences-in-sample_1fastq","text":"Hint Check out the *sleepy* file (with *cat* or *nano*); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out *chmod* command). Answer Put the answer to *5.13* into a file called *sample_1_dups.sh* (or whatever you want). Use *nano* to create the file. **Answer**: the contents of the file will look like this: #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: $ chmod +x sample_1_dups.sh You can run the script like so: $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13.","title":"5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq."},{"location":"tutorials/unix/unix/#515-advanced-modify-your-shell-script-so-that-it-accepts-the-name-of-the-input-fastq-file-as-a-command-line-parameter","text":"Hint Shell scripts can refer to command line arguments by their position using special variables called *$0*, *$1*, *$2* and so on. Additional Hint *$0* refers to the name of the script as it was called on the command line. *$1* refers to the first command line argument, and so on. Answer Copy the shell script from *5.14* into a new file: $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: $ ./fastq_dups.sh sample_1.fastq In the above example the script takes *sample_1.fastq* as input and prints the number of duplicated sequences as output. **A better Answer**: Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: #!/bin/bash if [ $# -eq 1 ]; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The '*if ...; then*' line means: do the following line(s) ONLY if the *...* (called condition) bit is true. The '*else*' line means: otherwise do the following line(s) instead. Note: it is optional. The '*fi*' line means: this marks the end of the current *if* or *else* section. The '*[ $# -eq 1 ]*' part is the condition: * *$#*: is a special shell variable that indicates how many command line arguments were given. * *-eq*: checks if the numbers on either side of it are equal. * *1*: is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you.","title":"5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter."},{"location":"tutorials/unix/unix/#516-advanced-modify-your-shell-script-so-that-it-accepts-zero-or-more-fastq-files-on-the-command-line-argument-and-outputs-the-number-of-duplicated-dna-sequences-in-each-file","text":"Answer We can add a loop to our script to accept multiple input FASTQ files: #!/bin/bash for file in $@; do dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l) echo \"$file $dups\" done There's a lot going on in this script. The *$@* is a sequence of all command line arguments. The '*for ...; do*' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called *file*. The *$(...)* allow us to capture the output of another command (in-place of the *...*). In this case we capture the output of the pipeline and save it to the variable called *dups*. If you had multiple FASTQ files available you could run the script like so: ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY","title":"5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file."},{"location":"tutorials/unix/unix/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don't forget to complete the training survey and give it back to the workshop facilitators.","title":"Finished"},{"location":"tutorials/using_git/","text":"PR reviewers and advice: Juan Nunez-Iglesias Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/using_git/Using_Git/","text":"Using Git and Github for revision control What is Git? Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly. Getting help There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough. A simple workflow Step 1, create a github account. Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics . Step 2, sign into github and create a repository. Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning) Step 3, clone your repository to your local computer. Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). $ git clone https://github.com/bjpop/test.git Cloning into 'test'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README.md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs Step 4, commit a file to the repository. Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: $ python hello.py hello world Check the status of your repository: $ git status # On branch master # Untracked files: # (use \"git add <file>...\" to include in what will be committed) # # hello.py nothing added to commit but untracked files present (use \"git add\" to track) Notice that git tells you that the new file hello.py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \"stage\" your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello.py have been staged and are ready to be committed. Notice that hello.py is no longer untracked. Commit your changes with a commit message: $ git commit -m \"A little greeting program\" [master b1cce11] A little greeting program 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 hello.py Re-check the status of your repository: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin/master ). Step 5, push your changes to github. Push the commit in your local repository to github (thus synchronising them). $ git push origin Username for 'https://github.com': <type your github username> Password for 'https://<your github username>@github.com': Counting objects: 4, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 305 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git 71a771a..b1cce11 master -> master Now if you look at your repository on github you should see the file hello.py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name: Step 6, create a branch in your local repository. You can ask git to tell you about the names of the current branches: $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello.py file: $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): $ git status # On branch documentation # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hello.py # no changes added to commit (use \"git add\" and/or \"git commit -a\") Stage the new changes and commit them, and check the status again: $ git add hello.py $ git commit -m \"Added a comment\" [documentation 9bbe430] Added a comment 1 files changed, 1 insertions(+), 0 deletions(-) $ git status # On branch documentation nothing to commit (working directory clean) Now we can push the new \u201cdocumentation\u201d branch to github: $ git push origin documentation Username for 'https://github.com': <your github username> Password for 'https://<your github username>@github.com': Counting objects: 5, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 314 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git * [new branch] documentation -> documentation On github you should be able to see the new branch: Step 7, merge the changes back into the master branch. To go back to the master branch you must check it out: $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: $ cat hello.py print(\"hello world\") Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: $ cat hello.py print(\"hello world\") #this is a comment Check the status of the master branch: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Push the changes in the master branch back to github: $ git push origin master Username for 'https://github.com': bjpop Password for 'https://bjpop@github.com': Total 0 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git b1cce11..9bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17:29:02 2013 -0700","title":"Using Git and Github for revision control"},{"location":"tutorials/using_git/Using_Git/#using-git-and-github-for-revision-control","text":"","title":"Using Git and Github for revision control"},{"location":"tutorials/using_git/Using_Git/#what-is-git","text":"Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly.","title":"What is Git?"},{"location":"tutorials/using_git/Using_Git/#getting-help","text":"There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough.","title":"Getting help"},{"location":"tutorials/using_git/Using_Git/#a-simple-workflow","text":"","title":"A simple workflow"},{"location":"tutorials/using_git/Using_Git/#step-1-create-a-github-account","text":"Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics .","title":"Step 1, create a github account."},{"location":"tutorials/using_git/Using_Git/#step-2-sign-into-github-and-create-a-repository","text":"Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning)","title":"Step 2, sign into github and create a repository."},{"location":"tutorials/using_git/Using_Git/#step-3-clone-your-repository-to-your-local-computer","text":"Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). $ git clone https://github.com/bjpop/test.git Cloning into 'test'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README.md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs","title":"Step 3, clone your repository to your local computer."},{"location":"tutorials/using_git/Using_Git/#step-4-commit-a-file-to-the-repository","text":"Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: $ python hello.py hello world Check the status of your repository: $ git status # On branch master # Untracked files: # (use \"git add <file>...\" to include in what will be committed) # # hello.py nothing added to commit but untracked files present (use \"git add\" to track) Notice that git tells you that the new file hello.py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \"stage\" your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello.py have been staged and are ready to be committed. Notice that hello.py is no longer untracked. Commit your changes with a commit message: $ git commit -m \"A little greeting program\" [master b1cce11] A little greeting program 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 hello.py Re-check the status of your repository: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin/master ).","title":"Step 4, commit a file to the repository."},{"location":"tutorials/using_git/Using_Git/#step-5-push-your-changes-to-github","text":"Push the commit in your local repository to github (thus synchronising them). $ git push origin Username for 'https://github.com': <type your github username> Password for 'https://<your github username>@github.com': Counting objects: 4, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 305 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git 71a771a..b1cce11 master -> master Now if you look at your repository on github you should see the file hello.py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name:","title":"Step 5, push your changes to github."},{"location":"tutorials/using_git/Using_Git/#step-6-create-a-branch-in-your-local-repository","text":"You can ask git to tell you about the names of the current branches: $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello.py file: $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): $ git status # On branch documentation # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hello.py # no changes added to commit (use \"git add\" and/or \"git commit -a\") Stage the new changes and commit them, and check the status again: $ git add hello.py $ git commit -m \"Added a comment\" [documentation 9bbe430] Added a comment 1 files changed, 1 insertions(+), 0 deletions(-) $ git status # On branch documentation nothing to commit (working directory clean) Now we can push the new \u201cdocumentation\u201d branch to github: $ git push origin documentation Username for 'https://github.com': <your github username> Password for 'https://<your github username>@github.com': Counting objects: 5, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 314 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git * [new branch] documentation -> documentation On github you should be able to see the new branch:","title":"Step 6, create a branch in your local repository."},{"location":"tutorials/using_git/Using_Git/#step-7-merge-the-changes-back-into-the-master","text":"branch. To go back to the master branch you must check it out: $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: $ cat hello.py print(\"hello world\") Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: $ cat hello.py print(\"hello world\") #this is a comment Check the status of the master branch: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Push the changes in the master branch back to github: $ git push origin master Username for 'https://github.com': bjpop Password for 'https://bjpop@github.com': Total 0 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git b1cce11..9bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17:29:02 2013 -0700","title":"Step 7, merge the changes back into the master"},{"location":"tutorials/var_detect_advanced/","text":"PR reviewers and advice: Clare Sloggett, Khalid Mahmood, Simon Gladman, Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/","text":"Variant Detection - Advanced Workshop Tutorial Overview In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information Background Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ Preparation Make sure you have an instance of Galaxy ready to go. If you don't have your own - go to our Galaxy-Tut or Galaxy Australia server. Log in so that your work will be saved. If you don't already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won't need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf . Section 1: Quality Control The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30). Section 2: Alignment and depth of coverage In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \"NA12878\" Set the platform to ILLUMINA Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download. If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it. Section 3. Local realignment Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant. Section 4. Calling variants with FreeBayes FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Section 5. Calling variants with GATK Unified Genotyper For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378. Section 6. Evaluate variants How can we evaluate our variants? We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data... (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \"^#\". ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\". As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \"FreeBayes\". As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\". Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to remove many false positives, but also a few true positives. Section 7. Annotation The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated ... and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF... header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \"missense_variant\". Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?","title":"Tutorial"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#variant-detection-advanced-workshop","text":"","title":"Variant Detection - Advanced Workshop"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#tutorial-overview","text":"In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information","title":"Tutorial Overview"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#background","text":"Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#preparation","text":"Make sure you have an instance of Galaxy ready to go. If you don't have your own - go to our Galaxy-Tut or Galaxy Australia server. Log in so that your work will be saved. If you don't already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won't need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf .","title":"Preparation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-1-quality-control","text":"The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).","title":"Section 1: Quality Control"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-2-alignment-and-depth-of-coverage","text":"In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \"NA12878\" Set the platform to ILLUMINA Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download. If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.","title":"Section 2: Alignment and depth of coverage"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-3-local-realignment","text":"Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.","title":"Section 3. Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-4-calling-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.","title":"Section 4. Calling variants with FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-5-calling-variants-with-gatk-unified-genotyper","text":"For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.","title":"Section 5. Calling variants with GATK Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-6-evaluate-variants","text":"How can we evaluate our variants? We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data... (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \"^#\". ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\". As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \"FreeBayes\". As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\". Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to remove many false positives, but also a few true positives.","title":"Section 6. Evaluate variants"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-7-annotation","text":"The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated ... and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF... header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \"missense_variant\". Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?","title":"Section 7. Annotation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/","text":"Introduction to Variant detection Background A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV) Variant Calling vs genotyping Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants. Process of variant calling Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position. Homozygous or Heterozygous mutations: What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events. Variant Calling Software: There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods Variant Calling using Samtools (Mpileup + bcftools) Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information Variant Calling using GATK-Unified Genotyper GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: P(Genotype | Data) = (P(Data | Genotype) * P(Genotype)) / P(Data) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information Variant Calling using FreeBayes FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information Evaluation of detected variants using Variant Eval The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file. Quality Matrix: TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance. Local realignment In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml The Galaxy workflow platform Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right. Data Format used in the tutorial Sequence Alignment Map format SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy Binary Sequence Alignment Map format BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV VCF file format What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. VCF format in Galaxy: Bcf file format: BCF format: BCF, or the binary variant call format, is the binary version of VCF. It keeps the same information in VCF, while much more efficient to process especially for many samples. The relationship between BCF and VCF is similar to that between BAM and SAM.","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#introduction-to-variant-detection","text":"","title":"Introduction to Variant detection"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#background","text":"A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV)","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-vs-genotyping","text":"Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants.","title":"Variant Calling vs genotyping"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#process-of-variant-calling","text":"Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.","title":"Process of variant calling"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#homozygous-or-heterozygous-mutations","text":"What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.","title":"Homozygous or Heterozygous mutations:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-software","text":"There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods","title":"Variant Calling Software:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-samtools-mpileup-bcftools","text":"Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information","title":"Variant Calling using Samtools (Mpileup + bcftools)"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-gatk-unified-genotyper","text":"GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: P(Genotype | Data) = (P(Data | Genotype) * P(Genotype)) / P(Data) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information","title":"Variant Calling using GATK-Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-freebayes","text":"FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information","title":"Variant Calling using FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#evaluation-of-detected-variants-using-variant-eval","text":"The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file.","title":"Evaluation of detected variants using Variant Eval"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#quality-matrix","text":"TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.","title":"Quality Matrix:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#local-realignment","text":"In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml","title":"Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#the-galaxy-workflow-platform","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.","title":"The Galaxy workflow platform"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#data-format-used-in-the-tutorial","text":"","title":"Data Format used in the tutorial"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#sequence-alignment-map-format","text":"SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy","title":"Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#binary-sequence-alignment-map-format","text":"BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV","title":"Binary Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-file-format","text":"What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT.","title":"VCF file format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-format-in-galaxy","text":"","title":"VCF format in Galaxy:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#bcf-file-format","text":"","title":"Bcf file format:"},{"location":"tutorials/variant_calling_galaxy_1/","text":"PR reviewers and advice: Clare Sloggett, Khalid Mahmood, Jessica Chung, Simon Gladman Current slides: https://docs.google.com/presentation/d/18rjOlb3Q_i_IY65_4cA2UWp76uzAyrrcS6YJS2AzquE (these are Clare's from May 2017 workshop round - fairly cut-down) Other slides: Original main folder: https://drive.google.com/drive/u/0/folders/0B-gCKa6V4E0lelJGVDFGaHBLSlk","title":"Home"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/","text":"Introduction to Variant Calling using Galaxy Overview This tutorial is designed to introduce the tools, data types and workflow of variant detection. We will align reads to the genome, look for differences between reads and reference genome sequence, and filter the detected genomic variation manually to understand the computational basis of variant calling. We cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22. Learning Objectives At the end of the course, you will be able to: Work with the FASTQ format and base quality scores Align reads to generate a BAM file and subsequently generate a pileup file Run the FreeBayes variant caller to find SNVs and indels Visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye Requirements This workshop uses Galaxy as a platform. It is recommended that participants who have not used Galaxy before either sign up for our Intro to GVL workshop, or work through this tutorial themselves beforehand. This is a hands-on workshop and attendees are required to bring their own laptops. Background Some background reading material - background Where is the data in this tutorial from? The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project. 1. Preparation Make sure you have an instance of Galaxy ready to go. For example, you can use the Galaxy Australia server . Create a new history for this tutorial. In the history pane, click on the cog icon at the top right. Click Create New . Click on Unnamed history and re-name it. Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis. 2. Quality Control The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome! 1. Take a look at the FASTQ file Click on the eye icon to the top right of the FASTQ file to view the a snippet of the file. Note that each read is represented by 4 lines: * read identifier * short read sequence * separator * short read sequence quality scores e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ . 2. Assessing read quality from the FASTQ files From the Galaxy tools panel, in the search box at the top, type in \"FastQC\". Click on FastQC The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). This will be a file called FastQC on data 1: Web page . Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30). 3. Alignment to the reference - (FASTQ to BAM) The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome. 1. Align the reads with BWA We will map (align) the reads with the BWA tool to the human reference genome. For this tutorial, use Human reference genome 19 (hg19) - this is hg19 from UCSC . Align the reads [3-5mins]: from the Galaxy tools panel, search for Map with BWA-MEM From the options: Will you selection a reference genome...: Use a built-in genome index Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file: from the Galaxy tools panel, search for SortSam From the options: Set the input file to be the output BAM file from the previous step. Sort by: set to Coordinate Keep other options as default and click execute 2. Examine the alignment To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, search for BAM-to-SAM From the options: BAM File to Convert: select your sorted BAM file Keep all options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon in the History pane next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc. 3. Assess the alignment data We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, search for the tool IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, search for Flagstat Select the sorted BAM file as input. Keep other options as default and click execute. Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant. 4. Visualise the BAM file with IGV To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \"Display with IGV web current \". This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22:36,006,744-36,007,406 Can you see a few variants? Don't close IGV yet as we'll be using it later. 5. Generate a pileup file A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence. 1. Generate a pileup file From the Galaxy tools panel, search for Generate pileup From the options: Call consensus according to MAQ model: set to Yes This generates a called 'consensus base' for each chromosomal position. This would allow us to use this pileup directly for variant detection, if we wanted. Keep other options as default and click execute Galaxy tries to assign a datatype attribute to every output file. In this case, you'll need to manually set the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you're editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores 2. Filter the pileup file If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, search for Filter Pileup From the options: which contains: set to Pileup with ten columns (with consensus). This is to match the Call consensus according to MAQ model option we selected earlier - the filter tool needs to know to expect the 10-column rather than 6-column format. Do not report positions with coverage lower than: set to 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row. 6. Call variants with FreeBayes FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial. 1. Call variants with FreeBayes. In the tool panel search for FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. (This may take a while.) 2. Check the generated list of variants Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification . 3. Visualise the variants and compare files Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22:36,006,744-36,007,406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1==\"chr22\" and c2 > 36006744 and c2 < 36007406 . 4. Optional: filter variants See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort: Filter tool we used above. 7. Further steps We've seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"Introduction to Variant Calling"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#introduction-to-variant-calling-using-galaxy","text":"","title":"Introduction to Variant Calling using Galaxy"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#overview","text":"This tutorial is designed to introduce the tools, data types and workflow of variant detection. We will align reads to the genome, look for differences between reads and reference genome sequence, and filter the detected genomic variation manually to understand the computational basis of variant calling. We cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.","title":"Overview"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#learning-objectives","text":"At the end of the course, you will be able to: Work with the FASTQ format and base quality scores Align reads to generate a BAM file and subsequently generate a pileup file Run the FreeBayes variant caller to find SNVs and indels Visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye","title":"Learning Objectives"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#requirements","text":"This workshop uses Galaxy as a platform. It is recommended that participants who have not used Galaxy before either sign up for our Intro to GVL workshop, or work through this tutorial themselves beforehand. This is a hands-on workshop and attendees are required to bring their own laptops.","title":"Requirements"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#background","text":"Some background reading material - background Where is the data in this tutorial from? The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project.","title":"Background"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-preparation","text":"Make sure you have an instance of Galaxy ready to go. For example, you can use the Galaxy Australia server . Create a new history for this tutorial. In the history pane, click on the cog icon at the top right. Click Create New . Click on Unnamed history and re-name it. Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis.","title":"1. Preparation"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-quality-control","text":"The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!","title":"2. Quality Control"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-take-a-look-at-the-fastq-file","text":"Click on the eye icon to the top right of the FASTQ file to view the a snippet of the file. Note that each read is represented by 4 lines: * read identifier * short read sequence * separator * short read sequence quality scores e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ .","title":"1. Take a look at the FASTQ file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-assessing-read-quality-from-the-fastq-files","text":"From the Galaxy tools panel, in the search box at the top, type in \"FastQC\". Click on FastQC The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). This will be a file called FastQC on data 1: Web page . Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30).","title":"2. Assessing read quality from the FASTQ files"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-alignment-to-the-reference-fastq-to-bam","text":"The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.","title":"3. Alignment to the reference - (FASTQ to BAM)"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-align-the-reads-with-bwa","text":"We will map (align) the reads with the BWA tool to the human reference genome. For this tutorial, use Human reference genome 19 (hg19) - this is hg19 from UCSC . Align the reads [3-5mins]: from the Galaxy tools panel, search for Map with BWA-MEM From the options: Will you selection a reference genome...: Use a built-in genome index Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file: from the Galaxy tools panel, search for SortSam From the options: Set the input file to be the output BAM file from the previous step. Sort by: set to Coordinate Keep other options as default and click execute","title":"1. Align the reads with BWA"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-examine-the-alignment","text":"To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, search for BAM-to-SAM From the options: BAM File to Convert: select your sorted BAM file Keep all options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon in the History pane next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.","title":"2. Examine the alignment"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-assess-the-alignment-data","text":"We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, search for the tool IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, search for Flagstat Select the sorted BAM file as input. Keep other options as default and click execute. Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.","title":"3. Assess the alignment data"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-visualise-the-bam-file-with-igv","text":"To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \"Display with IGV web current \". This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22:36,006,744-36,007,406 Can you see a few variants? Don't close IGV yet as we'll be using it later.","title":"4. Visualise the BAM file with IGV"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#5-generate-a-pileup-file","text":"A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.","title":"5. Generate a pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-generate-a-pileup-file","text":"From the Galaxy tools panel, search for Generate pileup From the options: Call consensus according to MAQ model: set to Yes This generates a called 'consensus base' for each chromosomal position. This would allow us to use this pileup directly for variant detection, if we wanted. Keep other options as default and click execute Galaxy tries to assign a datatype attribute to every output file. In this case, you'll need to manually set the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you're editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores","title":"1. Generate a pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-filter-the-pileup-file","text":"If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, search for Filter Pileup From the options: which contains: set to Pileup with ten columns (with consensus). This is to match the Call consensus according to MAQ model option we selected earlier - the filter tool needs to know to expect the 10-column rather than 6-column format. Do not report positions with coverage lower than: set to 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.","title":"2. Filter the pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#6-call-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial.","title":"6. Call variants with FreeBayes"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-call-variants-with-freebayes","text":"In the tool panel search for FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. (This may take a while.)","title":"1. Call variants with FreeBayes."},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-check-the-generated-list-of-variants","text":"Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification .","title":"2. Check the generated list of variants"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-visualise-the-variants-and-compare-files","text":"Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22:36,006,744-36,007,406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1==\"chr22\" and c2 > 36006744 and c2 < 36007406 .","title":"3. Visualise the variants and compare files"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-optional-filter-variants","text":"See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort: Filter tool we used above.","title":"4. Optional: filter variants"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#7-further-steps","text":"We've seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"7. Further steps"}]}