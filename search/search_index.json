{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutorials and protocols These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the Australian-made Genomics Virtual Laboratory and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.","title":"Home"},{"location":"#tutorials-and-protocols","text":"These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the Australian-made Genomics Virtual Laboratory and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.","title":"Tutorials and protocols"},{"location":"guides/RNA-seq/","text":"RNA-seq How can I analyse my RNA-seq data to investigate differential gene expression? RNA-seq is often used to examine the genes expressed under two experimental conditions. If a gene is important for tolerating a particular condition, that gene may be highly expressed. Thus, there will be more copies of the gene's RNA transcript in that condition. Tutorials Intro to Galaxy RNA-Seq DGE: Basic (Human) RNA-Seq DGE: Advanced RNA-Seq DGE: Experimental Design RNA-Seq DGE: in Galaxy (Microbial) RNA-Seq DGE: command line (Microbial) Useful links (but keep brief? and very up to date) journal article other training a protocol Help Attend a MB training course (link to \"how to attend\") Link with expertise group at MB (link how to)","title":"RNA-seq"},{"location":"guides/RNA-seq/#rna-seq","text":"How can I analyse my RNA-seq data to investigate differential gene expression? RNA-seq is often used to examine the genes expressed under two experimental conditions. If a gene is important for tolerating a particular condition, that gene may be highly expressed. Thus, there will be more copies of the gene's RNA transcript in that condition. Tutorials Intro to Galaxy RNA-Seq DGE: Basic (Human) RNA-Seq DGE: Advanced RNA-Seq DGE: Experimental Design RNA-Seq DGE: in Galaxy (Microbial) RNA-Seq DGE: command line (Microbial) Useful links (but keep brief? and very up to date) journal article other training a protocol Help Attend a MB training course (link to \"how to attend\") Link with expertise group at MB (link how to)","title":"RNA-seq"},{"location":"guides/bioinfo/","text":"What is bioinformatics? Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites. Getting started Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform. Resources Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published last year in BMC Bioinformatics .","title":"What is bioinformatics?"},{"location":"guides/bioinfo/#what-is-bioinformatics","text":"Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites.","title":"What is bioinformatics?"},{"location":"guides/bioinfo/#getting-started","text":"Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform.","title":"Getting started"},{"location":"guides/bioinfo/#resources","text":"Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published last year in BMC Bioinformatics .","title":"Resources"},{"location":"guides/cancer-genomics/","text":"Cancer Genomics Tutorials variant calling? RNA-seq? is there a more specific cancer genomcis link? https://www.melbournebioinformatics.org.au/project/human-genomics/","title":"Cancer genomics"},{"location":"guides/data-analysis/","text":"How can I analyse my data? The Galaxy Platform To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform. See our full guide here . Your local computer Install and run tools locally. We recommend these tutorials to get started: Unix Python Git Docker A high performance computer (cluster) Check this section with Andrew Isaac Melbourne Bioinformatics runs large clusters on which (unimelb only?) researchers can run analyses. For detailed information see the documentation here . To apply for usage, click here For information about logging on, click here . For information about running jobs, click here . Available systems are listed [here[]] systems https://www.melbournebioinformatics.org.au/capabilities/access-to-high-end-systems/ what is the GVL Difference between the GVL and Galaxy launch your own gvl","title":"Data analysis"},{"location":"guides/data-analysis/#how-can-i-analyse-my-data","text":"","title":"How can I analyse my data?"},{"location":"guides/data-analysis/#the-galaxy-platform","text":"To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform. See our full guide here .","title":"The Galaxy Platform"},{"location":"guides/data-analysis/#your-local-computer","text":"Install and run tools locally. We recommend these tutorials to get started: Unix Python Git Docker","title":"Your local computer"},{"location":"guides/data-analysis/#a-high-performance-computer-cluster","text":"Check this section with Andrew Isaac Melbourne Bioinformatics runs large clusters on which (unimelb only?) researchers can run analyses. For detailed information see the documentation here . To apply for usage, click here For information about logging on, click here . For information about running jobs, click here . Available systems are listed [here[]] systems https://www.melbournebioinformatics.org.au/capabilities/access-to-high-end-systems/ what is the GVL Difference between the GVL and Galaxy launch your own gvl","title":"A high performance computer (cluster)"},{"location":"guides/data-storage/","text":"Where should I store my data? Large datasets are difficult to store on your local computer and share with colleagues. A good option is to store data in the cloud. Data storage options Locally on your computer Your departmental server, but check storage limits and policy On a Galaxy server, but check storage limits and policies - link to galaxy guide On the Nectar cloud. Nectar is an Australian organization that provides a cloud for data storage. https://nectar.org.au/about/ general nectar storage link https://support.ehelp.edu.au/support/solutions/6000134520 nectar cloud storage link https://support.ehelp.edu.au/support/solutions/articles/6000055382-introduction-to-cloud-storage GenomeSpace links between storage and platforms (current?) - tutorial link","title":"Data storage"},{"location":"guides/data-storage/#where-should-i-store-my-data","text":"Large datasets are difficult to store on your local computer and share with colleagues. A good option is to store data in the cloud.","title":"Where should I store my data?"},{"location":"guides/data-storage/#data-storage-options","text":"Locally on your computer Your departmental server, but check storage limits and policy On a Galaxy server, but check storage limits and policies - link to galaxy guide On the Nectar cloud. Nectar is an Australian organization that provides a cloud for data storage. https://nectar.org.au/about/ general nectar storage link https://support.ehelp.edu.au/support/solutions/6000134520 nectar cloud storage link https://support.ehelp.edu.au/support/solutions/articles/6000055382-introduction-to-cloud-storage GenomeSpace links between storage and platforms (current?) - tutorial link","title":"Data storage options"},{"location":"guides/galaxy/","text":"The Galaxy Platform Galaxy is a web platform for bioinformatics analysis. Which Galaxy should I use? There are many different Galaxy servers - each one has a different web address. For researchers based in Melbourne, Australia, we recommend you use Galaxy-Mel . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server (e.g. Galaxy-Mel and useGalaxy.org) you need to register and log in separately for each server. They don't talk to each other. Tutorials Go to Galaxy-Mel and log in, then try these tutorials: Introduction to Galaxy Galaxy Workflows Other MB tutorials, e.g. Assembly, RNA-seq, etc. Galaxy Training Network The Galaxy Training Network hosts a large collection of useful training material here . To get started, go to https://usegalaxy.org/ , log in, then try these tutorials: Introduction to Galaxy Galaxy histories","title":"The Galaxy platform"},{"location":"guides/galaxy/#the-galaxy-platform","text":"Galaxy is a web platform for bioinformatics analysis.","title":"The Galaxy Platform"},{"location":"guides/galaxy/#which-galaxy-should-i-use","text":"There are many different Galaxy servers - each one has a different web address. For researchers based in Melbourne, Australia, we recommend you use Galaxy-Mel . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server (e.g. Galaxy-Mel and useGalaxy.org) you need to register and log in separately for each server. They don't talk to each other.","title":"Which Galaxy should I use?"},{"location":"guides/galaxy/#tutorials","text":"Go to Galaxy-Mel and log in, then try these tutorials: Introduction to Galaxy Galaxy Workflows Other MB tutorials, e.g. Assembly, RNA-seq, etc.","title":"Tutorials"},{"location":"guides/galaxy/#galaxy-training-network","text":"The Galaxy Training Network hosts a large collection of useful training material here . To get started, go to https://usegalaxy.org/ , log in, then try these tutorials: Introduction to Galaxy Galaxy histories","title":"Galaxy Training Network"},{"location":"guides/genome-assembly/","text":"Genome assembly Microbial Eukaryote Tutorials","title":"Genome assembly"},{"location":"guides/microbial_genomics/","text":"Sepsis tutorials: Assembly Annotation Variant Calling https://www.melbournebioinformatics.org.au/project/microbial-genomics/","title":"Microbial genomics"},{"location":"guides/more-help/","text":"More help Web resources SeqAnswers BioStars Melbourne Bioinformatics Tutorials - online Workshops - link subscriptions - https://www.melbournebioinformatics.org.au/capabilities/subscriptions/","title":"More help"},{"location":"guides/more-help/#web-resources","text":"SeqAnswers BioStars","title":"Web resources"},{"location":"guides/more-help/#melbourne-bioinformatics","text":"Tutorials - online Workshops - link subscriptions - https://www.melbournebioinformatics.org.au/capabilities/subscriptions/","title":"Melbourne Bioinformatics"},{"location":"guides/variant-calling/","text":"","title":"Variant calling"},{"location":"includes/connecting/","text":"Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into LIMS-HPC, for example, type this command at the prompt and press return (where the word username is replaced with your LIMS-HPC username): ssh -p 6022 username@lims-hpc-m.latrobe.edu.au The same procedure works for any other machine where you have an account except most other HPCs will not need the -p 6022 (which is telling ssh to connect on a non-standard port number). You may be presented with a message along the lines of: The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type yes and then press enter . If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html Documentation for using PuTTY is here: http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html When you start PuTTY you should see a window which looks something like this: To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and 6022 in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text \"login as:\" . An example terminal window is shown below. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Connecting"},{"location":"tutorials/alignment/","text":"NB: This tutorial is from a lab and probably obsolete; alignment is covered by variant calling workshops. PR reviewers and advice: Clare Sloggett Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/alignment/alignment/","text":"Alignment Tutorial In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We'll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials. Section 1: Alignment using Galaxy Preparation Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There's only one input file, so instead of importing a History, let's import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ . Alignment Run BWA For our alignment, we will use the tool BWA, which stands for \"Burrows-Wheeler Aligner\". You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \"@\", followed by one alignment row per read. Refer to the docs above to understand what you're seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \">\" lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that's aligned to a particular region of the genome. Section 2: View the alignment Load the VNC interface We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer's VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You'll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer's visual desktop. You'll notice two icons on the desktop. A terminal and a shortcut to IGV. Download the data The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You'll need to download both the BAM file and BAM index file. View the BAM file in IGV Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user's Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \"All\", instead select \"22\". Zoom in. If you're still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \"Go\". At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV's features - try right-clicking on a read to change the display options. Section 3: Alignment using the command line In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty. Get the data Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq This will download the datafile to the current directory. Set up the environment Now you'll need to load the bwa and samtools modules. Try module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: module load bwa module load samtools/1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands. Perform the alignment The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won't have time to index it during the lab, but your instance should already have access to an indexed genome. Try ls galaxy_genomes ls galaxy_genomes/hg19 ls galaxy_genomes/hg19/bwa_mem_index ls galaxy_genomes/hg19/bwa_mem_index/hg19/ Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: bwa mem galaxy_genomes/hg19/bwa_mem_index/hg19/hg19.fa NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq > aligned_reads.sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it. Make the BAM file Try to convert your SAM file to a BAM file using samtools view. The command you'll need is: samtools view -b -h aligned_reads.sam > aligned_reads.bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation. Other SAM tools commands Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You'll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV... That's it, I hope you enjoyed the tutorial!","title":"Alignment"},{"location":"tutorials/alignment/alignment/#alignment-tutorial","text":"In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We'll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials.","title":"Alignment Tutorial"},{"location":"tutorials/alignment/alignment/#section-1-alignment-using-galaxy","text":"","title":"Section 1: Alignment using Galaxy"},{"location":"tutorials/alignment/alignment/#preparation","text":"Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There's only one input file, so instead of importing a History, let's import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ .","title":"Preparation"},{"location":"tutorials/alignment/alignment/#alignment","text":"","title":"Alignment"},{"location":"tutorials/alignment/alignment/#run-bwa","text":"For our alignment, we will use the tool BWA, which stands for \"Burrows-Wheeler Aligner\". You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \"@\", followed by one alignment row per read. Refer to the docs above to understand what you're seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \">\" lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that's aligned to a particular region of the genome.","title":"Run BWA"},{"location":"tutorials/alignment/alignment/#section-2-view-the-alignment","text":"","title":"Section 2: View the alignment"},{"location":"tutorials/alignment/alignment/#load-the-vnc-interface","text":"We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer's VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You'll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer's visual desktop. You'll notice two icons on the desktop. A terminal and a shortcut to IGV.","title":"Load the VNC interface"},{"location":"tutorials/alignment/alignment/#download-the-data","text":"The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You'll need to download both the BAM file and BAM index file.","title":"Download the data"},{"location":"tutorials/alignment/alignment/#view-the-bam-file-in-igv","text":"Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user's Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \"All\", instead select \"22\". Zoom in. If you're still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \"Go\". At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV's features - try right-clicking on a read to change the display options.","title":"View the BAM file in IGV"},{"location":"tutorials/alignment/alignment/#section-3-alignment-using-the-command-line","text":"In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty.","title":"Section 3: Alignment using the command line"},{"location":"tutorials/alignment/alignment/#get-the-data","text":"Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq This will download the datafile to the current directory.","title":"Get the data"},{"location":"tutorials/alignment/alignment/#set-up-the-environment","text":"Now you'll need to load the bwa and samtools modules. Try module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: module load bwa module load samtools/1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands.","title":"Set up the environment"},{"location":"tutorials/alignment/alignment/#perform-the-alignment","text":"The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won't have time to index it during the lab, but your instance should already have access to an indexed genome. Try ls galaxy_genomes ls galaxy_genomes/hg19 ls galaxy_genomes/hg19/bwa_mem_index ls galaxy_genomes/hg19/bwa_mem_index/hg19/ Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: bwa mem galaxy_genomes/hg19/bwa_mem_index/hg19/hg19.fa NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq > aligned_reads.sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it.","title":"Perform the alignment"},{"location":"tutorials/alignment/alignment/#make-the-bam-file","text":"Try to convert your SAM file to a BAM file using samtools view. The command you'll need is: samtools view -b -h aligned_reads.sam > aligned_reads.bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation.","title":"Make the BAM file"},{"location":"tutorials/alignment/alignment/#other-sam-tools-commands","text":"Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You'll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV... That's it, I hope you enjoyed the tutorial!","title":"Other SAM tools commands"},{"location":"tutorials/assembly/","text":"PR reviewers and advice: Simon Gladman, Torsten Seeman, Dieter Bulach, Anna Syme Current slides: in folder https://drive.google.com/drive/u/0/folders/0B2iomOA3e6SuZHNxNjlPWG9hdTQ Other slides: Markdown slides at galaxyproject.github.io : http://galaxyproject.github.io/training-material/topics/assembly/","title":"Home"},{"location":"tutorials/assembly/assembly-background/","text":"De novo genome assembly using Velvet Background Introduction to de novo assembly DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\" \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system. The Galaxy workflow platform What is Galaxy? Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right. De novo assembly with Velvet and the Velvet Optimiser. Velvet Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically. de Bruijn graphs A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others. The Velvet algorithm Step 1: Hashing the reads. Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite. Step 2: Constructing the de Bruijn graph. Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.) Step 3: Simplification of the graph. Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.) Step 4: Read off the contigs. Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one. K-mer size and coverage cutoff values The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software. Velvet Optimiser The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time). References http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"Method Background"},{"location":"tutorials/assembly/assembly-background/#de-novo-genome-assembly-using-velvet","text":"","title":"De novo genome assembly using Velvet"},{"location":"tutorials/assembly/assembly-background/#background","text":"","title":"Background"},{"location":"tutorials/assembly/assembly-background/#introduction-to-de-novo-assembly","text":"DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\" \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system.","title":"Introduction to de novo assembly"},{"location":"tutorials/assembly/assembly-background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/assembly/assembly-background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right.","title":"What is Galaxy?"},{"location":"tutorials/assembly/assembly-background/#de-novo-assembly-with-velvet-and-the-velvet-optimiser","text":"","title":"De novo assembly with Velvet and the Velvet Optimiser."},{"location":"tutorials/assembly/assembly-background/#velvet","text":"Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.","title":"Velvet"},{"location":"tutorials/assembly/assembly-background/#de-bruijn-graphs","text":"A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.","title":"de Bruijn graphs"},{"location":"tutorials/assembly/assembly-background/#the-velvet-algorithm","text":"","title":"The Velvet algorithm"},{"location":"tutorials/assembly/assembly-background/#step-1-hashing-the-reads","text":"Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.","title":"Step 1: Hashing the reads."},{"location":"tutorials/assembly/assembly-background/#step-2-constructing-the-de-bruijn-graph","text":"Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.)","title":"Step 2: Constructing the de Bruijn graph."},{"location":"tutorials/assembly/assembly-background/#step-3-simplification-of-the-graph","text":"Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.)","title":"Step 3: Simplification of the graph."},{"location":"tutorials/assembly/assembly-background/#step-4-read-off-the-contigs","text":"Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.","title":"Step 4: Read off the contigs."},{"location":"tutorials/assembly/assembly-background/#k-mer-size-and-coverage-cutoff-values","text":"The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.","title":"K-mer size and coverage cutoff values"},{"location":"tutorials/assembly/assembly-background/#velvet-optimiser","text":"The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time).","title":"Velvet Optimiser"},{"location":"tutorials/assembly/assembly-background/#references","text":"http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"References"},{"location":"tutorials/assembly/assembly-protocol/","text":"De novo Genome Assembly for Illumina Data Protocol Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Protocol Overview / Introduction In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes. What is de novo genome assembly? Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1] Why do we want to assemble an organism\u2019s DNA? Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2]. The protocol in a nutshell: Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol. Raw read sequence file formats. Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth. Bioinformatics tools for this protocol. There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type. Genomics Virtual Laboratory resources for this protocol. Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources. Section 1: Read Quality Control Purpose: The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities. Steps involved and suggested tools: Examine the quality of your raw read files. For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s. Quality trimming/cleanup of read files. Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE for Paired End Files java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly. Possible alternate tools: Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml Section 2: Assembly Purpose: The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d Steps involved and suggested tools: Assembly of the reads. The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c). Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range. Examine the draft contigs and assessment of the assembly quality. The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ). Possible alternative software: Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs. Section 3: What next? Purpose: Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data? Possible tools for improving your assemblies: Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"De Novo Genome Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly-protocol/#de-novo-genome-assembly-for-illumina-data","text":"","title":"De novo Genome Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly-protocol/#protocol","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Protocol"},{"location":"tutorials/assembly/assembly-protocol/#protocol-overview-introduction","text":"In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.","title":"Protocol Overview / Introduction"},{"location":"tutorials/assembly/assembly-protocol/#what-is-de-novo-genome-assembly","text":"Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]","title":"What is de novo genome assembly?"},{"location":"tutorials/assembly/assembly-protocol/#why-do-we-want-to-assemble-an-organisms-dna","text":"Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].","title":"Why do we want to assemble an organism\u2019s DNA?"},{"location":"tutorials/assembly/assembly-protocol/#the-protocol-in-a-nutshell","text":"Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol.","title":"The protocol in a nutshell:"},{"location":"tutorials/assembly/assembly-protocol/#raw-read-sequence-file-formats","text":"Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.","title":"Raw read sequence file formats."},{"location":"tutorials/assembly/assembly-protocol/#bioinformatics-tools-for-this-protocol","text":"There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.","title":"Bioinformatics tools for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#genomics-virtual-laboratory-resources-for-this-protocol","text":"Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources.","title":"Genomics Virtual Laboratory resources for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#section-1-read-quality-control","text":"","title":"Section 1: Read Quality Control"},{"location":"tutorials/assembly/assembly-protocol/#purpose","text":"The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#examine-the-quality-of-your-raw-read-files","text":"For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.","title":"Examine the quality of your raw read files."},{"location":"tutorials/assembly/assembly-protocol/#quality-trimmingcleanup-of-read-files","text":"Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE for Paired End Files java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.","title":"Quality trimming/cleanup of read files."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternate-tools","text":"Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml","title":"Possible alternate tools:"},{"location":"tutorials/assembly/assembly-protocol/#section-2-assembly","text":"","title":"Section 2: Assembly"},{"location":"tutorials/assembly/assembly-protocol/#purpose_1","text":"The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools_1","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#assembly-of-the-reads","text":"The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c). Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.","title":"Assembly of the reads."},{"location":"tutorials/assembly/assembly-protocol/#examine-the-draft-contigs-and-assessment-of-the-assembly-quality","text":"The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ).","title":"Examine the draft contigs and assessment of the assembly quality."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternative-software","text":"Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs.","title":"Possible alternative software:"},{"location":"tutorials/assembly/assembly-protocol/#section-3-what-next","text":"","title":"Section 3: What next?"},{"location":"tutorials/assembly/assembly-protocol/#purpose_2","text":"Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data?","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#possible-tools-for-improving-your-assemblies","text":"Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"Possible tools for improving your assemblies:"},{"location":"tutorials/assembly/assembly/","text":"Microbial de novo Assembly for Illumina Data Introductory Tutorial Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Tutorial Overview In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies Background Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information. Preparation Login to Galaxy Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well. Import the DNA read data for the tutorial. You can do this in a few ways. If you're using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \" Microbial_assembly_input_data \". Then click 'Import History' at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it's type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it's type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format View the fastq files Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019 Section 1: Quality control The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here. Run FastQC on both input read files From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \"FASTQ reads\": ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them. Figure 1: Screenshot of FastQC interface in Galaxy Examine the FastQC output You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality. Figure 2: Screenshot of FastQC output in Galaxy Quality trim the reads using Trimmomatic. From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \"Input FASTQ file (R1/first of pair)\": ERR048396_1.fastq \"Input FASTQ file (R2/second of pair)\": ERR048396_2.fastq \"Perform initial ILLUMINACLIP step?\": Yes \"Adapter sequences to use\": TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \"How accurate ... read alignment\": 40 \"How accurate ... against a read\": 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases ... (LEADING) \"Minimum quality required to keep a base\": 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute Figure 3: Screenshot of Trimmomatic inputs in Galaxy Examine the Trimmomatic output FastQ files. You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed. Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser De novo assembly of the reads into contigs From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \"Start k-mer value\": 55 \"End k-mer value\": 69 In the input files section: \"Select first set of reads\": Trimmomatic on data 2 and data 1 (R1 paired) \"Select second set of reads\": Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \"Single or paired end reads\": Single \"Select the reads\": Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute . Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy Examine assembly output Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.) Figure 5: Screenshot of assembled contigs (a) and contig stats (b) a b Calculate some statistics on the assembled contigs From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \"Fasta or multifasta file\": Velvet Optimiser ... Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them. Section 3: Extension. Examine the contig coverage depth and blast a high coverage contig against a protein database. Examine the contig coverage depth. Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \"Filter\": Velvet Optimiser on data 8, data 7 and others: Contig stats \"With the following condition\": c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is. Extract a single sequence from the contigs file. Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \"Fasta or multifasta file\": Velvet Optimiser ... : Contigs \"Sequence ID (or partial): NODE_1_... (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format. Blast sequence to determine what it contains. We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage. Figure 6: Screenshot of the output from the NCBI Blast website","title":"Tutorial"},{"location":"tutorials/assembly/assembly/#microbial-de-novo-assembly-for-illumina-data","text":"","title":"Microbial de novo Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly/#introductory-tutorial","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introductory Tutorial"},{"location":"tutorials/assembly/assembly/#tutorial-overview","text":"In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies","title":"Tutorial Overview"},{"location":"tutorials/assembly/assembly/#background","text":"Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information.","title":"Background"},{"location":"tutorials/assembly/assembly/#preparation","text":"","title":"Preparation"},{"location":"tutorials/assembly/assembly/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.","title":"Login to Galaxy"},{"location":"tutorials/assembly/assembly/#import-the-dna-read-data-for-the-tutorial","text":"You can do this in a few ways. If you're using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \" Microbial_assembly_input_data \". Then click 'Import History' at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it's type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it's type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format","title":"Import the DNA read data for the tutorial."},{"location":"tutorials/assembly/assembly/#view-the-fastq-files","text":"Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019","title":"View the fastq files"},{"location":"tutorials/assembly/assembly/#section-1-quality-control","text":"The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here.","title":"Section 1: Quality control"},{"location":"tutorials/assembly/assembly/#run-fastqc-on-both-input-read-files","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \"FASTQ reads\": ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them.","title":"Run FastQC on both input read files"},{"location":"tutorials/assembly/assembly/#figure-1-screenshot-of-fastqc-interface-in-galaxy","text":"","title":"Figure 1: Screenshot of FastQC interface in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-fastqc-output","text":"You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.","title":"Examine the FastQC output"},{"location":"tutorials/assembly/assembly/#figure-2-screenshot-of-fastqc-output-in-galaxy","text":"","title":"Figure 2: Screenshot of FastQC output in Galaxy"},{"location":"tutorials/assembly/assembly/#quality-trim-the-reads-using-trimmomatic","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \"Input FASTQ file (R1/first of pair)\": ERR048396_1.fastq \"Input FASTQ file (R2/second of pair)\": ERR048396_2.fastq \"Perform initial ILLUMINACLIP step?\": Yes \"Adapter sequences to use\": TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \"How accurate ... read alignment\": 40 \"How accurate ... against a read\": 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases ... (LEADING) \"Minimum quality required to keep a base\": 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute","title":"Quality trim the reads using Trimmomatic."},{"location":"tutorials/assembly/assembly/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy","text":"","title":"Figure 3: Screenshot of Trimmomatic inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-trimmomatic-output-fastq-files","text":"You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.","title":"Examine the Trimmomatic output FastQ files."},{"location":"tutorials/assembly/assembly/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser","text":"The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser","title":"Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"},{"location":"tutorials/assembly/assembly/#de-novo-assembly-of-the-reads-into-contigs","text":"From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \"Start k-mer value\": 55 \"End k-mer value\": 69 In the input files section: \"Select first set of reads\": Trimmomatic on data 2 and data 1 (R1 paired) \"Select second set of reads\": Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \"Single or paired end reads\": Single \"Select the reads\": Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute .","title":"De novo assembly of the reads into contigs"},{"location":"tutorials/assembly/assembly/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy","text":"","title":"Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-assembly-output","text":"Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)","title":"Examine assembly output"},{"location":"tutorials/assembly/assembly/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b","text":"","title":"Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"},{"location":"tutorials/assembly/assembly/#a","text":"","title":"a"},{"location":"tutorials/assembly/assembly/#b","text":"","title":"b"},{"location":"tutorials/assembly/assembly/#calculate-some-statistics-on-the-assembled-contigs","text":"From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \"Fasta or multifasta file\": Velvet Optimiser ... Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.","title":"Calculate some statistics on the assembled contigs"},{"location":"tutorials/assembly/assembly/#section-3-extension","text":"Examine the contig coverage depth and blast a high coverage contig against a protein database.","title":"Section 3: Extension."},{"location":"tutorials/assembly/assembly/#examine-the-contig-coverage-depth","text":"Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \"Filter\": Velvet Optimiser on data 8, data 7 and others: Contig stats \"With the following condition\": c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is.","title":"Examine the contig coverage depth."},{"location":"tutorials/assembly/assembly/#extract-a-single-sequence-from-the-contigs-file","text":"Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \"Fasta or multifasta file\": Velvet Optimiser ... : Contigs \"Sequence ID (or partial): NODE_1_... (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format.","title":"Extract a single sequence from the contigs file."},{"location":"tutorials/assembly/assembly/#blast-sequence-to-determine-what-it-contains","text":"We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.","title":"Blast sequence to determine what it contains."},{"location":"tutorials/assembly/assembly/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website","text":"","title":"Figure 6: Screenshot of the output from the NCBI Blast website"},{"location":"tutorials/docker/docker/","text":"Workshop Slides (use the arrow keys to navigate) Part 1: Docker and Containers Part 2: Running Containers Part 3: Making your Own Image Part 4: Docker on HPC","title":"Containerized Bioinformatics"},{"location":"tutorials/galaxy-workflows/","text":"PR reviewers and advice: Simon Gladman, Clare Sloggett Current slides: Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Galaxy Workflows Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Background This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow Section 1: Preparation. The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy-mel Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button. Section 2: Create and run a workflow. This section will show you two different methods to create a workflow and then how to run one. Import the workflow history In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\" From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \" workflow_finished \" and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right. Workflow creation: Method 1 We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows. Some discussion Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor.. Workflow Creation: Method 2 We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file. Step 1: Create a workflow name and edit space. Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \"Workflow Name\" text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid. Step 2: Open the editor and place component tools Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\" \"Reads 1\" output to \"Select first set of reads\" \"Reads 2\" output to \"Select second set of reads\" Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \"Choose the source for the reference list:\" to History Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input. Connect the \"Reference data\" output to \"Freebayes\u2019\" Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \"Choose parameter selection level\": Complete list of all options \"Population model options\": Set population model options \"Set ploidy for the analysis\": 1 \"Input filters\": Set input filters \"Exclude alignments from analysis if they have a mapping quality less than\": 20 \"Exclude alleles from analysis if their supporting base quality is less than\": 20 \"Require at least this fraction of observations \u2026 to evaluate the position\": 0.9 \"Require at least this count of observations .. to evaluate the position\": 10 \"Population and mappability priors\": Set population and mappability priors \"Disable incorporation of prior expectations about observations\": Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input. In the right hand pane, change the following: \"With the following condition\": c6 > 500 \"Number of header lines to skip\": 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \"Map with BWA\u2019s\" bam file output. Click on the star next to \"Filter\u2019s\" output vcf. Step 3: Save it! Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it! Running the workflow We will now make a new history called \"Test\" and run the workflow on it\u2019s data. Create the new history From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \"New History Named:\" Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen Run the workflow On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli ... .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn. What now? Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"Tutorial"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#galaxy-workflows","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Galaxy Workflows"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#background","text":"This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow","title":"Background"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy-mel Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button.","title":"Section 1: Preparation."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-2-create-and-run-a-workflow","text":"This section will show you two different methods to create a workflow and then how to run one.","title":"Section 2: Create and run a workflow."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#import-the-workflow-history","text":"In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\" From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \" workflow_finished \" and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right.","title":"Import the workflow history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-1","text":"We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.","title":"Workflow creation: Method 1"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#some-discussion","text":"Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor..","title":"Some discussion"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-2","text":"We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.","title":"Workflow Creation: Method 2"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-1-create-a-workflow-name-and-edit-space","text":"Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \"Workflow Name\" text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid.","title":"Step 1: Create a workflow name and edit space."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-2-open-the-editor-and-place-component-tools","text":"Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\" \"Reads 1\" output to \"Select first set of reads\" \"Reads 2\" output to \"Select second set of reads\" Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \"Choose the source for the reference list:\" to History Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input. Connect the \"Reference data\" output to \"Freebayes\u2019\" Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \"Choose parameter selection level\": Complete list of all options \"Population model options\": Set population model options \"Set ploidy for the analysis\": 1 \"Input filters\": Set input filters \"Exclude alignments from analysis if they have a mapping quality less than\": 20 \"Exclude alleles from analysis if their supporting base quality is less than\": 20 \"Require at least this fraction of observations \u2026 to evaluate the position\": 0.9 \"Require at least this count of observations .. to evaluate the position\": 10 \"Population and mappability priors\": Set population and mappability priors \"Disable incorporation of prior expectations about observations\": Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input. In the right hand pane, change the following: \"With the following condition\": c6 > 500 \"Number of header lines to skip\": 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \"Map with BWA\u2019s\" bam file output. Click on the star next to \"Filter\u2019s\" output vcf.","title":"Step 2: Open the editor and place component tools"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-3-save-it","text":"Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it!","title":"Step 3: Save it!"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#running-the-workflow","text":"We will now make a new history called \"Test\" and run the workflow on it\u2019s data.","title":"Running the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#create-the-new-history","text":"From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \"New History Named:\" Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen","title":"Create the new history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#run-the-workflow","text":"On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli ... .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.","title":"Run the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#what-now","text":"Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"What now?"},{"location":"tutorials/galaxy-workflows/history_creation/","text":"History creation instructions for Workflow tutorial Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial. Step 1: Import the raw datafiles Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url's (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file. Step 2: Run BWA Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \"Will you select a reference genome from your history or use a built-in index?\": Use a genome from history and build index \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \"Select first set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \"Select second set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference. Step 3: Run Freebayes Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \"Choose the source for the reference genome\": History \"BAM file\": Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf). Step 4: Filter the VCF file. Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \"Filter\": FreeBayes on data 3 and data 4 (variants) \"With following condition\": c6 > 500 \"Number of header lines to skip\": 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"History Creation"},{"location":"tutorials/galaxy-workflows/history_creation/#history-creation-instructions-for-workflow-tutorial","text":"Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.","title":"History creation instructions for Workflow tutorial"},{"location":"tutorials/galaxy-workflows/history_creation/#step-1-import-the-raw-datafiles","text":"Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url's (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.","title":"Step 1: Import the raw datafiles"},{"location":"tutorials/galaxy-workflows/history_creation/#step-2-run-bwa","text":"Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \"Will you select a reference genome from your history or use a built-in index?\": Use a genome from history and build index \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \"Select first set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \"Select second set of reads\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.","title":"Step 2: Run BWA"},{"location":"tutorials/galaxy-workflows/history_creation/#step-3-run-freebayes","text":"Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \"Choose the source for the reference genome\": History \"BAM file\": Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \"Use the following dataset as the reference sequence\": https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf).","title":"Step 3: Run Freebayes"},{"location":"tutorials/galaxy-workflows/history_creation/#step-4-filter-the-vcf-file","text":"Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \"Filter\": FreeBayes on data 3 and data 4 (variants) \"With following condition\": c6 > 500 \"Number of header lines to skip\": 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"Step 4: Filter the VCF file."},{"location":"tutorials/galaxy_101/","text":"PR reviewers and advice: Simon Gladman, Gayle Philip, Clare Sloggett, Jessica Chung Current slides: Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy_101/galaxy_101/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Introduction to Galaxy Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Background Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools Learning Objectives At the end of this tutorial you should: Be able to register on and login to a Galaxy server. Be able to upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. Be able use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output. Section 1: Preparation. The purpose of this section is to get you to log in to the server.. Go to the ip address of your GVL Galaxy server (or if you don\u2019t have one, open Galaxy-Tut server) in Firefox or Chrome (your choice) - Please don\u2019t use Internet Explorer or Safari. If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit Section 2: Getting data into Galaxy There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then work use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it. Method 1: Upload a file from your own computer With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \"file format\" to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Method 2: Upload a file from a URL If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it. Method 2 (again): Get data from a public URL Now we are going to upload another file from the remote data source. Repeat the above for: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Note that this file is a fasta file and not a fastqsanger file. Reveal detailed instructions From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Set the file format to fasta . Click Start Once the progress bar has reached 100%, click Close //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this. The data Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illunina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format. Section 3: Play with the tools The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \"Name\" text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools.. Example 1: Histogram and summary statistics The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \"Cut Columns\" to: c1,c6 \"Delimited by\": Tab \"Cut from\": Contig_stats.txt Click Execute Examine the new file by clicking on it\u2019s icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \"Remove First\": 1 \"from\": Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \"Dataset\": Remove beginning on Data X \"Numerical column for X axis\": c2 \"Number of breaks\": 25 \"Plot title\": Histogram of Contig Coverage \"Label for X axis\": Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks.. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \"Summary statistics on\": Remove beginning on Data X \"Column or expression\": c2 Click Execute You\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator. Example 2: Convert Fastq to Fasta This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \"FASTQ file to convert\": Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2. Example 3: Find Ribosomal RNA Features in a DNA Sequence This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA's in a sequence From the tool panel, click on Annotation -> barrnap and set the following: \"Fasta file\": MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change it\u2019s name to something more appropriate (click on the icon.) There is also a STDERR output file from this tool - just ignore this one. Now lets say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \"Select lines from\": (whatever you called the barrnap gff3 output) \"the pattern\": 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations! What now? Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"Introduction to Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#introduction-to-galaxy","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introduction to Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#background","text":"Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools","title":"Background"},{"location":"tutorials/galaxy_101/galaxy_101/#learning-objectives","text":"At the end of this tutorial you should: Be able to register on and login to a Galaxy server. Be able to upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. Be able use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output.","title":"Learning Objectives"},{"location":"tutorials/galaxy_101/galaxy_101/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server.. Go to the ip address of your GVL Galaxy server (or if you don\u2019t have one, open Galaxy-Tut server) in Firefox or Chrome (your choice) - Please don\u2019t use Internet Explorer or Safari. If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit","title":"Section 1: Preparation."},{"location":"tutorials/galaxy_101/galaxy_101/#section-2-getting-data-into-galaxy","text":"There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then work use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it.","title":"Section 2: Getting data into Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#method-1-upload-a-file-from-your-own-computer","text":"With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \"file format\" to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Method 1: Upload a file from your own computer"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-upload-a-file-from-a-url","text":"If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.","title":"Method 2: Upload a file from a URL"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-again-get-data-from-a-public-url","text":"Now we are going to upload another file from the remote data source. Repeat the above for: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Note that this file is a fasta file and not a fastqsanger file. Reveal detailed instructions From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Set the file format to fasta . Click Start Once the progress bar has reached 100%, click Close //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this.","title":"Method 2 (again): Get data from a public URL"},{"location":"tutorials/galaxy_101/galaxy_101/#the-data","text":"Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illunina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format.","title":"The data"},{"location":"tutorials/galaxy_101/galaxy_101/#section-3-play-with-the-tools","text":"The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \"Name\" text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools..","title":"Section 3: Play with the tools"},{"location":"tutorials/galaxy_101/galaxy_101/#example-1-histogram-and-summary-statistics","text":"The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \"Cut Columns\" to: c1,c6 \"Delimited by\": Tab \"Cut from\": Contig_stats.txt Click Execute Examine the new file by clicking on it\u2019s icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \"Remove First\": 1 \"from\": Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \"Dataset\": Remove beginning on Data X \"Numerical column for X axis\": c2 \"Number of breaks\": 25 \"Plot title\": Histogram of Contig Coverage \"Label for X axis\": Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks.. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \"Summary statistics on\": Remove beginning on Data X \"Column or expression\": c2 Click Execute You\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator.","title":"Example 1: Histogram and summary statistics"},{"location":"tutorials/galaxy_101/galaxy_101/#example-2-convert-fastq-to-fasta","text":"This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \"FASTQ file to convert\": Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2.","title":"Example 2: Convert Fastq to Fasta"},{"location":"tutorials/galaxy_101/galaxy_101/#example-3-find-ribosomal-rna-features-in-a-dna-sequence","text":"This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA's in a sequence From the tool panel, click on Annotation -> barrnap and set the following: \"Fasta file\": MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change it\u2019s name to something more appropriate (click on the icon.) There is also a STDERR output file from this tool - just ignore this one. Now lets say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \"Select lines from\": (whatever you called the barrnap gff3 output) \"the pattern\": 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations!","title":"Example 3: Find Ribosomal RNA Features in a DNA Sequence"},{"location":"tutorials/galaxy_101/galaxy_101/#what-now","text":"Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"What now?"},{"location":"tutorials/genomespace/","text":"PR reviewers and advice: Yousef Kowsar Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/genomespace/genomespace/","text":"What is GenomeSpace? GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au . Prerequisites GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain. Registering a GenomeSpace account To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \"Register new GenomeSpace user\" link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website. Making a swift container (These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\". To make a container, click \"Create Container\". Mounting a swift container Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to. Basic file manipulation Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \"view file\". Adding a Galaxy service to your account: PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \"Add new\" button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName. File transfer to/from Galaxy PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"Introduction to GenomeSpace"},{"location":"tutorials/genomespace/genomespace/#what-is-genomespace","text":"GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au .","title":"What is GenomeSpace?"},{"location":"tutorials/genomespace/genomespace/#prerequisites","text":"GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.","title":"Prerequisites"},{"location":"tutorials/genomespace/genomespace/#registering-a-genomespace-account","text":"To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \"Register new GenomeSpace user\" link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.","title":"Registering a GenomeSpace account"},{"location":"tutorials/genomespace/genomespace/#making-a-swift-container","text":"(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\". To make a container, click \"Create Container\".","title":"Making a swift container"},{"location":"tutorials/genomespace/genomespace/#mounting-a-swift-container","text":"Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to.","title":"Mounting a swift container"},{"location":"tutorials/genomespace/genomespace/#basic-file-manipulation","text":"Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \"view file\".","title":"Basic file manipulation"},{"location":"tutorials/genomespace/genomespace/#adding-a-galaxy-service-to-your-account","text":"PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \"Add new\" button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName.","title":"Adding a Galaxy service to your account:"},{"location":"tutorials/genomespace/genomespace/#file-transfer-tofrom-galaxy","text":"PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"File transfer to/from Galaxy"},{"location":"tutorials/gvl_launch/","text":"PR reviewers and advice: Clare Sloggett, Simon Gladman, Nuwan Goonasekera Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/gvl_launch/gvl_launch/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } Launching a Personal GVL Server on the NeCTAR Research Cloud Tutorial Overview This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down Background What is the GVL? The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Melbourne , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources. What is NeCTAR? The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free). Section 1: Access the NeCTAR dashboard Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose 'The University of Melbourne' and not 'The University of Melbourne (with ECP)'. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here . Section 2: Get your cloud credentials Launching a GVL instance requires EC2 API keys from NeCTAR. Obtaining these keys is a simple 3 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top 'API Access' tab. Click on the '+ View Credentials' button on the top right. A window containing your API access key (1) and your secret key (2) will appear. Click on the eye icon to view your secret key. Keep this key secret and secure. Keep this page open for later reference. Section 3: Launch your personal GVL instance In a new browser tab, go to launch.genome.edu.au Fill in the required fields: Cloud: Keep the default: NeCTAR (OpenStack) Access key / Secret key: Copy and paste your access key and your secret key you obtained in the previous step in the required fields. Institutional email: Enter your institutional email address Cluster name: Choose 'Specify a new name' and enter a name for your instance (eg. GVL_workshop). It is recommended you choose a unique name if you launch multiple instances. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Cluster type: Keep the default: Cluster with Galaxy Storage type: Keep the default: Transient instance storage Optional advanced options Toggle the 'Show advanced startup options' option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Placement: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Image: The image to start. The default option is usually the latest, most stable GVL image. Flavor: Flavours are versions of the GVL with slightly customised toolsets. These toolsets are optimised for different usage patterns. More information about the different flavours can be found here . For this tutorial, keep the default option of 'GVL + Tutorial Indices' Click 'Create a cluster' to launch the image. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If your progress bar seems stuck on the 'Requesting' stage for > 5 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced startup options Placement field. You will need re-enter your secret key and your password. Section 4: Access your GVL instance Once your instance has finished launching, click on the cluster IP address to access your GVL dashboard. If you accidentally closed the launch page, you can find your cluster's address on the NeCTAR dashboard by navigating to Project > Compute > Instances on the left panel. This page contains a list of your instances and can be used to terminate your instance if anything goes wrong. Copy and paste the instance's IP address into your browser's URL navigation bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL. Section 5: GVL services Listed below are short descriptions of the services the GVL provides. Galaxy Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar. CloudMan CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username 'ubuntu' and your cluster password. You can also shut down your instance (permanently) with CloudMan. Lubuntu Desktop Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster password. SSH Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username 'ubuntu' or the username 'researcher' and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software). JupyterHub JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username 'researcher' and your cluster password. You may need to install Python packages you intend to use via the command line beforehand. RStudio RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username 'researcher' and your cluster password. Public HTML This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html will be publicly accessible. PacBio SMRT Portal PacBio's SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on 'Admin' in the top navigation bar. Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed. Scroll down to 'SMRT Analysis', and click 'install'. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user. Section 6: Shutting your machine down There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username 'ubuntu' and your cluster password. Click the Shut down... button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Launching a Personal GVL Server"},{"location":"tutorials/gvl_launch/gvl_launch/#launching-a-personal-gvl-server-on-the-nectar-research-cloud","text":"","title":"Launching a Personal GVL Server on the NeCTAR Research Cloud"},{"location":"tutorials/gvl_launch/gvl_launch/#tutorial-overview","text":"This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down","title":"Tutorial Overview"},{"location":"tutorials/gvl_launch/gvl_launch/#background","text":"","title":"Background"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-the-gvl","text":"The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Melbourne , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources.","title":"What is the GVL?"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-nectar","text":"The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free).","title":"What is NeCTAR?"},{"location":"tutorials/gvl_launch/gvl_launch/#section-1-access-the-nectar-dashboard","text":"Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose 'The University of Melbourne' and not 'The University of Melbourne (with ECP)'. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here .","title":"Section 1: Access the NeCTAR dashboard"},{"location":"tutorials/gvl_launch/gvl_launch/#section-2-get-your-cloud-credentials","text":"Launching a GVL instance requires EC2 API keys from NeCTAR. Obtaining these keys is a simple 3 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top 'API Access' tab. Click on the '+ View Credentials' button on the top right. A window containing your API access key (1) and your secret key (2) will appear. Click on the eye icon to view your secret key. Keep this key secret and secure. Keep this page open for later reference.","title":"Section 2: Get your cloud credentials"},{"location":"tutorials/gvl_launch/gvl_launch/#section-3-launch-your-personal-gvl-instance","text":"In a new browser tab, go to launch.genome.edu.au Fill in the required fields: Cloud: Keep the default: NeCTAR (OpenStack) Access key / Secret key: Copy and paste your access key and your secret key you obtained in the previous step in the required fields. Institutional email: Enter your institutional email address Cluster name: Choose 'Specify a new name' and enter a name for your instance (eg. GVL_workshop). It is recommended you choose a unique name if you launch multiple instances. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Cluster type: Keep the default: Cluster with Galaxy Storage type: Keep the default: Transient instance storage Optional advanced options Toggle the 'Show advanced startup options' option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Placement: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Image: The image to start. The default option is usually the latest, most stable GVL image. Flavor: Flavours are versions of the GVL with slightly customised toolsets. These toolsets are optimised for different usage patterns. More information about the different flavours can be found here . For this tutorial, keep the default option of 'GVL + Tutorial Indices' Click 'Create a cluster' to launch the image. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If your progress bar seems stuck on the 'Requesting' stage for > 5 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced startup options Placement field. You will need re-enter your secret key and your password.","title":"Section 3: Launch your personal GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-4-access-your-gvl-instance","text":"Once your instance has finished launching, click on the cluster IP address to access your GVL dashboard. If you accidentally closed the launch page, you can find your cluster's address on the NeCTAR dashboard by navigating to Project > Compute > Instances on the left panel. This page contains a list of your instances and can be used to terminate your instance if anything goes wrong. Copy and paste the instance's IP address into your browser's URL navigation bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL.","title":"Section 4: Access your GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-5-gvl-services","text":"Listed below are short descriptions of the services the GVL provides.","title":"Section 5: GVL services"},{"location":"tutorials/gvl_launch/gvl_launch/#galaxy","text":"Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar.","title":"Galaxy"},{"location":"tutorials/gvl_launch/gvl_launch/#cloudman","text":"CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username 'ubuntu' and your cluster password. You can also shut down your instance (permanently) with CloudMan.","title":"CloudMan"},{"location":"tutorials/gvl_launch/gvl_launch/#lubuntu-desktop","text":"Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster password.","title":"Lubuntu Desktop"},{"location":"tutorials/gvl_launch/gvl_launch/#ssh","text":"Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username 'ubuntu' or the username 'researcher' and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software).","title":"SSH"},{"location":"tutorials/gvl_launch/gvl_launch/#jupyterhub","text":"JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username 'researcher' and your cluster password. You may need to install Python packages you intend to use via the command line beforehand.","title":"JupyterHub"},{"location":"tutorials/gvl_launch/gvl_launch/#rstudio","text":"RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username 'researcher' and your cluster password.","title":"RStudio"},{"location":"tutorials/gvl_launch/gvl_launch/#public-html","text":"This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html will be publicly accessible.","title":"Public HTML"},{"location":"tutorials/gvl_launch/gvl_launch/#pacbio-smrt-portal","text":"PacBio's SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on 'Admin' in the top navigation bar. Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed. Scroll down to 'SMRT Analysis', and click 'install'. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user.","title":"PacBio SMRT Portal"},{"location":"tutorials/gvl_launch/gvl_launch/#section-6-shutting-your-machine-down","text":"There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username 'ubuntu' and your cluster password. Click the Shut down... button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Section 6: Shutting your machine down"},{"location":"tutorials/hpc/","text":"em {font-style: normal; font-family: courier new;} High-Performance Computing A hands-on-workshop covering High-Performance Computing (HPC) How to use this workshop The workshop is broken up into a number of Topics each focusing on a particular aspect of HPCs. You should take a short break between each to refresh and relax before tackling the next. Topic s may start with some background followed by a number of exercises . Each exercise begins with a question , then sometimes a hint (or two) and finishes with the suggested answer . Question An example question looks like: What is the Answer to Life? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Depending on how much of a challenge you like, you may choose to use hints. Even if you work out the answer without hints, its a good idea to read the hints afterwards because they contain extra information that is good to know. Note: hint s may be staged, that is, there may be a more section within a hint for further hints Hint <- click here to reveal hint What is the answer to everything? As featured in \"The Hitchhiker's Guide to the Galaxy\" More <- and here to show more It is probably a two digit number //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); if ($(\"#showable2\").hasClass(\"showable-hidden\")) { $(\"#showablelink2\").text(\"More\"); } else { $(\"#showablelink2\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Once you have worked out the answer to the question expand the Answer section to check if you got it correct. Answer <- click here to reveal answer Answer : 42 Ref: Number 42 (Wikipedia) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Usage Style This workshop attempts to cater for two usage styles: Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional): Attempt to answer the question by yourself. Use hints when you get stuck. Once solved, reveal the answer and read through our suggested solution. Its a good idea to read the hints and answer description as they often contain extra useful information. By example : for those who learn by following examples: Expand all sections Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the same (or similar) answers. Its a good idea to read the hints and answer description as they often contain extra useful information. Connecting to HPC To begin this workshop you will need to connect to an HPC. Today we will use the LIMS-HPC. The computer called lims-hpc-m (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks. Server details : host : lims-hpc-m.latrobe.edu.au port : 6022 username : trainingXX (where XX is a two digit number, provided at workshop) password : PROVIDED at workshop Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into LIMS-HPC, for example, type this command at the prompt and press return (where the word username is replaced with your LIMS-HPC username): ssh -p 6022 username@lims-hpc-m.latrobe.edu.au The same procedure works for any other machine where you have an account except most other HPCs will not need the -p 6022 (which is telling ssh to connect on a non-standard port number). You may be presented with a message along the lines of: The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type yes and then press enter . If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html Documentation for using PuTTY is here: http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html When you start PuTTY you should see a window which looks something like this: To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and 6022 in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text \"login as:\" . An example terminal window is shown below. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 1: Exploring an HPC An HPC (short for \u2018High Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions. Exercises 1.1) What is the contact email for your HPC's System Administrator? Hint When you login, you will be presented with a message; this is called the Message Of The Day and usually includes lots of useful information. On LIMS-HPC this includes a list of useful commands, the last login details for your account and the contact email of the system administrator //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink7\").click(function(e){ e.preventDefault(); $(\"#showable7\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer LIMS-HPC: andrew.robinson@latrobe.edu.au SNOWY & BARCOO: help@melbournebioinformatics.org.au //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 1.2) Run the sinfo command. How many nodes are there in this hpc? Hint The lims-hpc-[2-4] is shorthand for lims-hpc-2 lims-hpc-3 and lims-hpc-4 and lims-hpc-[1,5] is shorthand for lims-hpc-1 and lims-hpc-5 more Have a look at the NODELIST column. Only count each node once. $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m NOTE: the above list will vary depending on the HPC setup. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); if ($(\"#showable11\").hasClass(\"showable-hidden\")) { $(\"#showablelink11\").text(\"more\"); } else { $(\"#showablelink11\").text(\"less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink10\").click(function(e){ e.preventDefault(); $(\"#showable10\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The sinfo command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. LIMS-HPC: 6 ( lims-hpc-m and lims-hpc-1 through lims-hpc-5 ) MERRI: 84 ( turpin and merri001 through merri083 ) BARCOO: 70 ( barcoo001 through barcoo070 ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate Method An automatic (though more complex) way would have been running the following command: $ scontrol show node | grep NodeName | wc -l Where: scontrol show node : lists details of all nodes (over multiple lines) grep NodeName : only shows the NodeName line wc -l : counts the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 2: Software Modules Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily Exercises 2.1) What happens if you run the module command without any options / arguments? Hint Literally type module and press ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : It prints an error followed by a list of available options / flags $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.2) How do you find a list of available software? Hint Try the module command. Don't forget the man command to get help for a command More Run the command man module Use a search to find out about the avail subcommand (e.g. /avail<enter>) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink19\").click(function(e){ e.preventDefault(); $(\"#showable19\").toggleClass(\"showable-hidden\"); if ($(\"#showable19\").hasClass(\"showable-hidden\")) { $(\"#showablelink19\").text(\"More\"); } else { $(\"#showablelink19\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The module command is used to show details of software modules (tools). Answer : $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the /usr/local/Modules/modulefiles line are the science software; before this are a few built-in ones that you can ignore. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.3) How many modules are there starting with \u2018 f \u2019? Hint Run the command man module Use a search to find out about the avail subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the avail subcommand. More If an argument is given, then each directory in the MODULEPATH is searched for modulefiles whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink23\").click(function(e){ e.preventDefault(); $(\"#showable23\").toggleClass(\"showable-hidden\"); if ($(\"#showable23\").hasClass(\"showable-hidden\")) { $(\"#showablelink23\").text(\"More\"); } else { $(\"#showablelink23\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The man page told us that we could put a search term after module avail . $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastx_toolkit-gcc/0.0.14 freetype-gcc/2.5.3 Answer : 8 modules //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate Method To get a fully automated solution your could do the following command: $ module -l avail 2>&1 | grep \"^f\" | wc -l Where: module -l avail : lists all modules (in long format, i.e. one per line) 2>&1 : merges output from standard error to the standard output so it can be feed into grep. For some reason the developers of the module command thought it was a good idea to output the module names on the error stream rather than the logical output stream. grep \"^f\" : only shows lines beginning with f wc -l : counts the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.4) Run the pear command (without loading it), does it work? Hint This question is very literal //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink27\").click(function(e){ e.preventDefault(); $(\"#showable27\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' Answer : No, command not found //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.5) How would we load the pear module? Hint Check the man page for module again and look for a subcommand that might load modules; it is quite literal as well. More Run the command man module Use a search to find out about the load subcommand (e.g. /load<enter>) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); if ($(\"#showable31\").hasClass(\"showable-hidden\")) { $(\"#showablelink31\").text(\"More\"); } else { $(\"#showablelink31\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink30\").click(function(e){ e.preventDefault(); $(\"#showable30\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.6) Now it's load ed, run pear again, what does it do? Hint The paper citation gives a clue. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink34\").click(function(e){ e.preventDefault(); $(\"#showable34\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module load pear-gcc/0.9.4 [15:59:19] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____| / \\ | _ \\ | |_) | _| / _ \\ | |_) | | __/| |___ / ___ \\| _ < |_| |_____/_/ \\_\\_| \\_\\ PEAR v0.9.4 [August 8, 2014] - [+bzlib] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... Answer : \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink35\").click(function(e){ e.preventDefault(); $(\"#showable35\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.7) List all the loaded modules. How many are there? Where did all the others come from? Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink37\").click(function(e){ e.preventDefault(); $(\"#showable37\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer List all the loaded modules. How many are there? $ module list Currently Loaded Modulefiles: 1) gmp/5.1.3 3) mpc/1.0.2 5) bzip2-gcc/1.0.6 2) mpfr/3.1.2 4) gcc/4.8.2 6) pear-gcc/0.9.4 Answer : 6 Where did all the others come from? You may have noticed when we loaded pear-gcc the module called gcc was also loaded; this gives a hint as to where the others come from. Answer : They are dependencies ; that is, they are supporting software that is used by the module we loaded. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink38\").click(function(e){ e.preventDefault(); $(\"#showable38\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.8) How do you undo the loading of the pear module? List the loaded modules again, did they all disappear? Hint Computer Scientists are not always inventive with naming commands, try something starting with un //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink40\").click(function(e){ e.preventDefault(); $(\"#showable40\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer How do you undo the loading of the pear module? $ module unload pear-gcc Answer : the unload sub-command removes the named module from our current SSH session. List the loaded modules again, did they all disapear? Answer : Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink41\").click(function(e){ e.preventDefault(); $(\"#showable41\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.9) How do you clear ALL loaded modules? Hint It's easier than running unload for all modules This one isn't that straight forward; try a synonym of rid . More We will purge the list of loaded modules. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink44\").click(function(e){ e.preventDefault(); $(\"#showable44\").toggleClass(\"showable-hidden\"); if ($(\"#showable44\").hasClass(\"showable-hidden\")) { $(\"#showablelink44\").text(\"More\"); } else { $(\"#showablelink44\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink43\").click(function(e){ e.preventDefault(); $(\"#showable43\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module purge Answer : running the purge sub-command will unload all modules you loaded (and all dependencies). Alternative : if you close your SSH connection and re-open it the new session will be blank as well. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink45\").click(function(e){ e.preventDefault(); $(\"#showable45\").toggleClass(\"showable-hidden\"); }); }); //-->]]> LIMS-HPC Specific : The following questions use the moduleinfo command; this is only available on LIMS-HPC so if you are using another HPC then you will need to skip ahead to topic 3. 2.10) What does the moduleinfo command do? Hint Try running it (with no or only -h option) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink47\").click(function(e){ e.preventDefault(); $(\"#showable47\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo -h moduleinfo: support application for environment modules to provide licence and citation information about each module ... Answer : provides information about modules //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink48\").click(function(e){ e.preventDefault(); $(\"#showable48\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.11) Find a description of the biostreamtools module Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a description. Use the module command to find the full name for the biostreamtools module More Function : desc Module : biostreamtools-gcc/0.4.0 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink51\").click(function(e){ e.preventDefault(); $(\"#showable51\").toggleClass(\"showable-hidden\"); if ($(\"#showable51\").hasClass(\"showable-hidden\")) { $(\"#showablelink51\").text(\"More\"); } else { $(\"#showablelink51\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink50\").click(function(e){ e.preventDefault(); $(\"#showable50\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo desc biostreamtools-gcc/0.4.0 biostreamtools-gcc/0.4.0: A collection of fast generic bioinformatics tools implemented in C++ Answer : A collection of fast generic bioinformatics tools implemented in C++. Disclaimer : you may find that the author of this software also created this workshop :-P //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink52\").click(function(e){ e.preventDefault(); $(\"#showable52\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.12) How would you cite all currently loaded modules? Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a citation. More Function : cite //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink55\").click(function(e){ e.preventDefault(); $(\"#showable55\").toggleClass(\"showable-hidden\"); if ($(\"#showable55\").hasClass(\"showable-hidden\")) { $(\"#showablelink55\").text(\"More\"); } else { $(\"#showablelink55\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink54\").click(function(e){ e.preventDefault(); $(\"#showable54\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Assuming we had the pear module loaded $ moduleinfo cite gmp/5.1.3: No information recorded mpfr/3.1.2: No information recorded mpc/1.0.2: No information recorded gcc/4.8.2: No information recorded bzip2-gcc/1.0.6: No information recorded pear-gcc/0.9.4: J. Zhang, K. Kobert, T. Flouri, A. Stamatakis. PEAR: A fast and accurate Illimuna Paired-End reAd mergeR Answer : using the moduleinfo cite function with no module specified will display info for currently loaded modules. Note : When you see \"No information recorded\" it means that there is no moduleinfo record for that module. \"nil\" it means none was requested (at time software was installed, you should check software's website for updates since). \"No record\" means nothing could be found for this record/module //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink56\").click(function(e){ e.preventDefault(); $(\"#showable56\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.13) The malt module requires a special licence, how can you find out details of this? Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a description. Use the module command to find the full name for the malt module. More Verbose flag tells moduleinfo to give more information if it is available Function : licence Module : malt/0.1.0 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink59\").click(function(e){ e.preventDefault(); $(\"#showable59\").toggleClass(\"showable-hidden\"); if ($(\"#showable59\").hasClass(\"showable-hidden\")) { $(\"#showablelink59\").text(\"More\"); } else { $(\"#showablelink59\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink58\").click(function(e){ e.preventDefault(); $(\"#showable58\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo -v licence malt/0.1.0 [[malt/0.1.0]] -----[licence]----- Custom Academic: 1) You need to complete this form (which will send you an email): http://www-ab2.informatik.uni-tuebingen.de/software/megan5/register/index.php 2) Save the emailed licence details to a text file (suggested name: '~/megan-license.txt') on the LIMS-HPC. NOTE: you need to copy the text from the email starting at line \"User: ...\" and ending with line \"Signature: ...\" 3) When running the malt-* commands you need to specify this file. (Even for the --help option!!!). e.g. malt-build -L ~/megan-license.txt ... Answer : issuing the command moduleinfo -v licence malt/0.1.0 will display details on how to obtain the special licence for malt. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink60\").click(function(e){ e.preventDefault(); $(\"#showable60\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 3: Job Submission Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is ok for small jobs on small HPCs like LIMS-HPC, it's unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power Background On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can run be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal to a file; this file is called the output file. Exercises Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime 3.1) Which nodes could a \u2018compute\u2019 job go on? Hint Try the sinfo command more Have a look at the PARTITION and NODELIST columns. The lims-hpc-[2-4] is shorthand for lims-hpc-2 lims-hpc-3 and lims-hpc-4 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink63\").click(function(e){ e.preventDefault(); $(\"#showable63\").toggleClass(\"showable-hidden\"); if ($(\"#showable63\").hasClass(\"showable-hidden\")) { $(\"#showablelink63\").text(\"more\"); } else { $(\"#showablelink63\").text(\"less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink62\").click(function(e){ e.preventDefault(); $(\"#showable62\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The sinfo command will list the partitions . It summaries the nodes by their current status so there may be more that one line with compute in the partition column. It lists the nodes in shorthand i.e. lims-hpc-[1,3-5] means lims-hpc-1, lims-hpc-3, lims-hpc-4, lims-hpc-5. Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink64\").click(function(e){ e.preventDefault(); $(\"#showable64\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.2) What about an \u20188hour\u2019 job? Hint Use sinfo again but look at the 8hour rows //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink66\").click(function(e){ e.preventDefault(); $(\"#showable66\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5, lims-hpc-m //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink67\").click(function(e){ e.preventDefault(); $(\"#showable67\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Use the cat command to view the contents of task01 , task02 and task03 job script 3.3) How many cpu cores will each ask for? Hint Lookup the man page for sbatch command. sbatch 's options match up with the #SBATCH comments at the top of each job script. Some will be affected by more than one option More Non-exclusive (shared) jobs : It is --cpus-per-task x --ntasks but if --ntasks is not present it defaults to 1 so its --cpus-per-task x 1 Exclusive jobs : The --nodes options tells us how many nodes we ask for and the --exclusive option says give us all it has. This one is a bit tricky as we don't really know until it runs. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink70\").click(function(e){ e.preventDefault(); $(\"#showable70\").toggleClass(\"showable-hidden\"); if ($(\"#showable70\").hasClass(\"showable-hidden\")) { $(\"#showablelink70\").text(\"More\"); } else { $(\"#showablelink70\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink69\").click(function(e){ e.preventDefault(); $(\"#showable69\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : task01: 1 cpu core task02: 6 cpu cores task03: at least 1 as this has requested all cpu cores on the node its running on ( --exclusive ). However, since we know that all nodes on LIMS-HPC have 16, we know it will get 16. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink71\").click(function(e){ e.preventDefault(); $(\"#showable71\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.4) What about total memory? Hint Lookup the man page for sbatch command. sbatch 's options match up with the #SBATCH comments at the top of each job script. Some will be affected by more than one option More The --mem-per-cpu OR --mem options are holding the answer to total memory. For task01 and task02 the calculation is --mem-per-cpu x --cpus-per-task x --ntasks For task03, like with the cpus cores question, we get all the memory available on the node we get allocated //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink74\").click(function(e){ e.preventDefault(); $(\"#showable74\").toggleClass(\"showable-hidden\"); if ($(\"#showable74\").hasClass(\"showable-hidden\")) { $(\"#showablelink74\").text(\"More\"); } else { $(\"#showablelink74\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink73\").click(function(e){ e.preventDefault(); $(\"#showable73\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The --mem-per-cpu OR --mem options are holding the answer to total memory. For task01 and task02 the calculation is --mem-per-cpu x --ntasks x --cpus-per-task For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --shared ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. Answer : task01: 1024MB (1GB) i.e. 1024 x 1 x 1 task02: 12288MB (12GB) i.e. 2048 x 3 x 2 task03: at least 1024MB (1GB). The actual amount could be 128GB (nodes 2 to 5) or 256GB (node 1) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink75\").click(function(e){ e.preventDefault(); $(\"#showable75\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.5) How long can each run for? Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink77\").click(function(e){ e.preventDefault(); $(\"#showable77\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The --time option is what tells slurm how long your job will run for. Answer : task01: requests 30:00 (30mins 0secs) , uses ~30secs task02: requests 5:00 (5mins 0secs) , uses ~5secs task03: requests 1:00 (1min 0secs) , uses ~30secs //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink78\").click(function(e){ e.preventDefault(); $(\"#showable78\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.6) Is this maximum, minimum or both runtime? Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink80\").click(function(e){ e.preventDefault(); $(\"#showable80\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink81\").click(function(e){ e.preventDefault(); $(\"#showable81\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.7) Calculate the --time specification for the following runtimes: 1h30m: --time= 1m20s: --time= 1.5days: --time= 30m: --time= //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink82\").click(function(e){ e.preventDefault(); $(\"#showable82\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink83\").click(function(e){ e.preventDefault(); $(\"#showable83\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer 1h30m: --time=01:30:00 (alternatively: 0-01:30) 1m20s: --time=01:20 1.5days: --time=1-12 30m: --time=30 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink84\").click(function(e){ e.preventDefault(); $(\"#showable84\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.8) What do the following --time specifications mean? --time=12-00:20 --time=45 --time=00:30 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink85\").click(function(e){ e.preventDefault(); $(\"#showable85\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink86\").click(function(e){ e.preventDefault(); $(\"#showable86\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer --time=12-00:20 12 days and 20 minutes --time=45 45 minutes --time=00:30 30 seconds //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink87\").click(function(e){ e.preventDefault(); $(\"#showable87\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Now use sbatch to submit the task01 job: 3.9) What job id was your job given? Hint Use the man page for the sbatch command. The Synopsis at the top will give you an idea how to run it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink89\").click(function(e){ e.preventDefault(); $(\"#showable89\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer $ sbatch task01 Submitted batch job 9998 Answer : it's unique for each job; in the above example mine was 9998 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink90\").click(function(e){ e.preventDefault(); $(\"#showable90\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.10) Which node did your job go on? Hint The squeue command shows you the currently running jobs. If its been longer than 30 seconds since you submitted it you might have to resubmit it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink92\").click(function(e){ e.preventDefault(); $(\"#showable92\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Use the squeue command to show all jobs. Search for your jobid and look in the NODELIST column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 9999 compute task01 training R 0:05 1 lims-hpc-2 Answer : it's dependent on node availability at time; in the above example mine was lims-hpc-2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink93\").click(function(e){ e.preventDefault(); $(\"#showable93\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Advanced 3.11) Make a copy of task01 and call it prime_numbers . Make it load the training module and use the prime command calculate prime numbers for 20 seconds. Hint You can find the prime command in the training/1.0 module //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink95\").click(function(e){ e.preventDefault(); $(\"#showable95\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The key points to change in the task01 script are: adding the module load training/1.0 replacing the sleep (and echo ) statements with a call to prime 20 . #!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=training #SBATCH --time=30:00 module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink96\").click(function(e){ e.preventDefault(); $(\"#showable96\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.12) Submit the job. What was the largest prime number it found in 20 seconds? Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the SLURM output file ; this is called slurm-JOBID.out where JOBID is replaced by the actual job id. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink98\").click(function(e){ e.preventDefault(); $(\"#showable98\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16:11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16:11:27 AEST 2015 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink99\").click(function(e){ e.preventDefault(); $(\"#showable99\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.13) Modify your prime_numbers script to notify you via email when it starts and ends. Submit it again Did it start immediately or have some delay? How long did it actually run for? //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink100\").click(function(e){ e.preventDefault(); $(\"#showable100\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint There are two options that you will need to set. See sbatch manpage for details. More Both start with --mail //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink102\").click(function(e){ e.preventDefault(); $(\"#showable102\").toggleClass(\"showable-hidden\"); if ($(\"#showable102\").hasClass(\"showable-hidden\")) { $(\"#showablelink102\").text(\"More\"); } else { $(\"#showablelink102\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink101\").click(function(e){ e.preventDefault(); $(\"#showable101\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer #!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=training #SBATCH --time=30:00 #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Answers : Did it start immediately or have some delay? The Queued time value in the subject of start email will tell you how long it waited. How long did it actually run for? The Run time value in the subject of the end email will tell you how long it ran for which should be ~20 seconds. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink103\").click(function(e){ e.preventDefault(); $(\"#showable103\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 4: Job Monitoring It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU's, processing step etc.). In this topic we will cover some of the tools that are available to you to watch what is happening so we can make better predictions in the future. Exercises 4.1) What does the top command show? Hint When all else fails, try man ; specifically, the description section //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink105\").click(function(e){ e.preventDefault(); $(\"#showable105\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... Answer : in lay-person terms \"Continually updating CPU and Memory usage\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink106\").click(function(e){ e.preventDefault(); $(\"#showable106\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications). 4.2) How much total memory does this HPC (head-node) have? Hint This would be a system-wide statistic. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink108\").click(function(e){ e.preventDefault(); $(\"#showable108\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : If you look at the first value on the Mem line (line 4) it will tell you the total memory on this computer (node). LIMS-HPC : 132085396k or ~128 GigaBytes MERRI : 49413840k or ~48 GigaBytes BARCOO : 65942760k or ~64 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink109\").click(function(e){ e.preventDefault(); $(\"#showable109\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.3) What is the current total CPU usage? Hint This might be easier to work out what is not used and subtract it from 100% More Idle is another term for not used (or id for short) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink112\").click(function(e){ e.preventDefault(); $(\"#showable112\").toggleClass(\"showable-hidden\"); if ($(\"#showable112\").hasClass(\"showable-hidden\")) { $(\"#showablelink112\").text(\"More\"); } else { $(\"#showablelink112\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink111\").click(function(e){ e.preventDefault(); $(\"#showable111\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : If you subtract the %id value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink113\").click(function(e){ e.preventDefault(); $(\"#showable113\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low ? Hint Its not PID but from time to time it might be ordered sequentially. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink115\").click(function(e){ e.preventDefault(); $(\"#showable115\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : %CPU which gives you an indication of how much CPU time each process uses //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink116\").click(function(e){ e.preventDefault(); $(\"#showable116\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates. 4.5) Why might the numbers disagree? Hint It might have something to do with the total number of CPU Cores on the system. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink118\").click(function(e){ e.preventDefault(); $(\"#showable118\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : %CPU column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in top (excluding round errors) they would add up 100% x the number of cpu cores available; on LIMS-HPC this would be 0-1600% in the individual processes and 0-100% on the system-wide section. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink119\").click(function(e){ e.preventDefault(); $(\"#showable119\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.6) What command-line flag instructs top to sort results by %MEM ? Can you think of a reason that this might be useful? //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink120\").click(function(e){ e.preventDefault(); $(\"#showable120\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the top manpage. More \"m is for memory!\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink122\").click(function(e){ e.preventDefault(); $(\"#showable122\").toggleClass(\"showable-hidden\"); if ($(\"#showable122\").hasClass(\"showable-hidden\")) { $(\"#showablelink122\").text(\"More\"); } else { $(\"#showablelink122\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink121\").click(function(e){ e.preventDefault(); $(\"#showable121\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : top -m will cause top to sort the processes by memory usage. Can you think of a reason that this might be useful? Your program might be using a lot of memory and you want to know how much, by sorting by memory will cause your program to stay at the top. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink123\").click(function(e){ e.preventDefault(); $(\"#showable123\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.7) Run \"top -c\" . What does it do? How might this be helpful? Hint Use the top manpage. More \"c is for complete!\" \"c is also for command!\" which is another name for program //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink126\").click(function(e){ e.preventDefault(); $(\"#showable126\").toggleClass(\"showable-hidden\"); if ($(\"#showable126\").hasClass(\"showable-hidden\")) { $(\"#showablelink126\").text(\"More\"); } else { $(\"#showablelink126\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink125\").click(function(e){ e.preventDefault(); $(\"#showable125\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer What does it do? It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. How might this be helpful? Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the -c flag to show the complete command. NOTE : If top is running you can press the c key to toggle show/hide complete command //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink127\").click(function(e){ e.preventDefault(); $(\"#showable127\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.8) How can you get top to only show your processes? Why might this be useful? Hint Use the top manpage. More \"u is for user[name]!\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink130\").click(function(e){ e.preventDefault(); $(\"#showable130\").toggleClass(\"showable-hidden\"); if ($(\"#showable130\").hasClass(\"showable-hidden\")) { $(\"#showablelink130\").text(\"More\"); } else { $(\"#showablelink130\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink129\").click(function(e){ e.preventDefault(); $(\"#showable129\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer How can you get top to only show your processes? Answer 1 : top -u YOURUSERNAME Answer 2 : while running top press the u key, type YOURUSERNAME and press key Why might this be useful? When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink131\").click(function(e){ e.preventDefault(); $(\"#showable131\").toggleClass(\"showable-hidden\"); }); }); //-->]]> LIMS-HPC Specific LIMS-HPC has an extra monitoring and graphing tool called Munin. Open the munin webpage and have a look at the graphs Munin : http://munin-lims.latrobe.edu.au/lims-hpc.html 4.9) What are the graphs showing? Hint Take a look at the title on the graphs. Then the style of graphs. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink133\").click(function(e){ e.preventDefault(); $(\"#showable133\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : CPU usage (stacked by type of usage) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink134\").click(function(e){ e.preventDefault(); $(\"#showable134\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.10) How much is the Compute Node 5 being used currently? Show Answer You can either: estimate this off the right most position on the graph (everything except mid-yellow is the CPU doing something) or look at the cur value for idle and subtract it from 1600 (the maximum value for a 16 core server) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink136\").click(function(e){ e.preventDefault(); $(\"#showable136\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.11) And at midday yesterday? Hint Its easiest to think in reverse (i.e. What is not being used?) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink138\").click(function(e){ e.preventDefault(); $(\"#showable138\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer You have to estimate system idle at the point on the graph indicating 12:00 (yesterday). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink139\").click(function(e){ e.preventDefault(); $(\"#showable139\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 5: All Together This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job NOTE (for later) : to complete this topic from your regular LIMS-HPC account you will need to: first setup node logins. This has already been done for your *training* account so no need to do this today LIMS-HPC Node Login Setup use the *compute* partition instead of *training* Task 1: Write a job script Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : training Time : 5 mins Memory : 1 GB (remember to specify it in MB) Task 2: Load/use software module Edit your job script so that it: Loads the training module Runs the fakejob command with your name as the first parameter NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script. Task 3: Submit job Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task). Task 4: Monitor the job Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top wont be able to see the job. To be able to use top, you will first need to login to the compute node the is running your job. To login: $ ssh lims-hpc-X Where X is the actual node number you were allocated (See task 3.4). You are now connected from your computer to lims-hpc-m which is connected to lims-hpc-X. +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | LIMS-HPC-M | -- SSH --> | LIMS-HPC-X | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt [10:00:06] USERNAME@lims-hpc-m ~ $ Changes to: [10:06:05] USERNAME@lims-hpc-1 ~ $ Once logged in to the relevent compute node you can run top to view you job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs. How does the CPU and Memory usage change over time? Hint It should vary (within the limits you set in the job script) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink141\").click(function(e){ e.preventDefault(); $(\"#showable141\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The fakejob program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory (on lims-hpc-[m,2-5] this will equate to 0.4 to 0.8%) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink142\").click(function(e){ e.preventDefault(); $(\"#showable142\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Finished Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be confortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this Workshop for later reference. Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it back to the Workshop facilitators.","title":"Introduction to HPC"},{"location":"tutorials/hpc/#high-performance-computing","text":"A hands-on-workshop covering High-Performance Computing (HPC)","title":"High-Performance Computing"},{"location":"tutorials/hpc/#how-to-use-this-workshop","text":"The workshop is broken up into a number of Topics each focusing on a particular aspect of HPCs. You should take a short break between each to refresh and relax before tackling the next. Topic s may start with some background followed by a number of exercises . Each exercise begins with a question , then sometimes a hint (or two) and finishes with the suggested answer .","title":"How to use this workshop"},{"location":"tutorials/hpc/#question","text":"An example question looks like: What is the Answer to Life? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]>","title":"Question"},{"location":"tutorials/hpc/#hint","text":"Depending on how much of a challenge you like, you may choose to use hints. Even if you work out the answer without hints, its a good idea to read the hints afterwards because they contain extra information that is good to know. Note: hint s may be staged, that is, there may be a more section within a hint for further hints Hint <- click here to reveal hint What is the answer to everything? As featured in \"The Hitchhiker's Guide to the Galaxy\" More <- and here to show more It is probably a two digit number //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); if ($(\"#showable2\").hasClass(\"showable-hidden\")) { $(\"#showablelink2\").text(\"More\"); } else { $(\"#showablelink2\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Hint"},{"location":"tutorials/hpc/#answer","text":"Once you have worked out the answer to the question expand the Answer section to check if you got it correct. Answer <- click here to reveal answer Answer : 42 Ref: Number 42 (Wikipedia) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Answer"},{"location":"tutorials/hpc/#usage-style","text":"This workshop attempts to cater for two usage styles: Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional): Attempt to answer the question by yourself. Use hints when you get stuck. Once solved, reveal the answer and read through our suggested solution. Its a good idea to read the hints and answer description as they often contain extra useful information. By example : for those who learn by following examples: Expand all sections Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the same (or similar) answers. Its a good idea to read the hints and answer description as they often contain extra useful information.","title":"Usage Style"},{"location":"tutorials/hpc/#connecting-to-hpc","text":"To begin this workshop you will need to connect to an HPC. Today we will use the LIMS-HPC. The computer called lims-hpc-m (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks. Server details : host : lims-hpc-m.latrobe.edu.au port : 6022 username : trainingXX (where XX is a two digit number, provided at workshop) password : PROVIDED at workshop Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into LIMS-HPC, for example, type this command at the prompt and press return (where the word username is replaced with your LIMS-HPC username): ssh -p 6022 username@lims-hpc-m.latrobe.edu.au The same procedure works for any other machine where you have an account except most other HPCs will not need the -p 6022 (which is telling ssh to connect on a non-standard port number). You may be presented with a message along the lines of: The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type yes and then press enter . If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html Documentation for using PuTTY is here: http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html When you start PuTTY you should see a window which looks something like this: To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and 6022 in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text \"login as:\" . An example terminal window is shown below. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Connecting to HPC"},{"location":"tutorials/hpc/#topic-1-exploring-an-hpc","text":"An HPC (short for \u2018High Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions.","title":"Topic 1: Exploring an HPC"},{"location":"tutorials/hpc/#exercises","text":"1.1) What is the contact email for your HPC's System Administrator? Hint When you login, you will be presented with a message; this is called the Message Of The Day and usually includes lots of useful information. On LIMS-HPC this includes a list of useful commands, the last login details for your account and the contact email of the system administrator //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink7\").click(function(e){ e.preventDefault(); $(\"#showable7\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer LIMS-HPC: andrew.robinson@latrobe.edu.au SNOWY & BARCOO: help@melbournebioinformatics.org.au //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 1.2) Run the sinfo command. How many nodes are there in this hpc? Hint The lims-hpc-[2-4] is shorthand for lims-hpc-2 lims-hpc-3 and lims-hpc-4 and lims-hpc-[1,5] is shorthand for lims-hpc-1 and lims-hpc-5 more Have a look at the NODELIST column. Only count each node once. $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m NOTE: the above list will vary depending on the HPC setup. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); if ($(\"#showable11\").hasClass(\"showable-hidden\")) { $(\"#showablelink11\").text(\"more\"); } else { $(\"#showablelink11\").text(\"less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink10\").click(function(e){ e.preventDefault(); $(\"#showable10\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The sinfo command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. LIMS-HPC: 6 ( lims-hpc-m and lims-hpc-1 through lims-hpc-5 ) MERRI: 84 ( turpin and merri001 through merri083 ) BARCOO: 70 ( barcoo001 through barcoo070 ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate Method An automatic (though more complex) way would have been running the following command: $ scontrol show node | grep NodeName | wc -l Where: scontrol show node : lists details of all nodes (over multiple lines) grep NodeName : only shows the NodeName line wc -l : counts the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Exercises"},{"location":"tutorials/hpc/#topic-2-software-modules","text":"Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily","title":"Topic 2: Software Modules"},{"location":"tutorials/hpc/#exercises_1","text":"2.1) What happens if you run the module command without any options / arguments? Hint Literally type module and press ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : It prints an error followed by a list of available options / flags $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.2) How do you find a list of available software? Hint Try the module command. Don't forget the man command to get help for a command More Run the command man module Use a search to find out about the avail subcommand (e.g. /avail<enter>) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink19\").click(function(e){ e.preventDefault(); $(\"#showable19\").toggleClass(\"showable-hidden\"); if ($(\"#showable19\").hasClass(\"showable-hidden\")) { $(\"#showablelink19\").text(\"More\"); } else { $(\"#showablelink19\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The module command is used to show details of software modules (tools). Answer : $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the /usr/local/Modules/modulefiles line are the science software; before this are a few built-in ones that you can ignore. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.3) How many modules are there starting with \u2018 f \u2019? Hint Run the command man module Use a search to find out about the avail subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the avail subcommand. More If an argument is given, then each directory in the MODULEPATH is searched for modulefiles whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink23\").click(function(e){ e.preventDefault(); $(\"#showable23\").toggleClass(\"showable-hidden\"); if ($(\"#showable23\").hasClass(\"showable-hidden\")) { $(\"#showablelink23\").text(\"More\"); } else { $(\"#showablelink23\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The man page told us that we could put a search term after module avail . $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastx_toolkit-gcc/0.0.14 freetype-gcc/2.5.3 Answer : 8 modules //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate Method To get a fully automated solution your could do the following command: $ module -l avail 2>&1 | grep \"^f\" | wc -l Where: module -l avail : lists all modules (in long format, i.e. one per line) 2>&1 : merges output from standard error to the standard output so it can be feed into grep. For some reason the developers of the module command thought it was a good idea to output the module names on the error stream rather than the logical output stream. grep \"^f\" : only shows lines beginning with f wc -l : counts the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.4) Run the pear command (without loading it), does it work? Hint This question is very literal //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink27\").click(function(e){ e.preventDefault(); $(\"#showable27\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' Answer : No, command not found //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.5) How would we load the pear module? Hint Check the man page for module again and look for a subcommand that might load modules; it is quite literal as well. More Run the command man module Use a search to find out about the load subcommand (e.g. /load<enter>) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); if ($(\"#showable31\").hasClass(\"showable-hidden\")) { $(\"#showablelink31\").text(\"More\"); } else { $(\"#showablelink31\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink30\").click(function(e){ e.preventDefault(); $(\"#showable30\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.6) Now it's load ed, run pear again, what does it do? Hint The paper citation gives a clue. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink34\").click(function(e){ e.preventDefault(); $(\"#showable34\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module load pear-gcc/0.9.4 [15:59:19] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____| / \\ | _ \\ | |_) | _| / _ \\ | |_) | | __/| |___ / ___ \\| _ < |_| |_____/_/ \\_\\_| \\_\\ PEAR v0.9.4 [August 8, 2014] - [+bzlib] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al (2014) Bioinformatics 30(5): 614-620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... Answer : \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink35\").click(function(e){ e.preventDefault(); $(\"#showable35\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.7) List all the loaded modules. How many are there? Where did all the others come from? Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink37\").click(function(e){ e.preventDefault(); $(\"#showable37\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer List all the loaded modules. How many are there? $ module list Currently Loaded Modulefiles: 1) gmp/5.1.3 3) mpc/1.0.2 5) bzip2-gcc/1.0.6 2) mpfr/3.1.2 4) gcc/4.8.2 6) pear-gcc/0.9.4 Answer : 6 Where did all the others come from? You may have noticed when we loaded pear-gcc the module called gcc was also loaded; this gives a hint as to where the others come from. Answer : They are dependencies ; that is, they are supporting software that is used by the module we loaded. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink38\").click(function(e){ e.preventDefault(); $(\"#showable38\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.8) How do you undo the loading of the pear module? List the loaded modules again, did they all disappear? Hint Computer Scientists are not always inventive with naming commands, try something starting with un //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink40\").click(function(e){ e.preventDefault(); $(\"#showable40\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer How do you undo the loading of the pear module? $ module unload pear-gcc Answer : the unload sub-command removes the named module from our current SSH session. List the loaded modules again, did they all disapear? Answer : Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink41\").click(function(e){ e.preventDefault(); $(\"#showable41\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.9) How do you clear ALL loaded modules? Hint It's easier than running unload for all modules This one isn't that straight forward; try a synonym of rid . More We will purge the list of loaded modules. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink44\").click(function(e){ e.preventDefault(); $(\"#showable44\").toggleClass(\"showable-hidden\"); if ($(\"#showable44\").hasClass(\"showable-hidden\")) { $(\"#showablelink44\").text(\"More\"); } else { $(\"#showablelink44\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink43\").click(function(e){ e.preventDefault(); $(\"#showable43\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ module purge Answer : running the purge sub-command will unload all modules you loaded (and all dependencies). Alternative : if you close your SSH connection and re-open it the new session will be blank as well. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink45\").click(function(e){ e.preventDefault(); $(\"#showable45\").toggleClass(\"showable-hidden\"); }); }); //-->]]> LIMS-HPC Specific : The following questions use the moduleinfo command; this is only available on LIMS-HPC so if you are using another HPC then you will need to skip ahead to topic 3. 2.10) What does the moduleinfo command do? Hint Try running it (with no or only -h option) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink47\").click(function(e){ e.preventDefault(); $(\"#showable47\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo -h moduleinfo: support application for environment modules to provide licence and citation information about each module ... Answer : provides information about modules //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink48\").click(function(e){ e.preventDefault(); $(\"#showable48\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.11) Find a description of the biostreamtools module Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a description. Use the module command to find the full name for the biostreamtools module More Function : desc Module : biostreamtools-gcc/0.4.0 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink51\").click(function(e){ e.preventDefault(); $(\"#showable51\").toggleClass(\"showable-hidden\"); if ($(\"#showable51\").hasClass(\"showable-hidden\")) { $(\"#showablelink51\").text(\"More\"); } else { $(\"#showablelink51\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink50\").click(function(e){ e.preventDefault(); $(\"#showable50\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo desc biostreamtools-gcc/0.4.0 biostreamtools-gcc/0.4.0: A collection of fast generic bioinformatics tools implemented in C++ Answer : A collection of fast generic bioinformatics tools implemented in C++. Disclaimer : you may find that the author of this software also created this workshop :-P //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink52\").click(function(e){ e.preventDefault(); $(\"#showable52\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.12) How would you cite all currently loaded modules? Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a citation. More Function : cite //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink55\").click(function(e){ e.preventDefault(); $(\"#showable55\").toggleClass(\"showable-hidden\"); if ($(\"#showable55\").hasClass(\"showable-hidden\")) { $(\"#showablelink55\").text(\"More\"); } else { $(\"#showablelink55\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink54\").click(function(e){ e.preventDefault(); $(\"#showable54\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Assuming we had the pear module loaded $ moduleinfo cite gmp/5.1.3: No information recorded mpfr/3.1.2: No information recorded mpc/1.0.2: No information recorded gcc/4.8.2: No information recorded bzip2-gcc/1.0.6: No information recorded pear-gcc/0.9.4: J. Zhang, K. Kobert, T. Flouri, A. Stamatakis. PEAR: A fast and accurate Illimuna Paired-End reAd mergeR Answer : using the moduleinfo cite function with no module specified will display info for currently loaded modules. Note : When you see \"No information recorded\" it means that there is no moduleinfo record for that module. \"nil\" it means none was requested (at time software was installed, you should check software's website for updates since). \"No record\" means nothing could be found for this record/module //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink56\").click(function(e){ e.preventDefault(); $(\"#showable56\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.13) The malt module requires a special licence, how can you find out details of this? Hint View the help information provided when you ran moduleinfo -h . Search for a function that displays a description. Use the module command to find the full name for the malt module. More Verbose flag tells moduleinfo to give more information if it is available Function : licence Module : malt/0.1.0 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink59\").click(function(e){ e.preventDefault(); $(\"#showable59\").toggleClass(\"showable-hidden\"); if ($(\"#showable59\").hasClass(\"showable-hidden\")) { $(\"#showablelink59\").text(\"More\"); } else { $(\"#showablelink59\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink58\").click(function(e){ e.preventDefault(); $(\"#showable58\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ moduleinfo -v licence malt/0.1.0 [[malt/0.1.0]] -----[licence]----- Custom Academic: 1) You need to complete this form (which will send you an email): http://www-ab2.informatik.uni-tuebingen.de/software/megan5/register/index.php 2) Save the emailed licence details to a text file (suggested name: '~/megan-license.txt') on the LIMS-HPC. NOTE: you need to copy the text from the email starting at line \"User: ...\" and ending with line \"Signature: ...\" 3) When running the malt-* commands you need to specify this file. (Even for the --help option!!!). e.g. malt-build -L ~/megan-license.txt ... Answer : issuing the command moduleinfo -v licence malt/0.1.0 will display details on how to obtain the special licence for malt. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink60\").click(function(e){ e.preventDefault(); $(\"#showable60\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Exercises"},{"location":"tutorials/hpc/#topic-3-job-submission","text":"Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is ok for small jobs on small HPCs like LIMS-HPC, it's unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power","title":"Topic 3: Job Submission"},{"location":"tutorials/hpc/#background","text":"On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \"I want XX CPUS for YY hours\" and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can run be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal to a file; this file is called the output file.","title":"Background"},{"location":"tutorials/hpc/#exercises_2","text":"Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime 3.1) Which nodes could a \u2018compute\u2019 job go on? Hint Try the sinfo command more Have a look at the PARTITION and NODELIST columns. The lims-hpc-[2-4] is shorthand for lims-hpc-2 lims-hpc-3 and lims-hpc-4 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200-00:00: 3 mix lims-hpc-[2-4] compute* up 200-00:00: 2 idle lims-hpc-[1,5] bigmem up 200-00:00: 1 idle lims-hpc-1 8hour up 08:00:00 3 mix lims-hpc-[2-4] 8hour up 08:00:00 3 idle lims-hpc-[1,5],lims-hpc-m //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink63\").click(function(e){ e.preventDefault(); $(\"#showable63\").toggleClass(\"showable-hidden\"); if ($(\"#showable63\").hasClass(\"showable-hidden\")) { $(\"#showablelink63\").text(\"more\"); } else { $(\"#showablelink63\").text(\"less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink62\").click(function(e){ e.preventDefault(); $(\"#showable62\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The sinfo command will list the partitions . It summaries the nodes by their current status so there may be more that one line with compute in the partition column. It lists the nodes in shorthand i.e. lims-hpc-[1,3-5] means lims-hpc-1, lims-hpc-3, lims-hpc-4, lims-hpc-5. Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink64\").click(function(e){ e.preventDefault(); $(\"#showable64\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.2) What about an \u20188hour\u2019 job? Hint Use sinfo again but look at the 8hour rows //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink66\").click(function(e){ e.preventDefault(); $(\"#showable66\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : lims-hpc-1, lims-hpc-2 lims-hpc-3, lims-hpc-4, lims-hpc-5, lims-hpc-m //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink67\").click(function(e){ e.preventDefault(); $(\"#showable67\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Use the cat command to view the contents of task01 , task02 and task03 job script 3.3) How many cpu cores will each ask for? Hint Lookup the man page for sbatch command. sbatch 's options match up with the #SBATCH comments at the top of each job script. Some will be affected by more than one option More Non-exclusive (shared) jobs : It is --cpus-per-task x --ntasks but if --ntasks is not present it defaults to 1 so its --cpus-per-task x 1 Exclusive jobs : The --nodes options tells us how many nodes we ask for and the --exclusive option says give us all it has. This one is a bit tricky as we don't really know until it runs. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink70\").click(function(e){ e.preventDefault(); $(\"#showable70\").toggleClass(\"showable-hidden\"); if ($(\"#showable70\").hasClass(\"showable-hidden\")) { $(\"#showablelink70\").text(\"More\"); } else { $(\"#showablelink70\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink69\").click(function(e){ e.preventDefault(); $(\"#showable69\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : task01: 1 cpu core task02: 6 cpu cores task03: at least 1 as this has requested all cpu cores on the node its running on ( --exclusive ). However, since we know that all nodes on LIMS-HPC have 16, we know it will get 16. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink71\").click(function(e){ e.preventDefault(); $(\"#showable71\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.4) What about total memory? Hint Lookup the man page for sbatch command. sbatch 's options match up with the #SBATCH comments at the top of each job script. Some will be affected by more than one option More The --mem-per-cpu OR --mem options are holding the answer to total memory. For task01 and task02 the calculation is --mem-per-cpu x --cpus-per-task x --ntasks For task03, like with the cpus cores question, we get all the memory available on the node we get allocated //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink74\").click(function(e){ e.preventDefault(); $(\"#showable74\").toggleClass(\"showable-hidden\"); if ($(\"#showable74\").hasClass(\"showable-hidden\")) { $(\"#showablelink74\").text(\"More\"); } else { $(\"#showablelink74\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink73\").click(function(e){ e.preventDefault(); $(\"#showable73\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The --mem-per-cpu OR --mem options are holding the answer to total memory. For task01 and task02 the calculation is --mem-per-cpu x --ntasks x --cpus-per-task For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --shared ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. Answer : task01: 1024MB (1GB) i.e. 1024 x 1 x 1 task02: 12288MB (12GB) i.e. 2048 x 3 x 2 task03: at least 1024MB (1GB). The actual amount could be 128GB (nodes 2 to 5) or 256GB (node 1) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink75\").click(function(e){ e.preventDefault(); $(\"#showable75\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.5) How long can each run for? Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink77\").click(function(e){ e.preventDefault(); $(\"#showable77\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The --time option is what tells slurm how long your job will run for. Answer : task01: requests 30:00 (30mins 0secs) , uses ~30secs task02: requests 5:00 (5mins 0secs) , uses ~5secs task03: requests 1:00 (1min 0secs) , uses ~30secs //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink78\").click(function(e){ e.preventDefault(); $(\"#showable78\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.6) Is this maximum, minimum or both runtime? Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink80\").click(function(e){ e.preventDefault(); $(\"#showable80\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink81\").click(function(e){ e.preventDefault(); $(\"#showable81\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.7) Calculate the --time specification for the following runtimes: 1h30m: --time= 1m20s: --time= 1.5days: --time= 30m: --time= //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink82\").click(function(e){ e.preventDefault(); $(\"#showable82\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink83\").click(function(e){ e.preventDefault(); $(\"#showable83\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer 1h30m: --time=01:30:00 (alternatively: 0-01:30) 1m20s: --time=01:20 1.5days: --time=1-12 30m: --time=30 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink84\").click(function(e){ e.preventDefault(); $(\"#showable84\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.8) What do the following --time specifications mean? --time=12-00:20 --time=45 --time=00:30 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink85\").click(function(e){ e.preventDefault(); $(\"#showable85\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the man sbatch command to look up the time specification. If you search for --time it will describe the formats it uses (i.e. type /--time and press enter) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink86\").click(function(e){ e.preventDefault(); $(\"#showable86\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer --time=12-00:20 12 days and 20 minutes --time=45 45 minutes --time=00:30 30 seconds //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink87\").click(function(e){ e.preventDefault(); $(\"#showable87\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Now use sbatch to submit the task01 job: 3.9) What job id was your job given? Hint Use the man page for the sbatch command. The Synopsis at the top will give you an idea how to run it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink89\").click(function(e){ e.preventDefault(); $(\"#showable89\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer $ sbatch task01 Submitted batch job 9998 Answer : it's unique for each job; in the above example mine was 9998 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink90\").click(function(e){ e.preventDefault(); $(\"#showable90\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.10) Which node did your job go on? Hint The squeue command shows you the currently running jobs. If its been longer than 30 seconds since you submitted it you might have to resubmit it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink92\").click(function(e){ e.preventDefault(); $(\"#showable92\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Use the squeue command to show all jobs. Search for your jobid and look in the NODELIST column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 9999 compute task01 training R 0:05 1 lims-hpc-2 Answer : it's dependent on node availability at time; in the above example mine was lims-hpc-2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink93\").click(function(e){ e.preventDefault(); $(\"#showable93\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Exercises"},{"location":"tutorials/hpc/#advanced","text":"3.11) Make a copy of task01 and call it prime_numbers . Make it load the training module and use the prime command calculate prime numbers for 20 seconds. Hint You can find the prime command in the training/1.0 module //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink95\").click(function(e){ e.preventDefault(); $(\"#showable95\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The key points to change in the task01 script are: adding the module load training/1.0 replacing the sleep (and echo ) statements with a call to prime 20 . #!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=training #SBATCH --time=30:00 module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink96\").click(function(e){ e.preventDefault(); $(\"#showable96\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.12) Submit the job. What was the largest prime number it found in 20 seconds? Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the SLURM output file ; this is called slurm-JOBID.out where JOBID is replaced by the actual job id. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink98\").click(function(e){ e.preventDefault(); $(\"#showable98\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16:11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16:11:27 AEST 2015 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink99\").click(function(e){ e.preventDefault(); $(\"#showable99\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.13) Modify your prime_numbers script to notify you via email when it starts and ends. Submit it again Did it start immediately or have some delay? How long did it actually run for? //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink100\").click(function(e){ e.preventDefault(); $(\"#showable100\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint There are two options that you will need to set. See sbatch manpage for details. More Both start with --mail //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink102\").click(function(e){ e.preventDefault(); $(\"#showable102\").toggleClass(\"showable-hidden\"); if ($(\"#showable102\").hasClass(\"showable-hidden\")) { $(\"#showablelink102\").text(\"More\"); } else { $(\"#showablelink102\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink101\").click(function(e){ e.preventDefault(); $(\"#showable101\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer #!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=training #SBATCH --time=30:00 #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $(date)\" prime 20 echo \"Finished at: $(date)\" Answers : Did it start immediately or have some delay? The Queued time value in the subject of start email will tell you how long it waited. How long did it actually run for? The Run time value in the subject of the end email will tell you how long it ran for which should be ~20 seconds. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink103\").click(function(e){ e.preventDefault(); $(\"#showable103\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Advanced"},{"location":"tutorials/hpc/#topic-4-job-monitoring","text":"It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU's, processing step etc.). In this topic we will cover some of the tools that are available to you to watch what is happening so we can make better predictions in the future.","title":"Topic 4: Job Monitoring"},{"location":"tutorials/hpc/#exercises_3","text":"4.1) What does the top command show? Hint When all else fails, try man ; specifically, the description section //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink105\").click(function(e){ e.preventDefault(); $(\"#showable105\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... Answer : in lay-person terms \"Continually updating CPU and Memory usage\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink106\").click(function(e){ e.preventDefault(); $(\"#showable106\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications). 4.2) How much total memory does this HPC (head-node) have? Hint This would be a system-wide statistic. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink108\").click(function(e){ e.preventDefault(); $(\"#showable108\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : If you look at the first value on the Mem line (line 4) it will tell you the total memory on this computer (node). LIMS-HPC : 132085396k or ~128 GigaBytes MERRI : 49413840k or ~48 GigaBytes BARCOO : 65942760k or ~64 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink109\").click(function(e){ e.preventDefault(); $(\"#showable109\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.3) What is the current total CPU usage? Hint This might be easier to work out what is not used and subtract it from 100% More Idle is another term for not used (or id for short) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink112\").click(function(e){ e.preventDefault(); $(\"#showable112\").toggleClass(\"showable-hidden\"); if ($(\"#showable112\").hasClass(\"showable-hidden\")) { $(\"#showablelink112\").text(\"More\"); } else { $(\"#showablelink112\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink111\").click(function(e){ e.preventDefault(); $(\"#showable111\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : If you subtract the %id value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink113\").click(function(e){ e.preventDefault(); $(\"#showable113\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low ? Hint Its not PID but from time to time it might be ordered sequentially. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink115\").click(function(e){ e.preventDefault(); $(\"#showable115\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : %CPU which gives you an indication of how much CPU time each process uses //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink116\").click(function(e){ e.preventDefault(); $(\"#showable116\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates. 4.5) Why might the numbers disagree? Hint It might have something to do with the total number of CPU Cores on the system. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink118\").click(function(e){ e.preventDefault(); $(\"#showable118\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : %CPU column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in top (excluding round errors) they would add up 100% x the number of cpu cores available; on LIMS-HPC this would be 0-1600% in the individual processes and 0-100% on the system-wide section. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink119\").click(function(e){ e.preventDefault(); $(\"#showable119\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.6) What command-line flag instructs top to sort results by %MEM ? Can you think of a reason that this might be useful? //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink120\").click(function(e){ e.preventDefault(); $(\"#showable120\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use the top manpage. More \"m is for memory!\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink122\").click(function(e){ e.preventDefault(); $(\"#showable122\").toggleClass(\"showable-hidden\"); if ($(\"#showable122\").hasClass(\"showable-hidden\")) { $(\"#showablelink122\").text(\"More\"); } else { $(\"#showablelink122\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink121\").click(function(e){ e.preventDefault(); $(\"#showable121\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : top -m will cause top to sort the processes by memory usage. Can you think of a reason that this might be useful? Your program might be using a lot of memory and you want to know how much, by sorting by memory will cause your program to stay at the top. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink123\").click(function(e){ e.preventDefault(); $(\"#showable123\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.7) Run \"top -c\" . What does it do? How might this be helpful? Hint Use the top manpage. More \"c is for complete!\" \"c is also for command!\" which is another name for program //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink126\").click(function(e){ e.preventDefault(); $(\"#showable126\").toggleClass(\"showable-hidden\"); if ($(\"#showable126\").hasClass(\"showable-hidden\")) { $(\"#showablelink126\").text(\"More\"); } else { $(\"#showablelink126\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink125\").click(function(e){ e.preventDefault(); $(\"#showable125\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer What does it do? It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. How might this be helpful? Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the -c flag to show the complete command. NOTE : If top is running you can press the c key to toggle show/hide complete command //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink127\").click(function(e){ e.preventDefault(); $(\"#showable127\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.8) How can you get top to only show your processes? Why might this be useful? Hint Use the top manpage. More \"u is for user[name]!\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink130\").click(function(e){ e.preventDefault(); $(\"#showable130\").toggleClass(\"showable-hidden\"); if ($(\"#showable130\").hasClass(\"showable-hidden\")) { $(\"#showablelink130\").text(\"More\"); } else { $(\"#showablelink130\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink129\").click(function(e){ e.preventDefault(); $(\"#showable129\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer How can you get top to only show your processes? Answer 1 : top -u YOURUSERNAME Answer 2 : while running top press the u key, type YOURUSERNAME and press key Why might this be useful? When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink131\").click(function(e){ e.preventDefault(); $(\"#showable131\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Exercises"},{"location":"tutorials/hpc/#lims-hpc-specific","text":"LIMS-HPC has an extra monitoring and graphing tool called Munin. Open the munin webpage and have a look at the graphs Munin : http://munin-lims.latrobe.edu.au/lims-hpc.html 4.9) What are the graphs showing? Hint Take a look at the title on the graphs. Then the style of graphs. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink133\").click(function(e){ e.preventDefault(); $(\"#showable133\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer Answer : CPU usage (stacked by type of usage) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink134\").click(function(e){ e.preventDefault(); $(\"#showable134\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.10) How much is the Compute Node 5 being used currently? Show Answer You can either: estimate this off the right most position on the graph (everything except mid-yellow is the CPU doing something) or look at the cur value for idle and subtract it from 1600 (the maximum value for a 16 core server) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink136\").click(function(e){ e.preventDefault(); $(\"#showable136\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.11) And at midday yesterday? Hint Its easiest to think in reverse (i.e. What is not being used?) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink138\").click(function(e){ e.preventDefault(); $(\"#showable138\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer You have to estimate system idle at the point on the graph indicating 12:00 (yesterday). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink139\").click(function(e){ e.preventDefault(); $(\"#showable139\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"LIMS-HPC Specific"},{"location":"tutorials/hpc/#topic-5-all-together","text":"This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job NOTE (for later) : to complete this topic from your regular LIMS-HPC account you will need to: first setup node logins. This has already been done for your *training* account so no need to do this today LIMS-HPC Node Login Setup use the *compute* partition instead of *training*","title":"Topic 5: All Together"},{"location":"tutorials/hpc/#task-1-write-a-job-script","text":"Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : training Time : 5 mins Memory : 1 GB (remember to specify it in MB)","title":"Task 1: Write a job script"},{"location":"tutorials/hpc/#task-2-loaduse-software-module","text":"Edit your job script so that it: Loads the training module Runs the fakejob command with your name as the first parameter NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script.","title":"Task 2: Load/use software module"},{"location":"tutorials/hpc/#task-3-submit-job","text":"Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task).","title":"Task 3: Submit job"},{"location":"tutorials/hpc/#task-4-monitor-the-job","text":"Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top wont be able to see the job. To be able to use top, you will first need to login to the compute node the is running your job. To login: $ ssh lims-hpc-X Where X is the actual node number you were allocated (See task 3.4). You are now connected from your computer to lims-hpc-m which is connected to lims-hpc-X. +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | LIMS-HPC-M | -- SSH --> | LIMS-HPC-X | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt [10:00:06] USERNAME@lims-hpc-m ~ $ Changes to: [10:06:05] USERNAME@lims-hpc-1 ~ $ Once logged in to the relevent compute node you can run top to view you job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs. How does the CPU and Memory usage change over time? Hint It should vary (within the limits you set in the job script) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink141\").click(function(e){ e.preventDefault(); $(\"#showable141\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Show Answer The fakejob program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory (on lims-hpc-[m,2-5] this will equate to 0.4 to 0.8%) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink142\").click(function(e){ e.preventDefault(); $(\"#showable142\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Task 4: Monitor the job"},{"location":"tutorials/hpc/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be confortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this Workshop for later reference. Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it back to the Workshop facilitators.","title":"Finished"},{"location":"tutorials/ngs_overview/","text":"PR reviewers and advice: Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/ngs_overview/NGS_Overview/","text":"Next Generation Sequencing Overview Sequencing technologies Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction) Applications Genomic Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a 'metasample') Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target Epigenomic CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment Transcriptomic (RNA-seq) Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis Types of outcome Variation Detection Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements De Novo assembly Concatemers (length?) Comparative gene expression Matrix of gene vs expression level Splicing detection List of alternative splice isoforms Resequencing alignment strategies Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex) Categories of analysis NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - 'base calling'. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis. Sample preparation Fragment Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read Paired-end sequencing Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus 'related' and have extra information about the structure of the sample. The information is that the two reads from the single fragment are 'close' - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn Mate-pair Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn Barcoded fragments Multiple samples are prepared independently. Each sample is 'labelled' with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes. Enrichment for target sequences PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation Potential services in NGS Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data Appendix Paired end sequencing - empirical observations paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, \\~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all. Mate-pair sequencing Illumina refers to \"paired end\" as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the 'internal' ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard 'paired end' sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"NGS Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#next-generation-sequencing-overview","text":"","title":"Next Generation Sequencing Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#sequencing-technologies","text":"Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction)","title":"Sequencing technologies"},{"location":"tutorials/ngs_overview/NGS_Overview/#applications","text":"","title":"Applications"},{"location":"tutorials/ngs_overview/NGS_Overview/#genomic","text":"Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a 'metasample') Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target","title":"Genomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#epigenomic","text":"CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment","title":"Epigenomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#transcriptomic-rna-seq","text":"Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis","title":"Transcriptomic (RNA-seq)"},{"location":"tutorials/ngs_overview/NGS_Overview/#types-of-outcome","text":"","title":"Types of outcome"},{"location":"tutorials/ngs_overview/NGS_Overview/#variation-detection","text":"Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements","title":"Variation Detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#de-novo-assembly","text":"Concatemers (length?)","title":"De Novo assembly"},{"location":"tutorials/ngs_overview/NGS_Overview/#comparative-gene-expression","text":"Matrix of gene vs expression level","title":"Comparative gene expression"},{"location":"tutorials/ngs_overview/NGS_Overview/#splicing-detection","text":"List of alternative splice isoforms","title":"Splicing detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#resequencing-alignment-strategies","text":"Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex)","title":"Resequencing alignment strategies"},{"location":"tutorials/ngs_overview/NGS_Overview/#categories-of-analysis","text":"NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - 'base calling'. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis.","title":"Categories of analysis"},{"location":"tutorials/ngs_overview/NGS_Overview/#sample-preparation","text":"","title":"Sample preparation"},{"location":"tutorials/ngs_overview/NGS_Overview/#fragment","text":"Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read","title":"Fragment"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing","text":"Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus 'related' and have extra information about the structure of the sample. The information is that the two reads from the single fragment are 'close' - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn","title":"Paired-end sequencing"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair","text":"Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn","title":"Mate-pair"},{"location":"tutorials/ngs_overview/NGS_Overview/#barcoded-fragments","text":"Multiple samples are prepared independently. Each sample is 'labelled' with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes.","title":"Barcoded fragments"},{"location":"tutorials/ngs_overview/NGS_Overview/#enrichment-for-target-sequences","text":"PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation","title":"Enrichment for target sequences"},{"location":"tutorials/ngs_overview/NGS_Overview/#potential-services-in-ngs","text":"Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data","title":"Potential services in NGS"},{"location":"tutorials/ngs_overview/NGS_Overview/#appendix","text":"","title":"Appendix"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing-empirical-observations","text":"paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, \\~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all.","title":"Paired end sequencing - empirical observations"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair-sequencing","text":"Illumina refers to \"paired end\" as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the 'internal' ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard 'paired end' sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"Mate-pair sequencing"},{"location":"tutorials/proteomics_basic/","text":"PR reviewers and advice: Ira Cooke Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/proteomics_basic/background_data_formats/","text":"Data Formats and Pre-processing Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats Background"},{"location":"tutorials/proteomics_basic/background_data_formats/#data-formats-and-pre-processing","text":"Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats and Pre-processing"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/","text":"Create a workflow Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create find count items workflow"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/#create-a-workflow","text":"Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create a workflow"},{"location":"tutorials/proteomics_basic/background_galaxy/","text":"How to use Galaxy This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki . Create a new History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \"Unnamed History\". Click its title to rename it. Rename a history item Locate the item in your history and click its pencil icon Enter a new name in the Name: field and click Save Find someone's exact username Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username Share a History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user's full username Import a History Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name Copy Datasets Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories Multi File Inputs Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected. Saved Histories To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"Galaxy Background"},{"location":"tutorials/proteomics_basic/background_galaxy/#how-to-use-galaxy","text":"This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki .","title":"How to use Galaxy"},{"location":"tutorials/proteomics_basic/background_galaxy/#create-a-new-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \"Unnamed History\". Click its title to rename it.","title":"Create a new History"},{"location":"tutorials/proteomics_basic/background_galaxy/#rename-a-history-item","text":"Locate the item in your history and click its pencil icon Enter a new name in the Name: field and click Save","title":"Rename a history item"},{"location":"tutorials/proteomics_basic/background_galaxy/#find-someones-exact-username","text":"Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username","title":"Find someone's exact username"},{"location":"tutorials/proteomics_basic/background_galaxy/#share-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user's full username","title":"Share a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#import-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name","title":"Import a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#copy-datasets","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories","title":"Copy Datasets"},{"location":"tutorials/proteomics_basic/background_galaxy/#multi-file-inputs","text":"Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected.","title":"Multi File Inputs"},{"location":"tutorials/proteomics_basic/background_galaxy/#saved-histories","text":"To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"Saved Histories"},{"location":"tutorials/proteomics_basic/background_protein_databases/","text":"Protein Databases In a perfect experiment we would obtain fragment ions for all the b,y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database. Large vs Small Database Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue. Typical sources of data for search databases Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome. Should I include decoys? Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to 'pin down' the negative distribution.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#protein-databases","text":"In a perfect experiment we would obtain fragment ions for all the b,y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#large-vs-small-database","text":"Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue.","title":"Large vs Small Database"},{"location":"tutorials/proteomics_basic/background_protein_databases/#typical-sources-of-data-for-search-databases","text":"Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome.","title":"Typical sources of data for search databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#should-i-include-decoys","text":"Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to 'pin down' the negative distribution.","title":"Should I include decoys?"},{"location":"tutorials/proteomics_basic/background_protein_prophet/","text":"Protein Prophet The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It's citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let's look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1.0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr|Q3UN47|Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp|O08600|NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1.0 ) but probabilities group members are different. The first member of the group has a high probability 0.99 but all other members have probabilities of 0.0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_protein_prophet/#protein-prophet","text":"The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It's citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let's look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1.0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr|Q3UN47|Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp|O08600|NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1.0 ) but probabilities group members are different. The first member of the group has a high probability 0.99 but all other members have probabilities of 0.0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_search_engines/","text":"How Search Engines Work When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it's old, I recommend this presentation by Brian Searle","title":"Search Engines"},{"location":"tutorials/proteomics_basic/background_search_engines/#how-search-engines-work","text":"When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it's old, I recommend this presentation by Brian Searle","title":"How Search Engines Work"},{"location":"tutorials/proteomics_basic/proteomics_basic/","text":"Identifying proteins from mass spec data Overview This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together Login to Galaxy Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server Import mass spec data Create a new history in Galaxy and name it \"Organelle Tutorial\" Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \"Edit attributes\". This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \"/\" Your item should then be named OrganelleSample.mzML Dont forget to click \"Save\" Basic properties of the data Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\". There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \"MS1 spectrum\" in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); if ($(\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate data formats Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); if ($(\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Obtain a Search Database Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; sp|Q9CQV8|1433B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Run a search using X!Tandem A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716.fasta MSMS File OrganelleSample.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0.5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine's work . Convert Results to tabular format Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Sort tabular outputs Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys. Convert raw scores to probabilities Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to Peptide Prophet is Percolator . If you're curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \"greater than or equal to\" in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. c11>=0.95 To answer the second question use the Select tool on the filtered table to select lines matching \"decoy_\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); if ($(\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); if ($(\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Perform Protein Inference Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we've viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings. Functional enrichment analysis This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \"pipes\" that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Tutorial"},{"location":"tutorials/proteomics_basic/proteomics_basic/#identifying-proteins-from-mass-spec-data","text":"","title":"Identifying proteins from mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#overview","text":"This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together","title":"Overview"},{"location":"tutorials/proteomics_basic/proteomics_basic/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server","title":"Login to Galaxy"},{"location":"tutorials/proteomics_basic/proteomics_basic/#import-mass-spec-data","text":"Create a new history in Galaxy and name it \"Organelle Tutorial\" Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \"Edit attributes\". This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \"/\" Your item should then be named OrganelleSample.mzML Dont forget to click \"Save\"","title":"Import mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#basic-properties-of-the-data","text":"Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\". There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \"MS1 spectrum\" in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); if ($(\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Basic properties of the data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#alternate-data-formats","text":"Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); if ($(\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Alternate data formats"},{"location":"tutorials/proteomics_basic/proteomics_basic/#obtain-a-search-database","text":"Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; sp|Q9CQV8|1433B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Obtain a Search Database"},{"location":"tutorials/proteomics_basic/proteomics_basic/#run-a-search-using-xtandem","text":"A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716.fasta MSMS File OrganelleSample.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0.5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine's work .","title":"Run a search using X!Tandem"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-results-to-tabular-format","text":"Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert Results to tabular format"},{"location":"tutorials/proteomics_basic/proteomics_basic/#sort-tabular-outputs","text":"Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys.","title":"Sort tabular outputs"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-raw-scores-to-probabilities","text":"Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to Peptide Prophet is Percolator . If you're curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \"greater than or equal to\" in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. c11>=0.95 To answer the second question use the Select tool on the filtered table to select lines matching \"decoy_\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); if ($(\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); $(\"#showable25\").toggleClass(\"showable-hidden\"); if ($(\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert raw scores to probabilities"},{"location":"tutorials/proteomics_basic/proteomics_basic/#perform-protein-inference","text":"Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we've viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.","title":"Perform Protein Inference"},{"location":"tutorials/proteomics_basic/proteomics_basic/#functional-enrichment-analysis","text":"This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \"pipes\" that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Functional enrichment analysis"},{"location":"tutorials/python_overview/","text":"PR reviewers and advice: Bernard Pope Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/python_overview/python_overview/","text":"Authors: Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne General information Python modules are stored in files containing a \".py\" suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used. Python 2 versus Python 3 Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary. Indentation for grouping code blocks Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code. Style Guide A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: # Compute factorial of n, # assuming n >= 0 def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result print(factorial(10)) C program for computing factorial: #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial(int n) { int result = 1; while (n > 0) { result *= n; n -= 1; } return result; } int main(void) { printf(\"%d\\n\", factorial(10)); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer). Comments Program comments start with a hash character \"#\" and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. ''' Running a Python program There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented. Objects and types Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \".\" operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type(x) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): >>> # Create a list, assign to the variable x >>> x = [3, 1, 2, 3] >>> # Ask for the type of the value assigned to x >>> type(x) <class 'list'> >>> # Ask for the type of the first item in the list (an integer) >>> type(x[0]) <class 'int'> >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x.count(3) 2 >>> # Sort the contents of the list in-place. >>> # Note that this mutates the list object! >>> # Also note that Python does not print the result in this case. >>> x.sort() >>> # Ask Python to show the value of the list >>> # assigned to the variable x (note it is now sorted) >>> x [1, 2, 3, 3] >>> # Assign x to an object of a different type (a float) >>> x = 3.142 >>> type(x) <class 'float'> Booleans Represent truth values Values: True , False Type: bool Operators: and , or , not bool(x) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0.0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: >>> not True False >>> not False True >>> not () True >>> not [1,2,3] False >>> True and False False >>> True and () () Conditional Statements Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: if expression: statement-block elif expression: statement-block ... else: statement-block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: >>> if []: ... print(\"Was considered True\") ... else: ... print(\"Was considered False\") ... Was considered False Numbers and basic mathematics Integers Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means (4 * 10) + 2 ) Hexadecimal literals start with 0x , octal literals start with 0o , binary literals start with 0b . int(x) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0x10 16 >>> 0b10 2 >>> -0 == 0 True >>> int(\"123\") 123 >>> int(\"3.142\") Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ValueError: invalid literal for int() with base 10: '3.142' Floating Point Numbers Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys.float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3.142 Exponential: 314.2e-2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float(x) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: >>> type(3.142) <class 'float'> >>> type(12) <class 'int'> >>> 3.142 + 12 15.142 >>> 3.142 == 314.2e-2 True >>> 3. == 3.0 True >>> 1/0 Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ZeroDivisionError: division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3.3333333333333335 >>> float(\"123\") 123.0 >>> float(\"3.142\") 3.142 Complex Numbers Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real +/- imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: >>> 5j + 3j 8j >>> 2-5j (2-5j) >>> 2-5j + 3j (2-2j) Numeric Operators Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): >>> 3 + 4 * 5 23 >>> (3 + 4) * 5 35 >>> 10 / 3 3.3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> (2 ** 3) ** 4 4096 Strings Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn't going to work though' File \"<stdin>\", line 1 'this isn't going to work though' ^ SyntaxError: invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\n newline \\t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s[5] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): >>> type(\"hello\") <class 'str'> >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\".upper() 'BONJOUR' >>> len(\"bonjour\") 7 >>> \"bonjour\".startswith(\"b\") True >>> \"cat,sat,flat\".split(\",\") ['cat', 'sat', 'flat'] >>> # Print the first 5 Chinese unicode characters >>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04') \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x[0] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: # Prompt the user to input a string: input = raw_input(\"Enter string: \") # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input: if char in vowels: count += 1 # Print the count to the standard output print(count) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels.py : python vowels.py Enter string: abracadabra 5 Lists Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [1, 2, 3] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x[3] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: >>> type([1, 2, 3]) <class 'list'> >>> x = [] >>> len(x) 0 >>> x.append(\"hello\") >>> x ['hello'] >>> len(x) 1 >>> x[0] 'hello' >>> x.insert(0, True) >>> x [True, 'hello'] >>> del x[1] >>> x [True] >>> x += [42, \"Newton\", 3.142] >>> x [True, 42, 'Newton', 3.142] Dictionaries Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. {12: \"XII\", 6: \"VI\"} Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: >>> type({12: \"XII\", 6: \"VI\"}) <class 'dict'> >>> friends = {} >>> friends['Fred'] = ['Barney', 'Dino'] >>> friends {'Fred': ['Barney', 'Dino']} >>> friends['Fred'] ['Barney', 'Dino'] >>> friends['Barney'] Traceback (most recent call last): File \"\\<stdin\\>\", line 1, in \\<module\\> KeyError: 'Barney' >>> friends['Wilma'] = ['Betty'] >>> friends {'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']} >>> friends.keys() dict_keys(['Fred', 'Wilma']) >>> friends.values() dict_values([['Barney', 'Dino'], ['Betty']]) >>> 'Dino' in friends False Example program: # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys.stdin: # Parse the next input as an integer next_integer = int(line) # Update the histogram accordingly if next_integer in histogram: # We've seen this integer before histogram[next_integer] += 1 else: # First occurrence of this integer in the input histogram[next_integer] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted(histogram): print(\"{} {}\".format(key, histogram[key]))s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo.py : python histo.py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 3 43 12 19 3 12 12 43 Program prints its output: 3 2 12 3 19 1 43 2 Tuples Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: (1, 2, 3) The can be used as keys in dictionaries (unlike lists). Loops While loops Iterate until condition is False Syntax: while expression: statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result For loops Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: for variable in expression: statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range() function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: def factorial(n): result = 1 for item in range(n + 1): result *= item return result Break and continue Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly. Functions Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: def variable(parameter_list): statement_block Anonymous function syntax: lambda parameter_list: expression Example: def is_leap_year(year): if year % 4 == 0 and year % 100 != 0: return True else: return year % 400 == 0 for year in range(2000, 2100 + 1): result = is_leap_year(year) print(\"{} {}\".format(year, result)) Anonymous function example: >>> squared = lambda x: x ** 2 >>> squared(2) 4 >>> list(map(lambda x: x + 1, [1, 2, 3])) [2, 3, 4] Input and output The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys.argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys.argv[1] # Open the file file = open(filename) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file: num_lines += 1 num_words += len(line.split()) file.close() print(\"Number of lines and words in {}: {} {}\" \\ .format(filename, num_lines, num_words)) Advanced Topics Classes Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: class variable(superclass_list): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: class Vector(object): def __init__(self, x=0, y=0, z=0): self.x = x self.y = y self.z = z def magnitude(self): return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) def normalise(self): magnitude = self.magnitude() if magnitude == 0: # Somehow we have a degenerate vector. return self else: return self / self.magnitude() def angle(self, other): dp = self.dot_product(other) return acos(dp / self.magnitude() * other.magnitude()) def dot_product(self, other): return self.x * other.x + self.y * other.y + self.z * other.z Exceptions Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: try: statement_block except exception_type as variable: statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: # alternative version of the histogram code from the section on # dictionaries for line in sys.stdin: next_integer = int(line) try: histogram[next_integer] += 1 except KeyError: histogram[next_integer] = 1 Modules A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: >>> import math >>> math.sqrt(100) 10.0 >>> sqrt(100) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NameError: name 'sqrt' is not defined >>> from math import sqrt >>> sqrt(100) 10.0 >>> import math as m >>> m.sqrt(100) 10.0 Packages A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \"sandboxed\" Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Python Overview"},{"location":"tutorials/python_overview/python_overview/#authors","text":"Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne","title":"Authors:"},{"location":"tutorials/python_overview/python_overview/#general-information","text":"Python modules are stored in files containing a \".py\" suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used.","title":"General information"},{"location":"tutorials/python_overview/python_overview/#python-2-versus-python-3","text":"Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary.","title":"Python 2 versus Python 3"},{"location":"tutorials/python_overview/python_overview/#indentation-for-grouping-code-blocks","text":"Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.","title":"Indentation for grouping code blocks"},{"location":"tutorials/python_overview/python_overview/#style-guide","text":"A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: # Compute factorial of n, # assuming n >= 0 def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result print(factorial(10)) C program for computing factorial: #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial(int n) { int result = 1; while (n > 0) { result *= n; n -= 1; } return result; } int main(void) { printf(\"%d\\n\", factorial(10)); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer).","title":"Style Guide"},{"location":"tutorials/python_overview/python_overview/#comments","text":"Program comments start with a hash character \"#\" and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. '''","title":"Comments"},{"location":"tutorials/python_overview/python_overview/#running-a-python-program","text":"There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented.","title":"Running a Python program"},{"location":"tutorials/python_overview/python_overview/#objects-and-types","text":"Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \".\" operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type(x) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): >>> # Create a list, assign to the variable x >>> x = [3, 1, 2, 3] >>> # Ask for the type of the value assigned to x >>> type(x) <class 'list'> >>> # Ask for the type of the first item in the list (an integer) >>> type(x[0]) <class 'int'> >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x.count(3) 2 >>> # Sort the contents of the list in-place. >>> # Note that this mutates the list object! >>> # Also note that Python does not print the result in this case. >>> x.sort() >>> # Ask Python to show the value of the list >>> # assigned to the variable x (note it is now sorted) >>> x [1, 2, 3, 3] >>> # Assign x to an object of a different type (a float) >>> x = 3.142 >>> type(x) <class 'float'>","title":"Objects and types"},{"location":"tutorials/python_overview/python_overview/#booleans","text":"Represent truth values Values: True , False Type: bool Operators: and , or , not bool(x) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0.0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: >>> not True False >>> not False True >>> not () True >>> not [1,2,3] False >>> True and False False >>> True and () ()","title":"Booleans"},{"location":"tutorials/python_overview/python_overview/#conditional-statements","text":"Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: if expression: statement-block elif expression: statement-block ... else: statement-block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: >>> if []: ... print(\"Was considered True\") ... else: ... print(\"Was considered False\") ... Was considered False","title":"Conditional Statements"},{"location":"tutorials/python_overview/python_overview/#numbers-and-basic-mathematics","text":"","title":"Numbers and basic mathematics"},{"location":"tutorials/python_overview/python_overview/#integers","text":"Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means (4 * 10) + 2 ) Hexadecimal literals start with 0x , octal literals start with 0o , binary literals start with 0b . int(x) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0x10 16 >>> 0b10 2 >>> -0 == 0 True >>> int(\"123\") 123 >>> int(\"3.142\") Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ValueError: invalid literal for int() with base 10: '3.142'","title":"Integers"},{"location":"tutorials/python_overview/python_overview/#floating-point-numbers","text":"Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys.float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3.142 Exponential: 314.2e-2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float(x) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: >>> type(3.142) <class 'float'> >>> type(12) <class 'int'> >>> 3.142 + 12 15.142 >>> 3.142 == 314.2e-2 True >>> 3. == 3.0 True >>> 1/0 Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ZeroDivisionError: division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3.3333333333333335 >>> float(\"123\") 123.0 >>> float(\"3.142\") 3.142","title":"Floating Point Numbers"},{"location":"tutorials/python_overview/python_overview/#complex-numbers","text":"Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real +/- imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: >>> 5j + 3j 8j >>> 2-5j (2-5j) >>> 2-5j + 3j (2-2j)","title":"Complex Numbers"},{"location":"tutorials/python_overview/python_overview/#numeric-operators","text":"Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): >>> 3 + 4 * 5 23 >>> (3 + 4) * 5 35 >>> 10 / 3 3.3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> (2 ** 3) ** 4 4096","title":"Numeric Operators"},{"location":"tutorials/python_overview/python_overview/#strings","text":"Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn't going to work though' File \"<stdin>\", line 1 'this isn't going to work though' ^ SyntaxError: invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\n newline \\t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s[5] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): >>> type(\"hello\") <class 'str'> >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\".upper() 'BONJOUR' >>> len(\"bonjour\") 7 >>> \"bonjour\".startswith(\"b\") True >>> \"cat,sat,flat\".split(\",\") ['cat', 'sat', 'flat'] >>> # Print the first 5 Chinese unicode characters >>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04') \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x[0] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: # Prompt the user to input a string: input = raw_input(\"Enter string: \") # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input: if char in vowels: count += 1 # Print the count to the standard output print(count) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels.py : python vowels.py Enter string: abracadabra 5","title":"Strings"},{"location":"tutorials/python_overview/python_overview/#lists","text":"Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [1, 2, 3] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x[3] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: >>> type([1, 2, 3]) <class 'list'> >>> x = [] >>> len(x) 0 >>> x.append(\"hello\") >>> x ['hello'] >>> len(x) 1 >>> x[0] 'hello' >>> x.insert(0, True) >>> x [True, 'hello'] >>> del x[1] >>> x [True] >>> x += [42, \"Newton\", 3.142] >>> x [True, 42, 'Newton', 3.142]","title":"Lists"},{"location":"tutorials/python_overview/python_overview/#dictionaries","text":"Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. {12: \"XII\", 6: \"VI\"} Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: >>> type({12: \"XII\", 6: \"VI\"}) <class 'dict'> >>> friends = {} >>> friends['Fred'] = ['Barney', 'Dino'] >>> friends {'Fred': ['Barney', 'Dino']} >>> friends['Fred'] ['Barney', 'Dino'] >>> friends['Barney'] Traceback (most recent call last): File \"\\<stdin\\>\", line 1, in \\<module\\> KeyError: 'Barney' >>> friends['Wilma'] = ['Betty'] >>> friends {'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']} >>> friends.keys() dict_keys(['Fred', 'Wilma']) >>> friends.values() dict_values([['Barney', 'Dino'], ['Betty']]) >>> 'Dino' in friends False Example program: # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys.stdin: # Parse the next input as an integer next_integer = int(line) # Update the histogram accordingly if next_integer in histogram: # We've seen this integer before histogram[next_integer] += 1 else: # First occurrence of this integer in the input histogram[next_integer] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted(histogram): print(\"{} {}\".format(key, histogram[key]))s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo.py : python histo.py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 3 43 12 19 3 12 12 43 Program prints its output: 3 2 12 3 19 1 43 2","title":"Dictionaries"},{"location":"tutorials/python_overview/python_overview/#tuples","text":"Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: (1, 2, 3) The can be used as keys in dictionaries (unlike lists).","title":"Tuples"},{"location":"tutorials/python_overview/python_overview/#loops","text":"","title":"Loops"},{"location":"tutorials/python_overview/python_overview/#while-loops","text":"Iterate until condition is False Syntax: while expression: statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: def factorial(n): result = 1 while n > 0: result *= n n -= 1 return result","title":"While loops"},{"location":"tutorials/python_overview/python_overview/#for-loops","text":"Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: for variable in expression: statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range() function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: def factorial(n): result = 1 for item in range(n + 1): result *= item return result","title":"For loops"},{"location":"tutorials/python_overview/python_overview/#break-and-continue","text":"Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly.","title":"Break and continue"},{"location":"tutorials/python_overview/python_overview/#functions","text":"Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: def variable(parameter_list): statement_block Anonymous function syntax: lambda parameter_list: expression Example: def is_leap_year(year): if year % 4 == 0 and year % 100 != 0: return True else: return year % 400 == 0 for year in range(2000, 2100 + 1): result = is_leap_year(year) print(\"{} {}\".format(year, result)) Anonymous function example: >>> squared = lambda x: x ** 2 >>> squared(2) 4 >>> list(map(lambda x: x + 1, [1, 2, 3])) [2, 3, 4]","title":"Functions"},{"location":"tutorials/python_overview/python_overview/#input-and-output","text":"The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys.argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys.argv[1] # Open the file file = open(filename) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file: num_lines += 1 num_words += len(line.split()) file.close() print(\"Number of lines and words in {}: {} {}\" \\ .format(filename, num_lines, num_words))","title":"Input and output"},{"location":"tutorials/python_overview/python_overview/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"tutorials/python_overview/python_overview/#classes","text":"Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: class variable(superclass_list): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: class Vector(object): def __init__(self, x=0, y=0, z=0): self.x = x self.y = y self.z = z def magnitude(self): return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) def normalise(self): magnitude = self.magnitude() if magnitude == 0: # Somehow we have a degenerate vector. return self else: return self / self.magnitude() def angle(self, other): dp = self.dot_product(other) return acos(dp / self.magnitude() * other.magnitude()) def dot_product(self, other): return self.x * other.x + self.y * other.y + self.z * other.z","title":"Classes"},{"location":"tutorials/python_overview/python_overview/#exceptions","text":"Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: try: statement_block except exception_type as variable: statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: # alternative version of the histogram code from the section on # dictionaries for line in sys.stdin: next_integer = int(line) try: histogram[next_integer] += 1 except KeyError: histogram[next_integer] = 1","title":"Exceptions"},{"location":"tutorials/python_overview/python_overview/#modules","text":"A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: >>> import math >>> math.sqrt(100) 10.0 >>> sqrt(100) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NameError: name 'sqrt' is not defined >>> from math import sqrt >>> sqrt(100) 10.0 >>> import math as m >>> m.sqrt(100) 10.0","title":"Modules"},{"location":"tutorials/python_overview/python_overview/#packages","text":"A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \"sandboxed\" Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Packages"},{"location":"tutorials/rna_seq_dge_advanced/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Advanced Tutorial Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial. Background [15 min] Where does the data in this tutorial come from? The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total-- two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8. Section 1: Preparation [15 min] 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Adv_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed. Section 2: Alignment [30 mins] In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Map/align the reads with Tophat to the S. cerevisiae reference In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is. 2. Rename the output files You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. 'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam') by using the pen icon next to the file. 3. Visualise the aligned reads with Trackster On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select S. cerevisiae (sacCer2) as the reference genome build. Click the Add Datasets to Visualization button and select 'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the checkboxes on the left. Select chrI from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Load one of the splice junction files such as 'Tophat on data 2 and data 1: splice junctions'. Explore the data and try to find a splice junction. Next to the drop down list, click on the chromosomal position number display and specify the location chrI:86985-87795 to view an intron junction. Ideally we would add a gene model to the visualisation; but the genes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser) has a slightly different naming convention for one of the chromosomes than the reference genome used by Galaxy, which will cause an error to be thrown by Trackster if you try to add it. This is very typical of genomics currently! If you are interested, you can fiddle with the genes.gtf file to rename the chromosome '2-micron' to '2micron', which will fix the problem. Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar. Section 3. Cuffdiff [40 min] The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes. 1. Run Cuffdiff to identify differentially expressed genes and transcripts In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This step may take a while, depending on how busy the server is. 2. Explore the Cuffdiff output files There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 43, data 38, and others\". We'll mostly be interested in the file ending with 'gene differential expression testing' which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Cuffdiff_Significant_DE_Genes\". Section 4. Count reads in features [30 min] HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes. Section 5: edgeR [30 min] edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models. 1. Generate a list of differentially expressed genes using edgeR In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute 2. Examine the outputs from the previous step Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user's guide at Bioconductor . 3. Extract the significant differentially expressed genes. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_edgeR_topTable_edgeR.xls\" With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"edgeR_Significant_DE_Genes\". Section 6. DESeq2 [30 min] DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity. 1. Generate a list of differentially expressed genes using DESeq2 In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2 2. Examine the outputs the previous step Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Filter out the significant differentially expressed genes. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_DESeq2_topTable_DESeq2.xls\" With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see the first few differentially expressed genes are similar to the ones identified by EdgeR. Section 7: How much concordance is there between methods? We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like 'Cuffdiff_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like 'edgeR_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like 'DESeq2_gene_list' Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_common_gene_list' Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list' We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools. Section 8: Gene set enrichment analysis The biological question being asked in the original paper is essentially: \"What is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\" We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As... Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one. Optional extension: Degust Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ . 1. Load count data into Degust In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\" generated in Section 4 using the disk icon . Go to vicbioinformatics.com/degust/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data. Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Explore your data Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. References [1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"RNA-seq DGE Advanced"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#rna-seq-differential-gene-expression-advanced-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Advanced Tutorial"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#tutorial-overview","text":"In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#background-15-min","text":"","title":"Background [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total-- two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account","text":"Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Adv_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-2-alignment-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Alignment [30 mins]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-mapalign-the-reads-with-tophat-to-the-s-cerevisiae-reference","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Map/align the reads with Tophat to the S. cerevisiae reference"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-rename-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. 'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam') by using the pen icon next to the file.","title":"2.  Rename the output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-visualise-the-aligned-reads-with-trackster","text":"On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select S. cerevisiae (sacCer2) as the reference genome build. Click the Add Datasets to Visualization button and select 'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the checkboxes on the left. Select chrI from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Load one of the splice junction files such as 'Tophat on data 2 and data 1: splice junctions'. Explore the data and try to find a splice junction. Next to the drop down list, click on the chromosomal position number display and specify the location chrI:86985-87795 to view an intron junction. Ideally we would add a gene model to the visualisation; but the genes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser) has a slightly different naming convention for one of the chromosomes than the reference genome used by Galaxy, which will cause an error to be thrown by Trackster if you try to add it. This is very typical of genomics currently! If you are interested, you can fiddle with the genes.gtf file to rename the chromosome '2-micron' to '2micron', which will fix the problem. Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar.","title":"3.  Visualise the aligned reads with Trackster"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-3-cuffdiff-40-min","text":"The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes.","title":"Section 3. Cuffdiff [40 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This step may take a while, depending on how busy the server is.","title":"1.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 43, data 38, and others\". We'll mostly be interested in the file ending with 'gene differential expression testing' which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Cuffdiff_Significant_DE_Genes\".","title":"2.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-4-count-reads-in-features-30-min","text":"HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes.","title":"Section 4. Count reads in features [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-5-edger-30-min","text":"edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models.","title":"Section 5: edgeR  [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-edger","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute","title":"1.  Generate a list of differentially expressed genes using edgeR"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-from-the-previous-step","text":"Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user's guide at Bioconductor .","title":"2.  Examine the outputs from the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-extract-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_edgeR_topTable_edgeR.xls\" With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".","title":"3.  Extract the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-6-deseq2-30-min","text":"DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity.","title":"Section 6. DESeq2 [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-deseq2","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2","title":"1.  Generate a list of differentially expressed genes using DESeq2"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-the-previous-step","text":"Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Examine the outputs the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-filter-out-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Differential_Counts_DESeq2_topTable_DESeq2.xls\" With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see the first few differentially expressed genes are similar to the ones identified by EdgeR.","title":"3.  Filter out the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-7-how-much-concordance-is-there-between-methods","text":"We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like 'Cuffdiff_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like 'edgeR_gene_list' Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like 'DESeq2_gene_list' Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_common_gene_list' Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list' We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools.","title":"Section 7: How much concordance is there between methods?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-8-gene-set-enrichment-analysis","text":"The biological question being asked in the original paper is essentially: \"What is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\" We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As... Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one.","title":"Section 8: Gene set enrichment analysis"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#optional-extension-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ .","title":"Optional extension: Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-load-count-data-into-degust","text":"In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\" generated in Section 4 using the disk icon . Go to vicbioinformatics.com/degust/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts.","title":"1. Load count data into Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-configure-your-uploaded-data","text":"Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data. Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2. Configure your uploaded data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-explore-your-data","text":"Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right.","title":"3. Explore your data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#4-explore-the-demo-data","text":"Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"4. Explore the demo data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#references","text":"[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"References"},{"location":"tutorials/rna_seq_dge_basic/","text":"PR reviewers and advice: Jessica Chung, Clare Sloggett Current slides (Jessica's slides from October 2017): - Overview: https://docs.google.com/presentation/d/1HovpEc5xzB_plxlIpWJAoTpb4309ujfiqOcLKGCDWYw - Workshop: https://docs.google.com/presentation/d/1YmJl8ks7tCg9UOYcjOg1rzr97Qrid5HsS4FqCqxgqq4 Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/","text":"Background Introduction to RNA-seq RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites The Galaxy workflow platform What is Galaxy? Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right. Differential gene expression analysis using Tophat and Cufflinks Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome. The Alternate protocol The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes. Figure 2: General workflow for testing expression differences between two experimental conditions Tophat Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cuffdiff The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences The full Tuxedo Protocol Figure 3: Full Tuxedo protocol workflow Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots. Protocols recommendations Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult. Limitations of the protocols Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques. References Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#introduction-to-rna-seq","text":"RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites","title":"Introduction to RNA-seq"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here","title":"What is Galaxy?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-1-the-galaxy-interface","text":"Tools on the left, data in the middle, analysis workflow on the right.","title":"Figure 1: The Galaxy interface"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#differential-gene-expression-analysis-using-tophat-and-cufflinks","text":"Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.","title":"Differential gene expression analysis using Tophat and Cufflinks"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-alternate-protocol","text":"The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.","title":"The Alternate protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-2-general-workflow-for-testing-expression-differences-between-two-experimental-conditions","text":"","title":"Figure 2: General workflow for testing expression differences between two experimental conditions"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#tophat","text":"Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file)","title":"Tophat"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#cuffdiff","text":"The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences","title":"Cuffdiff"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-full-tuxedo-protocol","text":"","title":"The full Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-3-full-tuxedo-protocol-workflow","text":"Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.","title":"Figure 3: Full Tuxedo protocol workflow"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#protocols-recommendations","text":"Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.","title":"Protocols recommendations"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#limitations-of-the-protocols","text":"Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.","title":"Limitations of the protocols"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"References"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 2px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.extra { color: #444444; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq - Differential Gene Expression Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie Tutorial Overview In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives At the end of this tutorial you should: Be familiar with basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes The data The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditioins, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO. Section 1: Preparation 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_2017 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here . Section 2: Alignment with HISAT2 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Input data format FASTQ Single end or paired reads? Individual paired reads Forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Source for the reference genome to align against: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. This means we can't use the eye icon to view the data in Galaxy; we need to use software that can read the file or convert it into it's plain-text equivalent (SAM) to view it as text. In section 3, we'll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 16046 reads; of these: 16046 (100.00%) were paired; of these: 104 (0.65%) aligned concordantly 0 times 13558 (84.49%) aligned concordantly exactly 1 time 2384 (14.86%) aligned concordantly >1 times ---- 104 pairs aligned concordantly 0 times; of these: 1 (0.96%) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly; of these: 206 mates make up the pairs; of these: 106 (51.46%) aligned 0 times 91 (44.17%) aligned exactly 1 time 9 (4.37%) aligned >1 times 99.67% overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination. Section 3: Visualise the aligned reads The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the BAM files, for example 'WT_01.bam'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select 'KO_02.bam' and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. [Optional] Visualise the aligned reads in Trackster If you have trouble getting IGV to work, you can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select WT_01.bam and KO_01.bam by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:816349-830862 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar. Section 4. Quantification HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples. Section 5. Degust Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. Section 5. DESeq2 In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there? Section 6. The importance of replicates Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. Optional extension Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#rna-seq-differential-gene-expression","text":"Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie","title":"RNA-Seq - Differential Gene Expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#learning-objectives","text":"At the end of this tutorial you should: Be familiar with basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#the-data","text":"The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditioins, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO.","title":"The data"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-1-preparation","text":"1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_2017 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as 'fastqsanger' when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you're unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here .","title":"Section 1: Preparation"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-2-alignment-with-hisat2","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Input data format FASTQ Single end or paired reads? Individual paired reads Forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Source for the reference genome to align against: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam') by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. This means we can't use the eye icon to view the data in Galaxy; we need to use software that can read the file or convert it into it's plain-text equivalent (SAM) to view it as text. In section 3, we'll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \"info\" button (view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\" row under \"Job Information\" in the table. Click the \"stderr\" link to view the alignment summary output. 16046 reads; of these: 16046 (100.00%) were paired; of these: 104 (0.65%) aligned concordantly 0 times 13558 (84.49%) aligned concordantly exactly 1 time 2384 (14.86%) aligned concordantly >1 times ---- 104 pairs aligned concordantly 0 times; of these: 1 (0.96%) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly; of these: 206 mates make up the pairs; of these: 106 (51.46%) aligned 0 times 91 (44.17%) aligned exactly 1 time 9 (4.37%) aligned >1 times 99.67% overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination.","title":"Section 2: Alignment with HISAT2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-3-visualise-the-aligned-reads","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the BAM files, for example 'WT_01.bam'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select 'KO_02.bam' and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can't find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The 'Sox102F' gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. [Optional] Visualise the aligned reads in Trackster If you have trouble getting IGV to work, you can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select WT_01.bam and KO_01.bam by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:816349-830862 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar.","title":"Section 3: Visualise the aligned reads"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-4-quantification","text":"HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select 'Multiple datasets', then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \"(no feature)\") contains the stats for the reads that weren't able to be uniquely aligned to a gene. We don't need the \"(no feature)\" files so we can remove then with the delete \"X\" button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples.","title":"Section 4. Quantification"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \"Upload your counts file\". Click \"Choose file\" and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \"gene_id\". Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \"Show R code\" under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \"Try the demo\" button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"Section 5. Degust"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-deseq2","text":"In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"DESeq2 result file on data ...\" With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there?","title":"Section 5. DESeq2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-6-the-importance-of-replicates","text":"Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene's counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not.","title":"Section 6. The importance of replicates"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#optional-extension","text":"Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called 'batch' and 'chem', and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Optional extension"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Basic Tutorial Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Background Where does the data in this tutorial come from? The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \"Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\" (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions. The Tuxedo Protocol The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here . Section 1: Preparation [15 min] 1. Register as a new user in Galaxy if you don\u2019t already have an account ( what is Galaxy? ) Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as 'fastqsanger' and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 Section 2: Align reads with Tophat [30 mins] In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Align the RNA-seq short reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is. 2. Examine the output files You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. 3. Visualise the aligned reads with IGV The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example 'Tophat on data 1: accepted_hits'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as 'TopHat on data 1: splice junctions'. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select 'TopHat on data 4: accepted_hits' (this is the accepted hits alignment file from first replicate of condition C2) and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. 4. [Optional] Visualise the aligned reads in Trackster We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar. Section 3: Test differential expression with Cuffdiff [45 min] The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here . 1. Examine the reference transcriptome Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites. 2. Run Cuffdiff to identify differentially expressed genes and transcripts In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Explore the Cuffdiff output files There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 37, data 32, and others\". FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM tracking\" by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene differential expression testing\" by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Significant_DE_Genes\". Examine the sorted list of differentially expressed genes. Click on the eye icon next to \"Significant_DE_Genes\" to view the data. How many genes are in the Significant_DE_Genes file? What are their names? Two genes have been identified as differentially expressed between conditions C1 and C2: Ank located at chr4:137014-150378, and CG2177 located at chr4:331557-334534 Both genes have q-values of 0.00175. \"CG2177\" located at chr4:331557-334534 was the gene that we intuitively (with IGV) saw to be differentially expressed in the previous section, in the broader region of chr4:325197-341887. Section 4. Repeat without replicates [20 min] In this section, we will run Cuffdiff with fewer replicates. Stop and think: Why do we need replicates for an RNA-seq differential gene expression experiment? What do you expect to happen if we only use one sample from each condition for our analysis? Repeat the differential gene expression testing from section 2, but this time only use one replicate from each condition group (C1 and C2). From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_C1_R1_vs_C2_R1\" Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R1. You should get no differentially expressed genes at statistical significance of 0.05. The \"Ank\" gene and the \"CG1277\", which were found to be significantly differentially expressed in our first analysis, are not identified as differentially expressed when we only use one sample for each condition. Repeat this no-replicates analysis, but this time specify a different set of samples. From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 5: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_C1_R1_vs_C2_R2\" Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R2. We now see \"CG2177\" appear again in the list as significantly differentially expressed, but not \"Ank\". How can we interpret the difference in results from using different replicates? There is a larger absolute difference in CG1277 expression between samples 1 (C1_R1) and 5 (C2_R2) than samples 1 (C1_R1) and 4 (C2_R1), hence Cuffdiff identifies CG1277 as differentially expressed between C1_R1 and C2_R2, but not between C1_R1 and C2_R1. On the other hand, differences in level of expression of Ank is much smaller between samples, so we need to see it consistently across multiple replicates for Cuffdiff to be confident it actually exists. One replicate is not enough. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. If we say that genes Ank and CG2177 are truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. [Optional step] Repeat this analysis, specifying groups of two replicates each. What do you get? How many replicates do we need to identify Ank as differentially expressed? Section 5. Optional Extension [20 min] Extension on the Tuxedo Protocol The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn't have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_using_Cufflinks_Assembly\" Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177). Transcript-level differential expression One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \"unit of aggregation\" to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation. References Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"Tuxedo Protocol Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#rna-seq-differential-gene-expression-basic-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Basic Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#learning-objectives","text":"At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \"Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\" (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#the-tuxedo-protocol","text":"The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here .","title":"The Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account-what-is-galaxy","text":"Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Melbourne server . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account (what is Galaxy?)"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on the top right and \"start using this history\" to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as 'fastqsanger' and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don't need to specify the type for this file as Galaxy will auto-detect the file as a GTF file.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-view-and-have-an-understanding-of-the-files-involved-in-rna-seq-analysis","text":"You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019","title":"3.  View and have an understanding of the files involved in RNA-seq analysis."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-2-align-reads-with-tophat-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Align reads with Tophat [30 mins]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-align-the-rna-seq-short-reads-to-a-reference-genome","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Align the RNA-seq short reads to a reference genome."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-examine-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history.","title":"2.  Examine the output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-visualise-the-aligned-reads-with-igv","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) -- an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example 'Tophat on data 1: accepted_hits'. Click on Display with IGV 'webcurrent' (or 'local' if you have IGV installed on your computer. You will need to open IGV before you click on 'local'). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as 'TopHat on data 1: splice junctions'. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select 'TopHat on data 4: accepted_hits' (this is the accepted hits alignment file from first replicate of condition C2) and click on 'display with IGV local'. This time we are using the 'local' link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"3.  Visualise the aligned reads with IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#4-optional-visualise-the-aligned-reads-in-trackster","text":"We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking 'Analyze Data' on the top Galaxy toolbar.","title":"4.  [Optional] Visualise the aligned reads in Trackster"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-3-test-differential-expression-with-cuffdiff-45-min","text":"The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed-- this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here .","title":"Section 3: Test differential expression with Cuffdiff [45 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-examine-the-reference-transcriptome","text":"Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites.","title":"1.  Examine the reference transcriptome"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \"Cuffdiff on data 37, data 32, and others\". FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM tracking\" by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene differential expression testing\" by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \"Filter on data x\" to \"Significant_DE_Genes\". Examine the sorted list of differentially expressed genes. Click on the eye icon next to \"Significant_DE_Genes\" to view the data.","title":"3.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-4-repeat-without-replicates-20-min","text":"In this section, we will run Cuffdiff with fewer replicates.","title":"Section 4. Repeat without replicates [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-5-optional-extension-20-min","text":"","title":"Section 5. Optional Extension [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#extension-on-the-tuxedo-protocol","text":"The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn't have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \"Cuffdiff on data....: gene differential expression testing\" With following condition: c14=='yes' Execute Rename the output file to something meaningful like \"Significant_DE_Genes_using_Cufflinks_Assembly\" Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177).","title":"Extension on the Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#transcript-level-differential-expression","text":"One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \"unit of aggregation\" to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation.","title":"Transcript-level differential expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"References"},{"location":"tutorials/rna_seq_exp_design/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/","text":"RNA-Seq Experimental Design What is RNA-seq? RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes. Why is a good experimental design vital? An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you're planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage. Terminology Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth. The importance of replicates to estimate variance When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can't be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis. How many replicates and how many reads do I need? Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions. Sequencing options to consider How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3' end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility. Summary A good experimental design is vital for a successful experiment. If you're planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"RNA-seq DGE Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#rna-seq-experimental-design","text":"","title":"RNA-Seq Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#what-is-rna-seq","text":"RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes.","title":"What is RNA-seq?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#why-is-a-good-experimental-design-vital","text":"An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you're planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage.","title":"Why is a good experimental design vital?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#terminology","text":"Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth.","title":"Terminology"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#the-importance-of-replicates-to-estimate-variance","text":"When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can't be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis.","title":"The importance of replicates to estimate variance"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#how-many-replicates-and-how-many-reads-do-i-need","text":"Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions.","title":"How many replicates and how many reads do I need?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#sequencing-options-to-consider","text":"How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3' end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility.","title":"Sequencing options to consider"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#summary","text":"A good experimental design is vital for a successful experiment. If you're planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"Summary"},{"location":"tutorials/unix/","text":"em {font-style: normal; font-family: courier new;} Introduction to Unix A hands-on-workshop covering the basics of the Unix/Linux command line interface How to use this workshop The workshop is broken up into a number of Topics each focusing on a particular aspect of Unix. You should take a short break between each to refresh and relax before tackling the next. Topic s may start with some background followed by a number of exercises . Each exercise begins with a question , then sometimes a hint (or two) and finishes with the suggested answer . Question An example question looks like: What is the Answer to Life? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Depending on how much of a challenge you like, you may choose to use hints. Even if you work out the answer without hints, its a good idea to read the hints afterwards because they contain extra information that is good to know. Note: hint s may be staged, that is, there may be a more section within a hint for further hints Hint <- click here to reveal hint What is the answer to everything? As featured in \"The Hitchhiker's Guide to the Galaxy\" More <- and here to show more It is probably a two digit number //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); if ($(\"#showable2\").hasClass(\"showable-hidden\")) { $(\"#showablelink2\").text(\"More\"); } else { $(\"#showablelink2\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Once you have worked out the answer to the question expand the Answer section to check if you got it correct. Answer <- click here to reveal answer Answer : 42 Ref: Number 42 (Wikipedia) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Usage Style This workshop attempts to cater for two usage styles: Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional): Attempt to answer the question by yourself. Use hints when you get stuck. Once solved, reveal the answer and read through our suggested solution. Its a good idea to read the hints and answer description as they often contain extra useful information. By example : for those who learn by following examples: Expand all sections Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the same (or similar) answers. Its a good idea to read the hints and answer description as they often contain extra useful information. Topic 1: Remote log in In this topic we will learn how to connect to a Unix computer via a method called SSH and run a few basic commands. Connecting to a Unix computer To begin this workshop you will need to connect to an HPC. Today we will use the LIMS-HPC. The computer called lims-hpc-m (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks. Server details : host : lims-hpc-m.latrobe.edu.au port : 6022 username : trainingXX (where XX is a two digit number, provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into LIMS-HPC, for example, type this command at the prompt and press return (where the word username is replaced with your LIMS-HPC username): ssh -p 6022 username@lims-hpc-m.latrobe.edu.au The same procedure works for any other machine where you have an account except most other HPCs will not need the -p 6022 (which is telling ssh to connect on a non-standard port number). You may be presented with a message along the lines of: The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type yes and then press enter . If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html Documentation for using PuTTY is here: http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html When you start PuTTY you should see a window which looks something like this: To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and 6022 in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text \"login as:\" . An example terminal window is shown below. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. LIMS-HPC is a high performance computer for La Trobe Users. Logging in connects your local computer (e.g. laptop) to LIMS-HPC, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on LIMS-HPC for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of LIMS-HPC, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into LIMS-HPC over ssh. Exercises 1.1) When you\u2019ve logged into LIMS-HPC run the following commands and see what they do: who whoami date cal hostname /home/group/common/training/Intro_to_Unix/hi //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer who : displays a list of the users who are currently using this Unix computer. whoami : displays your username (i.e. they person currently logged in). date : displays the current date and time. cal : displays a calendar on the terminal. It can be configured to display more than just the current month. hostname : displays the name of the computer we are logged in to. /home/group/common/training/Intro_to_Unix/hi : displays the text \"Hello World\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink7\").click(function(e){ e.preventDefault(); $(\"#showable7\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 2: Exploring your home directory In this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer. 2.1) Use the ls command to list the files in your home directory. How many files are there? Hint Literally, type ls and press the ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls exp01 file01 muscle.fq When running the ls command with no options it will list files in your current working directory. The place where you start when you first login is your HOME directory. Answer : 3 (exp01, file01 and muscle.fq) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink10\").click(function(e){ e.preventDefault(); $(\"#showable10\").toggleClass(\"showable-hidden\"); }); }); //-->]]> The above answer is not quite correct. There are a number of hidden files in your home directory as well. 2.2) What flag might you use to display all files with the ls command? How many files are really there? Hint Take the all quite literally. More Type ls --all and press the ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); if ($(\"#showable13\").hasClass(\"showable-hidden\")) { $(\"#showablelink13\").text(\"More\"); } else { $(\"#showablelink13\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer 1 : --all (or -a ) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely . and .. which are not real files but instead, shortcuts. . is a shortcut for the current directory and .. a shortcut for the directory above the current one. Answer 2 : 10 files (don't count . and .. ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink14\").click(function(e){ e.preventDefault(); $(\"#showable14\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.3) What is the full path name of your home directory? Hint Remember your Current Working Directory start's in your home directory (and the hint from the slides). More Try a shortened version of print working directory //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink17\").click(function(e){ e.preventDefault(); $(\"#showable17\").toggleClass(\"showable-hidden\"); if ($(\"#showable17\").hasClass(\"showable-hidden\")) { $(\"#showablelink17\").text(\"More\"); } else { $(\"#showablelink17\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: $ pwd /home/trainingXY Answer : /home/trainingXY where XY is replaced by some 2 digit sequence. Alternate method : You can also find out the name of your home directory by printing the value of the $HOME shell variable: echo $HOME //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.4) Run ls using the long flag ( -l ), how did the output change? Hint Run ls -l //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : it changed the output to place 1 file/directory per line. It also added some extra information about each. $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11:28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq Details : drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 \\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/ permission | username group size date name /---^---\\ linkcount Where: permissions : 4 parts, file type, user perms, group perms and other perms filetype : 1 character, d = directory and - regular file user permissions: 3 characters, r = read, w = write, x = execute and - no permission group permissions: same as user except for users within the owner group other permissions: same as user except for users that are not in either user or group username : the user who owns this file/directory group : the group name who owns this file/directory size : the number of bytes this file/directory takes to store on disk date : the date and time when this file/directory was last edited name : name of the file linkcount : technical detail which represents the number of links this file has in the file system (safe to ignore) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.5) What type of file is exp01 and muscle.fq ? Hint Check the output from the ls -l . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink23\").click(function(e){ e.preventDefault(); $(\"#showable23\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : exp01 : Directory (given the 'd' as the first letter of its permissions) muscle.fq : Regular File (given the '-') //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.6) Who has permission to read , write and execute your home directory? Hint You can also give ls a filename as the first option. More ls -l will show you the contents of the CWD ; how might you see the contents of the parent directory? (remember the slides) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink27\").click(function(e){ e.preventDefault(); $(\"#showable27\").toggleClass(\"showable-hidden\"); if ($(\"#showable27\").hasClass(\"showable-hidden\")) { $(\"#showablelink27\").text(\"More\"); } else { $(\"#showablelink27\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer If you pass the -l flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. Method 1 : given we know the CWD is our home directory. $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... The .. refers to the parent directory. Method 2 : using $HOME. This works no matter what our CWD is set to. You could list the permissions of all files and directories in the parent directory of your home: $ ls -l $HOME/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... In this case we use the shell variable to refer to our home directory. Method 3 : using ~ (tilde) shortcut You may also refer to your home directory using the ~ (tilde) character: $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like trainingXY . Where XY is replaced by a two digit string Altername : using the -a flag and looking at the . (dot) special file. $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 . ... Answer : drwxr-x--- You : read (see filenames), write (add, delete files), execute (change your CWD to this directory). Training users : read, execute Everyone else : No access Discussion on Permissions : The permission string is \"drwxr-x---\" . The d means it is a directory. The rwx means that the owner of the directory (your user account) can read , write and execute the directory. Execute permissions on a directory means that you can cd into the directory. The r-x means that anyone in the same user group as training can read or execute the directory. The --- means that nobody else (other users on the system) can do anything with the directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q . 2.7) Use the man command to find out what the -h flag does for ls Hint Give ls as an option to man command. More man ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); if ($(\"#showable31\").hasClass(\"showable-hidden\")) { $(\"#showablelink31\").text(\"More\"); } else { $(\"#showablelink31\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink30\").click(function(e){ e.preventDefault(); $(\"#showable30\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the following command to view the man page for ls : $ man ls Answer : You should discover that the -h option prints file sizes in human readable format -h, --human-readable with -l, print sizes in human readable format (e.g., 1K 234M 2G) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.8) Use the -h , how did the output change of muscle.fq ? Hint Don't forget the -l option too. More Run ls -lh //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink35\").click(function(e){ e.preventDefault(); $(\"#showable35\").toggleClass(\"showable-hidden\"); if ($(\"#showable35\").hasClass(\"showable-hidden\")) { $(\"#showablelink35\").text(\"More\"); } else { $(\"#showablelink35\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink34\").click(function(e){ e.preventDefault(); $(\"#showable34\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls -lh ... -rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq Answer : it changed the output so the filesize of muscle.fq is now 2.5K instead of 2461 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink36\").click(function(e){ e.preventDefault(); $(\"#showable36\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 3: Exploring the file system In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file 3.1) Print the value of your current working directory. Answer The pwd command prints the value of your current working directory. $ pwd /home/training01 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink38\").click(function(e){ e.preventDefault(); $(\"#showable38\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.2) List the contents of the root directory, called ' / ' (forward slash). Hint ls expects a single option which is the directory to change too. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink40\").click(function(e){ e.preventDefault(); $(\"#showable40\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that ls can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink41\").click(function(e){ e.preventDefault(); $(\"#showable41\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.3) Use the cd command to change your working directory to the root directory. Did your prompt change? Hint cd expects a single option which is the directory to change to //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink43\").click(function(e){ e.preventDefault(); $(\"#showable43\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The cd command changes the value of your current working directory. To change to the root directory use the following command: $ cd / Answer : Yes, it now says the CWD is / instead of ~ . Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink44\").click(function(e){ e.preventDefault(); $(\"#showable44\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen. 3.4) List the contents of the CWD and verify it matches the list in 3.2 Hint ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink46\").click(function(e){ e.preventDefault(); $(\"#showable46\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Assuming you have changed to the root directory then this can be achieved with ls , or ls -a (for all files) or ls -la for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Answer : Yes, we got the same output as exercise 3.2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink47\").click(function(e){ e.preventDefault(); $(\"#showable47\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system? Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory More $HOME , ~ , /home/trainingXY are all methods to name your home directory. Yet there is a simpler method; the answer is buried in man cd however cd doesn't its own manpage so you will need to search for it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink50\").click(function(e){ e.preventDefault(); $(\"#showable50\").toggleClass(\"showable-hidden\"); if ($(\"#showable50\").hasClass(\"showable-hidden\")) { $(\"#showablelink50\").text(\"More\"); } else { $(\"#showablelink50\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink49\").click(function(e){ e.preventDefault(); $(\"#showable49\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the cd command to change your working directory to your home directory. There are a number of ways to refer to your home directory: cd $HOME is equivalent to: cd ~ The simplest way to change your current working directory to your home directory is to run the cd command with no arguments: Answer : the simplest for is cd with NO options. cd This is a special-case behaviour which is built into cd for convenience. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink51\").click(function(e){ e.preventDefault(); $(\"#showable51\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.6) Change your working directory to /home/group/common/training/Intro_to_Unix/ Answer cd /home/group/common/training/Intro_to_Unix/ //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink53\").click(function(e){ e.preventDefault(); $(\"#showable53\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.7) List the contents of that directory. How many files does it contain? Hint ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink55\").click(function(e){ e.preventDefault(); $(\"#showable55\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can do this with ls $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Answer : 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink56\").click(function(e){ e.preventDefault(); $(\"#showable56\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.8) What kind of file is /home/group/common/training/Intro_to_Unix/sleepy ? Hint Take the word file quite literally. More file sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink59\").click(function(e){ e.preventDefault(); $(\"#showable59\").toggleClass(\"showable-hidden\"); if ($(\"#showable59\").hasClass(\"showable-hidden\")) { $(\"#showablelink59\").text(\"More\"); } else { $(\"#showablelink59\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink58\").click(function(e){ e.preventDefault(); $(\"#showable58\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the file command to get extra information about the contents of a file: Assuming your current working directory is /home/group/common/training/Intro_to_Unix/ $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: $ file /home/group/common/training/Intro_to_Unix/sleepy Bourne-Again shell script text executable Answer : Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The file command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: man file //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink60\").click(function(e){ e.preventDefault(); $(\"#showable60\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.9) What kind of file is /home/group/common/training/Intro_to_Unix/hi ? Hint Take the word file quite literally. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink62\").click(function(e){ e.preventDefault(); $(\"#showable62\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the file command again. If you are in the same directory as hi then: $ file hi ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.9, not stripped Answer : ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called hi contains a binary executable program (raw instructions that the computer can execute directly). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink63\").click(function(e){ e.preventDefault(); $(\"#showable63\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.10) What are the file permissions of /home/group/common/training/Intro_to_Unix/sleepy ? What do they mean? Hint Remember the ls command, and don't forget the -l flag //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink65\").click(function(e){ e.preventDefault(); $(\"#showable65\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can find the permissions of sleepy using the ls command with the -l flag. If you are in the same directory as sleepy then: $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16:36 sleepy Answer : We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and writeable only to training01. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the ln command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink66\").click(function(e){ e.preventDefault(); $(\"#showable66\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.11) Change your working directory back to your home directory ready for the next topic. Hint cd //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink68\").click(function(e){ e.preventDefault(); $(\"#showable68\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You should know how to do this with the cd command: cd //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink69\").click(function(e){ e.preventDefault(); $(\"#showable69\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 4: Working with files and directories In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections. 4.1) In your home directory make a sub-directory called test. Hint You are trying to make a directory , which of the above commands looks like a shortened version of this? More mkdir //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink72\").click(function(e){ e.preventDefault(); $(\"#showable72\").toggleClass(\"showable-hidden\"); if ($(\"#showable72\").hasClass(\"showable-hidden\")) { $(\"#showablelink72\").text(\"More\"); } else { $(\"#showablelink72\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink71\").click(function(e){ e.preventDefault(); $(\"#showable71\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Make sure you are in your home directory first. If not cd to your home directory. Use the mkdir command to make new directories: $ mkdir test Use the ls command to check that the new directory was created. $ ls exp01 file01 muscle.fq test //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink73\").click(function(e){ e.preventDefault(); $(\"#showable73\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.2) Copy all the files from /home/group/common/training/Intro_to_Unix/ into the newly created test directory. Hint You are trying to copy , which of the above commands looks like a shortened version of this? More $ man cp ... SYNOPSIS cp [OPTION]... [-T] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. which means cp expects zero or more flags, a SOURCE file followed by a DEST file or directory //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink76\").click(function(e){ e.preventDefault(); $(\"#showable76\").toggleClass(\"showable-hidden\"); if ($(\"#showable76\").hasClass(\"showable-hidden\")) { $(\"#showablelink76\").text(\"More\"); } else { $(\"#showablelink76\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink75\").click(function(e){ e.preventDefault(); $(\"#showable75\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the cp command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to cp . You only need to perform one of these ways, but we show multiple ones for your reference. Answer 1 : From your home directory: $ cp /home/group/common/training/Intro_to_Unix/* test Answer 2 : Change to the test directory and then copy (assuming you started in your home directory): $ cd test $ cp /home/group/common/training/Intro_to_Unix/* . In the example above the ' . ' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. Answer 3 : Change to the /home/group/common/training/Intro_to_Unix/ directory and then copy: cd /home/group/common/training/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink77\").click(function(e){ e.preventDefault(); $(\"#showable77\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : This exercise assumes that the copy command from the previous exercise was successful. 4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to. Hint Remember ls can show you the file size (with one of its flags) More ls -l //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink80\").click(function(e){ e.preventDefault(); $(\"#showable80\").toggleClass(\"showable-hidden\"); if ($(\"#showable80\").hasClass(\"showable-hidden\")) { $(\"#showablelink80\").text(\"More\"); } else { $(\"#showablelink80\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink79\").click(function(e){ e.preventDefault(); $(\"#showable79\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use ls -l to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: $ ls -l /home/group/common/training/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. Answer : They should each be 1033773 bytes Alternate : Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the -h option for ls: $ ls -lh /home/group/common/training/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /home/group/common/training/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as 1010K . Larger files are reported in megabytes, gigabytes etcetera. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink81\").click(function(e){ e.preventDefault(); $(\"#showable81\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test 4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to. Hint What is the opposite of same ? More diff erence //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink84\").click(function(e){ e.preventDefault(); $(\"#showable84\").toggleClass(\"showable-hidden\"); if ($(\"#showable84\").hasClass(\"showable-hidden\")) { $(\"#showablelink84\").text(\"More\"); } else { $(\"#showablelink84\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink83\").click(function(e){ e.preventDefault(); $(\"#showable83\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the diff command to compare the contents of two files. $ diff /home/group/common/training/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the diff command will NOT produce any output) Answer : Yes, they are the same since no output was given. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink85\").click(function(e){ e.preventDefault(); $(\"#showable85\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.5) How many lines, words and characters are in expectations.txt? Hint Initialisms are key More w ord c ount //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink88\").click(function(e){ e.preventDefault(); $(\"#showable88\").toggleClass(\"showable-hidden\"); if ($(\"#showable88\").hasClass(\"showable-hidden\")) { $(\"#showablelink88\").text(\"More\"); } else { $(\"#showablelink88\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink87\").click(function(e){ e.preventDefault(); $(\"#showable87\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the wc (for \"word count\") to count the number of characters, lines and words in a file: $ wc expectations.txt 20415 187465 1033773 expectations.txt Answer : There are 20415 lines, 187465 words and 1033773 characters in expectations.txt. To get just the line, word or character count: $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink89\").click(function(e){ e.preventDefault(); $(\"#showable89\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano . Hint nano FILENAME Once nano is open it displays some command hints along the bottom of the screen. More ^O means hold the Control (or CTRL) key while pressing the o . Dispite what it displays, you need to type the lower-case letter that follows the ^ character. WriteOut is another name for Save. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink92\").click(function(e){ e.preventDefault(); $(\"#showable92\").toggleClass(\"showable-hidden\"); if ($(\"#showable92\").hasClass(\"showable-hidden\")) { $(\"#showablelink92\").text(\"More\"); } else { $(\"#showablelink92\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink91\").click(function(e){ e.preventDefault(); $(\"#showable91\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Take some time to play around with the nano text editor. Nano is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as vim and emacs , however they take a substantial amount of time to learn. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink93\").click(function(e){ e.preventDefault(); $(\"#showable93\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.7) Did the changes you made to ~/test/expectations.txt have any effect on /home/group/common/training/Intro_to_Unix/expectations.txt ? How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 More Use diff //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink96\").click(function(e){ e.preventDefault(); $(\"#showable96\").toggleClass(\"showable-hidden\"); if ($(\"#showable96\").hasClass(\"showable-hidden\")) { $(\"#showablelink96\").text(\"More\"); } else { $(\"#showablelink96\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink95\").click(function(e){ e.preventDefault(); $(\"#showable95\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use diff to check that the two files are different after you have made the change to the copy of expectations.txt in your ~/test directory. diff ~/test/expectations.txt \\ /home/group/common/training/Intro_to_Unix/expectations.txt You could also use ls to check that the files have different sizes. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink97\").click(function(e){ e.preventDefault(); $(\"#showable97\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.8) In your test subdirectory, rename expectations.txt to foo.txt . Hint Another way to think of it is moving it from expectations.txt to foo.txt More mv Use man mv if you need to work out how to use it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink100\").click(function(e){ e.preventDefault(); $(\"#showable100\").toggleClass(\"showable-hidden\"); if ($(\"#showable100\").hasClass(\"showable-hidden\")) { $(\"#showablelink100\").text(\"More\"); } else { $(\"#showablelink100\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink99\").click(function(e){ e.preventDefault(); $(\"#showable99\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the mv command to rename the file: $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink101\").click(function(e){ e.preventDefault(); $(\"#showable101\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.9) Rename foo.txt back to expectations.txt. Answer Use the mv command to rename the file: $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use ls to check that the file is in fact renamed. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink103\").click(function(e){ e.preventDefault(); $(\"#showable103\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.10) Remove the file expectations.txt from your test directory. Hint We are trying to remove a file, check the commands at the top of this topic. More rm //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink106\").click(function(e){ e.preventDefault(); $(\"#showable106\").toggleClass(\"showable-hidden\"); if ($(\"#showable106\").hasClass(\"showable-hidden\")) { $(\"#showablelink106\").text(\"More\"); } else { $(\"#showablelink106\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink105\").click(function(e){ e.preventDefault(); $(\"#showable105\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the rm command to remove files (carefully): $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink107\").click(function(e){ e.preventDefault(); $(\"#showable107\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.11) Remove the entire test directory and all the files within it. Hint We are trying to remove a directory . More You could use rmdir but there is an easier way using just rm and a flag. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink110\").click(function(e){ e.preventDefault(); $(\"#showable110\").toggleClass(\"showable-hidden\"); if ($(\"#showable110\").hasClass(\"showable-hidden\")) { $(\"#showablelink110\").text(\"More\"); } else { $(\"#showablelink110\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink109\").click(function(e){ e.preventDefault(); $(\"#showable109\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You could use the rm command to remove each file individually, and then use the rmdir command to remove the directory. Note that rmdir will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the -r (for recursive) flag to rm to remove all the files and the directory in one go: Logical Answer : cd ~ rm test/* rmdir test Easier Answer : cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink111\").click(function(e){ e.preventDefault(); $(\"#showable111\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.12) Recreate the test directory in your home directory and copy all the files from /home/group/common/training/Intro_to_Unix/ back into the test directory. Hint See exercises 4.1 and 4.2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink113\").click(function(e){ e.preventDefault(); $(\"#showable113\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Repeat exercises 4.1 and 4.2. $ cd ~ $ mkdir test $ cp /home/group/common/training/Intro_to_Unix/* test //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink114\").click(function(e){ e.preventDefault(); $(\"#showable114\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c Hint Use man if you can't guess how it might work. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink116\").click(function(e){ e.preventDefault(); $(\"#showable116\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ cd ~/test $ cat hello.c #include <stdio.h> int main(void) { printf (\"Hello World\\n\"); return 0; } hello.c contains the source code of a C program. The compiled executable version of this code is in the file called hi , which you can run like so: $ ./hi Hello World //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink117\").click(function(e){ e.preventDefault(); $(\"#showable117\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.14) Use the head command to view the first 20 lines of the file sample_1.fastq Hint Remember your best friend! More Use man to find out what option you need to add to display a given number of lines . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink120\").click(function(e){ e.preventDefault(); $(\"#showable120\").toggleClass(\"showable-hidden\"); if ($(\"#showable120\").hasClass(\"showable-hidden\")) { $(\"#showablelink120\").text(\"More\"); } else { $(\"#showablelink120\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink119\").click(function(e){ e.preventDefault(); $(\"#showable119\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa`a```^aaaaa`_]aaa`aaa__a_X]`` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink121\").click(function(e){ e.preventDefault(); $(\"#showable121\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq Hint Its very much like head . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink123\").click(function(e){ e.preventDefault(); $(\"#showable123\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab]abbaa`babaaabbb`bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 `aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__ //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink124\").click(function(e){ e.preventDefault(); $(\"#showable124\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.16) Use the grep command to find out all the lines in moby.txt that contain the word \"Ahab\" Hint One might say we are 'looking for the pattern \"Ahab\"' More $ man grep ... SYNOPSIS grep [OPTIONS] PATTERN [FILE...] ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink127\").click(function(e){ e.preventDefault(); $(\"#showable127\").toggleClass(\"showable-hidden\"); if ($(\"#showable127\").hasClass(\"showable-hidden\")) { $(\"#showablelink127\").text(\"More\"); } else { $(\"#showablelink127\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink126\").click(function(e){ e.preventDefault(); $(\"#showable126\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the wc -l command: $ grep Ahab moby.txt | wc -l 491 which shows that there are 491 lines in moby.txt that contain the word Ahab. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink128\").click(function(e){ e.preventDefault(); $(\"#showable128\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera) . Hint One might say we are ignoring case . More $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. (-i is specified by POSIX.) ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink131\").click(function(e){ e.preventDefault(); $(\"#showable131\").toggleClass(\"showable-hidden\"); if ($(\"#showable131\").hasClass(\"showable-hidden\")) { $(\"#showablelink131\").text(\"More\"); } else { $(\"#showablelink131\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink130\").click(function(e){ e.preventDefault(); $(\"#showable130\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the -i flag to grep to make it perform case insensitive search: $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [Project Gutenberg Editor's Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to wc -l to count the number of lines: $ grep -i the expectations.txt | wc -l 8165 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink132\").click(function(e){ e.preventDefault(); $(\"#showable132\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.18) Use the gzip command to compress the file sample_1.fastq . Use gunzip to decompress it back to the original contents. Hint Use the above commands along with man and ls to see what happens to the file. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink134\").click(function(e){ e.preventDefault(); $(\"#showable134\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Check the file size of sample_1.fastq before compressing it: # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq You will see that when it was compressed it is 26997595 bytes in size, making it about 0.3 times the size of the original file. Note : in the above section the lines starting with # are comments so don't need to be copied but if you do then they wont do anything. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink135\").click(function(e){ e.preventDefault(); $(\"#showable135\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Topic 5: Pipes, output redirection and shell scripts In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \"tip of the iceberg\". Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut 5.1) How many reads are contained in the file sample_1.fastq ? Hint Examine some of the file to work out how many lines each read takes up. More Count the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink138\").click(function(e){ e.preventDefault(); $(\"#showable138\").toggleClass(\"showable-hidden\"); if ($(\"#showable138\").hasClass(\"showable-hidden\")) { $(\"#showablelink138\").text(\"More\"); } else { $(\"#showablelink138\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink137\").click(function(e){ e.preventDefault(); $(\"#showable137\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer We can answer this question by counting the number of lines in the file and dividing by 4: $ wc -l sample_1.fastq 3000000 Answer : There are 3000000 lines in the file representing 750000 reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called bc : $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. bc is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink139\").click(function(e){ e.preventDefault(); $(\"#showable139\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.2) How many reads in sample_1.fastq contain the sequence GATTACA ? Hint Check out exercise 4.16 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink141\").click(function(e){ e.preventDefault(); $(\"#showable141\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use grep to find all the lines that contain GATTACA and \"pipe\" the output to wc -l to count them: $ grep GATTACA sample_1.fastq | wc -l 1119 Answer : 1119 If you are unsure about the possibility of upper and lower case characters then consider using the -i (ignore case option for grep). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink142\").click(function(e){ e.preventDefault(); $(\"#showable142\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.3) On what line numbers do the sequences containing GATTACA occur? Hint We are looking for the line numbers . More Check out the manpage for grep and/or nl //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink145\").click(function(e){ e.preventDefault(); $(\"#showable145\").toggleClass(\"showable-hidden\"); if ($(\"#showable145\").hasClass(\"showable-hidden\")) { $(\"#showablelink145\").text(\"More\"); } else { $(\"#showablelink145\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink144\").click(function(e){ e.preventDefault(); $(\"#showable144\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can use the -n flag to grep to make it prefix each line with a line number: Answer 1 : $ grep -n GATTACA sample_1.fastq 5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... Answer 2 : Or you can use the nl command to number each line of sample_1.fastq and then search for GATTACA in the numbered lines: $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... Just the line numbers : If you just want to see the line numbers then you can \"pipe\" the output of the above command into cut -f 1 : $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... cut will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the -f 1 option) $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink146\").click(function(e){ e.preventDefault(); $(\"#showable146\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning. Hint Check answer to 5.3. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink148\").click(function(e){ e.preventDefault(); $(\"#showable148\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with control-c (hold the control key down and simultaneously press the \" c \" character). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink149\").click(function(e){ e.preventDefault(); $(\"#showable149\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl . Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and ' / ' to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, > FILENAME is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink151\").click(function(e){ e.preventDefault(); $(\"#showable151\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \" > \" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the head command like so: $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... The less command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the ' / ' followed by the search term. You can quit by pressing \" q \". Note that the less command is used by default to display man pages. $ less sample_1.fastq.nl //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink152\").click(function(e){ e.preventDefault(); $(\"#showable152\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq.nl into TSV format: $ cat sample_1.fastq | paste - - - - > sample_1.tsv //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink153\").click(function(e){ e.preventDefault(); $(\"#showable153\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The '-' (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all wil support it. The paste command is useful for merging multiple files together line-by-line, such that the Nth line from each file is joined together into one line in the output, separated by default with a tab character. In the above example we give paste 4 copies of the contents of sample_1.fastq , which causes it to join consecutive groups of 4 lines from the file into one line of output. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink154\").click(function(e){ e.preventDefault(); $(\"#showable154\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.7) Do you expect the output of the following command to produce the same output as above? and why? $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink155\").click(function(e){ e.preventDefault(); $(\"#showable155\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use less to examine it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink156\").click(function(e){ e.preventDefault(); $(\"#showable156\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : No, in the second instance we get 4 copies of each line. Why : In the first command paste will use the input file (standard input) 4 times since the cat command will only give one copy of the file to paste , where as, in the second command paste will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink157\").click(function(e){ e.preventDefault(); $(\"#showable157\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file. Hint Remember the wc command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink159\").click(function(e){ e.preventDefault(); $(\"#showable159\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer We can count the number of lines in sample_1.tsv using wc : $ wc -l sample_1.tsv The output should be 750000 as expected (1/4 of the number of lines in sample_1.fastq). To view the first 20 lines of sample_1.tsv use the head command: $ head -20 sample_1.tsv //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink160\").click(function(e){ e.preventDefault(); $(\"#showable160\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.9) Use the cut command to print out the second column of sample_1.tsv . Redirect the output to a file called sample_1.dna.txt . Hint See exercise 5.3 (for cut) and 5.5 (redirection) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink162\").click(function(e){ e.preventDefault(); $(\"#showable162\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using head or less . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink163\").click(function(e){ e.preventDefault(); $(\"#showable163\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt . Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As. Hint Use man (sort) and see exercise 5.5 (redirection) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink165\").click(function(e){ e.preventDefault(); $(\"#showable165\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running head on the output file reveals that there are duplicate DNA sequences in the input FASTQ file. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink166\").click(function(e){ e.preventDefault(); $(\"#showable166\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt , redirect the result to sample_1.dna.uniq.txt . Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt . Hint I am pretty sure you have already used man (or just guessed how to use uniq ). You're also a gun at redirection now. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink168\").click(function(e){ e.preventDefault(); $(\"#showable168\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of sample_1.dna.uniq.txt to check that the duplicate DNA sequences have been removed. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink169\").click(function(e){ e.preventDefault(); $(\"#showable169\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt ? Hint Checkout the uniq manpage //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink171\").click(function(e){ e.preventDefault(); $(\"#showable171\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Look at the man page for uniq. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink172\").click(function(e){ e.preventDefault(); $(\"#showable172\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the -d flag to uniq to print out only the duplicated lines from the file: $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink173\").click(function(e){ e.preventDefault(); $(\"#showable173\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq. Hint That is, piping most of the commands you used above instead of redirecting to file More I.e. 6 commands ( cat , paste , cut , sort , uniq , wc ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink176\").click(function(e){ e.preventDefault(); $(\"#showable176\").toggleClass(\"showable-hidden\"); if ($(\"#showable176\").hasClass(\"showable-hidden\")) { $(\"#showablelink176\").text(\"More\"); } else { $(\"#showablelink176\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink175\").click(function(e){ e.preventDefault(); $(\"#showable175\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: Answer : $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have 56079 lines. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink177\").click(function(e){ e.preventDefault(); $(\"#showable177\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq. Hint Check out the sleepy file (with cat or nano ); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out chmod command). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink179\").click(function(e){ e.preventDefault(); $(\"#showable179\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Put the answer to 5.13 into a file called sample_1_dups.sh (or whatever you want). Use nano to create the file. Answer : the contents of the file will look like this: #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: $ chmod +x sample_1_dups.sh You can run the script like so: $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink180\").click(function(e){ e.preventDefault(); $(\"#showable180\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter. Hint Shell scripts can refer to command line arguments by their position using special variables called $0 , $1 , $2 and so on. More $0 refers to the name of the script as it was called on the command line. $1 refers to the first command line argument, and so on. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink183\").click(function(e){ e.preventDefault(); $(\"#showable183\").toggleClass(\"showable-hidden\"); if ($(\"#showable183\").hasClass(\"showable-hidden\")) { $(\"#showablelink183\").text(\"More\"); } else { $(\"#showablelink183\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink182\").click(function(e){ e.preventDefault(); $(\"#showable182\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Copy the shell script from 5.14 into a new file: $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: $ ./fastq_dups.sh sample_1.fastq In the above example the script takes sample_1.fastq as input and prints the number of duplicated sequences as output. A better Answer : Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: #!/bin/bash if [ $# -eq 1 ]; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The ' if ...; then ' line means: do the following line(s) ONLY if the ... (called condition) bit is true. The ' else ' line means: otherwise do the following line(s) instead. Note: it is optional. The ' fi ' line means: this marks the end of the current if or else section. The ' [ $# -eq 1 ] ' part is the condition: $# : is a special shell variable that indicates how many command line arguments were given. -eq : checks if the numbers on either side if it are equal. 1 : is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink184\").click(function(e){ e.preventDefault(); $(\"#showable184\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file. Answer We can add a loop to our script to accept multiple input FASTQ files: #!/bin/bash for file in $@; do dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l) echo \"$file $dups\" done There's a lot going on in this script. The $@ is a sequence of all command line arguments. The ' for ...; do ' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called file . The $(...) allow us to capture the output of another command (in-place of the ... ). In this case we capture the output of the pipeline and save it to the variable called dups . If you had multiple FASTQ files available you could run the script like so: ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink186\").click(function(e){ e.preventDefault(); $(\"#showable186\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Finished Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be confortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this Workshop for later reference. Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it back to the Workshop facilitators.","title":"Introduction to Unix"},{"location":"tutorials/unix/#introduction-to-unix","text":"A hands-on-workshop covering the basics of the Unix/Linux command line interface","title":"Introduction to Unix"},{"location":"tutorials/unix/#how-to-use-this-workshop","text":"The workshop is broken up into a number of Topics each focusing on a particular aspect of Unix. You should take a short break between each to refresh and relax before tackling the next. Topic s may start with some background followed by a number of exercises . Each exercise begins with a question , then sometimes a hint (or two) and finishes with the suggested answer .","title":"How to use this workshop"},{"location":"tutorials/unix/#question","text":"An example question looks like: What is the Answer to Life? //<![CDATA[<!-- (function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]>","title":"Question"},{"location":"tutorials/unix/#hint","text":"Depending on how much of a challenge you like, you may choose to use hints. Even if you work out the answer without hints, its a good idea to read the hints afterwards because they contain extra information that is good to know. Note: hint s may be staged, that is, there may be a more section within a hint for further hints Hint <- click here to reveal hint What is the answer to everything? As featured in \"The Hitchhiker's Guide to the Galaxy\" More <- and here to show more It is probably a two digit number //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); if ($(\"#showable2\").hasClass(\"showable-hidden\")) { $(\"#showablelink2\").text(\"More\"); } else { $(\"#showablelink2\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Hint"},{"location":"tutorials/unix/#answer","text":"Once you have worked out the answer to the question expand the Answer section to check if you got it correct. Answer <- click here to reveal answer Answer : 42 Ref: Number 42 (Wikipedia) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink3\").click(function(e){ e.preventDefault(); $(\"#showable3\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Answer"},{"location":"tutorials/unix/#usage-style","text":"This workshop attempts to cater for two usage styles: Problem solver : for those who like a challenge and learn best be trying to solve the problems by-them-selves (hints optional): Attempt to answer the question by yourself. Use hints when you get stuck. Once solved, reveal the answer and read through our suggested solution. Its a good idea to read the hints and answer description as they often contain extra useful information. By example : for those who learn by following examples: Expand all sections Expand the Answer section at the start of each question and follow along with the commands that are shown and check you get the same (or similar) answers. Its a good idea to read the hints and answer description as they often contain extra useful information.","title":"Usage Style"},{"location":"tutorials/unix/#topic-1-remote-log-in","text":"In this topic we will learn how to connect to a Unix computer via a method called SSH and run a few basic commands.","title":"Topic 1: Remote log in"},{"location":"tutorials/unix/#connecting-to-a-unix-computer","text":"To begin this workshop you will need to connect to an HPC. Today we will use the LIMS-HPC. The computer called lims-hpc-m (m is for master which is another name for head node) is the one that coordinates all the HPCs tasks. Server details : host : lims-hpc-m.latrobe.edu.au port : 6022 username : trainingXX (where XX is a two digit number, provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into LIMS-HPC, for example, type this command at the prompt and press return (where the word username is replaced with your LIMS-HPC username): ssh -p 6022 username@lims-hpc-m.latrobe.edu.au The same procedure works for any other machine where you have an account except most other HPCs will not need the -p 6022 (which is telling ssh to connect on a non-standard port number). You may be presented with a message along the lines of: The authenticity of host 'lims-hpc-m.latrobe.edu.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type yes and then press enter . If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html Documentation for using PuTTY is here: http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html When you start PuTTY you should see a window which looks something like this: To connect to LIMS-HPC you should enter its hostname into the box entitled \"Host Name (or IP address)\" and 6022 in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of \"The server's host key is not cached in the registry\" . This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text \"login as:\" . An example terminal window is shown below. You should type your LIMS-HPC username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); $(\"#showable5\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. LIMS-HPC is a high performance computer for La Trobe Users. Logging in connects your local computer (e.g. laptop) to LIMS-HPC, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on LIMS-HPC for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of LIMS-HPC, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into LIMS-HPC over ssh.","title":"Connecting to a Unix computer"},{"location":"tutorials/unix/#exercises","text":"1.1) When you\u2019ve logged into LIMS-HPC run the following commands and see what they do: who whoami date cal hostname /home/group/common/training/Intro_to_Unix/hi //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer who : displays a list of the users who are currently using this Unix computer. whoami : displays your username (i.e. they person currently logged in). date : displays the current date and time. cal : displays a calendar on the terminal. It can be configured to display more than just the current month. hostname : displays the name of the computer we are logged in to. /home/group/common/training/Intro_to_Unix/hi : displays the text \"Hello World\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink7\").click(function(e){ e.preventDefault(); $(\"#showable7\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Exercises"},{"location":"tutorials/unix/#topic-2-exploring-your-home-directory","text":"In this topic we will learn how to \"look\" at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer. 2.1) Use the ls command to list the files in your home directory. How many files are there? Hint Literally, type ls and press the ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls exp01 file01 muscle.fq When running the ls command with no options it will list files in your current working directory. The place where you start when you first login is your HOME directory. Answer : 3 (exp01, file01 and muscle.fq) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink10\").click(function(e){ e.preventDefault(); $(\"#showable10\").toggleClass(\"showable-hidden\"); }); }); //-->]]> The above answer is not quite correct. There are a number of hidden files in your home directory as well. 2.2) What flag might you use to display all files with the ls command? How many files are really there? Hint Take the all quite literally. More Type ls --all and press the ENTER key. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); if ($(\"#showable13\").hasClass(\"showable-hidden\")) { $(\"#showablelink13\").text(\"More\"); } else { $(\"#showablelink13\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); $(\"#showable12\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer 1 : --all (or -a ) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely . and .. which are not real files but instead, shortcuts. . is a shortcut for the current directory and .. a shortcut for the directory above the current one. Answer 2 : 10 files (don't count . and .. ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink14\").click(function(e){ e.preventDefault(); $(\"#showable14\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.3) What is the full path name of your home directory? Hint Remember your Current Working Directory start's in your home directory (and the hint from the slides). More Try a shortened version of print working directory //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink17\").click(function(e){ e.preventDefault(); $(\"#showable17\").toggleClass(\"showable-hidden\"); if ($(\"#showable17\").hasClass(\"showable-hidden\")) { $(\"#showablelink17\").text(\"More\"); } else { $(\"#showablelink17\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: $ pwd /home/trainingXY Answer : /home/trainingXY where XY is replaced by some 2 digit sequence. Alternate method : You can also find out the name of your home directory by printing the value of the $HOME shell variable: echo $HOME //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.4) Run ls using the long flag ( -l ), how did the output change? Hint Run ls -l //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : it changed the output to place 1 file/directory per line. It also added some extra information about each. $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11:28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11:28 muscle.fq Details : drwxr-x--- 2 training01 training 2048 Jun 14 11:28 exp01 \\--------/ ^ \\--------/ \\------/ \\--/ \\----------/ \\---/ permission | username group size date name /---^---\\ linkcount Where: permissions : 4 parts, file type, user perms, group perms and other perms filetype : 1 character, d = directory and - regular file user permissions: 3 characters, r = read, w = write, x = execute and - no permission group permissions: same as user except for users within the owner group other permissions: same as user except for users that are not in either user or group username : the user who owns this file/directory group : the group name who owns this file/directory size : the number of bytes this file/directory takes to store on disk date : the date and time when this file/directory was last edited name : name of the file linkcount : technical detail which represents the number of links this file has in the file system (safe to ignore) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); $(\"#showable21\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.5) What type of file is exp01 and muscle.fq ? Hint Check the output from the ls -l . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink23\").click(function(e){ e.preventDefault(); $(\"#showable23\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : exp01 : Directory (given the 'd' as the first letter of its permissions) muscle.fq : Regular File (given the '-') //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.6) Who has permission to read , write and execute your home directory? Hint You can also give ls a filename as the first option. More ls -l will show you the contents of the CWD ; how might you see the contents of the parent directory? (remember the slides) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink27\").click(function(e){ e.preventDefault(); $(\"#showable27\").toggleClass(\"showable-hidden\"); if ($(\"#showable27\").hasClass(\"showable-hidden\")) { $(\"#showablelink27\").text(\"More\"); } else { $(\"#showablelink27\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer If you pass the -l flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. Method 1 : given we know the CWD is our home directory. $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... The .. refers to the parent directory. Method 2 : using $HOME. This works no matter what our CWD is set to. You could list the permissions of all files and directories in the parent directory of your home: $ ls -l $HOME/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... In this case we use the shell variable to refer to our home directory. Method 3 : using ~ (tilde) shortcut You may also refer to your home directory using the ~ (tilde) character: $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like trainingXY . Where XY is replaced by a two digit string Altername : using the -a flag and looking at the . (dot) special file. $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14:18 . ... Answer : drwxr-x--- You : read (see filenames), write (add, delete files), execute (change your CWD to this directory). Training users : read, execute Everyone else : No access Discussion on Permissions : The permission string is \"drwxr-x---\" . The d means it is a directory. The rwx means that the owner of the directory (your user account) can read , write and execute the directory. Execute permissions on a directory means that you can cd into the directory. The r-x means that anyone in the same user group as training can read or execute the directory. The --- means that nobody else (other users on the system) can do anything with the directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q . 2.7) Use the man command to find out what the -h flag does for ls Hint Give ls as an option to man command. More man ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); if ($(\"#showable31\").hasClass(\"showable-hidden\")) { $(\"#showablelink31\").text(\"More\"); } else { $(\"#showablelink31\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink30\").click(function(e){ e.preventDefault(); $(\"#showable30\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the following command to view the man page for ls : $ man ls Answer : You should discover that the -h option prints file sizes in human readable format -h, --human-readable with -l, print sizes in human readable format (e.g., 1K 234M 2G) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 2.8) Use the -h , how did the output change of muscle.fq ? Hint Don't forget the -l option too. More Run ls -lh //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink35\").click(function(e){ e.preventDefault(); $(\"#showable35\").toggleClass(\"showable-hidden\"); if ($(\"#showable35\").hasClass(\"showable-hidden\")) { $(\"#showablelink35\").text(\"More\"); } else { $(\"#showablelink35\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink34\").click(function(e){ e.preventDefault(); $(\"#showable34\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls -lh ... -rw-r----- 1 training01 training 2.5K Jun 14 11:28 muscle.fq Answer : it changed the output so the filesize of muscle.fq is now 2.5K instead of 2461 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink36\").click(function(e){ e.preventDefault(); $(\"#showable36\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Topic 2: Exploring your home directory"},{"location":"tutorials/unix/#topic-3-exploring-the-file-system","text":"In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file 3.1) Print the value of your current working directory. Answer The pwd command prints the value of your current working directory. $ pwd /home/training01 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink38\").click(function(e){ e.preventDefault(); $(\"#showable38\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.2) List the contents of the root directory, called ' / ' (forward slash). Hint ls expects a single option which is the directory to change too. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink40\").click(function(e){ e.preventDefault(); $(\"#showable40\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that ls can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink41\").click(function(e){ e.preventDefault(); $(\"#showable41\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.3) Use the cd command to change your working directory to the root directory. Did your prompt change? Hint cd expects a single option which is the directory to change to //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink43\").click(function(e){ e.preventDefault(); $(\"#showable43\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The cd command changes the value of your current working directory. To change to the root directory use the following command: $ cd / Answer : Yes, it now says the CWD is / instead of ~ . Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink44\").click(function(e){ e.preventDefault(); $(\"#showable44\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen. 3.4) List the contents of the CWD and verify it matches the list in 3.2 Hint ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink46\").click(function(e){ e.preventDefault(); $(\"#showable46\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Assuming you have changed to the root directory then this can be achieved with ls , or ls -a (for all files) or ls -la for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Answer : Yes, we got the same output as exercise 3.2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink47\").click(function(e){ e.preventDefault(); $(\"#showable47\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system? Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory More $HOME , ~ , /home/trainingXY are all methods to name your home directory. Yet there is a simpler method; the answer is buried in man cd however cd doesn't its own manpage so you will need to search for it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink50\").click(function(e){ e.preventDefault(); $(\"#showable50\").toggleClass(\"showable-hidden\"); if ($(\"#showable50\").hasClass(\"showable-hidden\")) { $(\"#showablelink50\").text(\"More\"); } else { $(\"#showablelink50\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink49\").click(function(e){ e.preventDefault(); $(\"#showable49\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the cd command to change your working directory to your home directory. There are a number of ways to refer to your home directory: cd $HOME is equivalent to: cd ~ The simplest way to change your current working directory to your home directory is to run the cd command with no arguments: Answer : the simplest for is cd with NO options. cd This is a special-case behaviour which is built into cd for convenience. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink51\").click(function(e){ e.preventDefault(); $(\"#showable51\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.6) Change your working directory to /home/group/common/training/Intro_to_Unix/ Answer cd /home/group/common/training/Intro_to_Unix/ //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink53\").click(function(e){ e.preventDefault(); $(\"#showable53\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.7) List the contents of that directory. How many files does it contain? Hint ls //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink55\").click(function(e){ e.preventDefault(); $(\"#showable55\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can do this with ls $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Answer : 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink56\").click(function(e){ e.preventDefault(); $(\"#showable56\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.8) What kind of file is /home/group/common/training/Intro_to_Unix/sleepy ? Hint Take the word file quite literally. More file sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink59\").click(function(e){ e.preventDefault(); $(\"#showable59\").toggleClass(\"showable-hidden\"); if ($(\"#showable59\").hasClass(\"showable-hidden\")) { $(\"#showablelink59\").text(\"More\"); } else { $(\"#showablelink59\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink58\").click(function(e){ e.preventDefault(); $(\"#showable58\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the file command to get extra information about the contents of a file: Assuming your current working directory is /home/group/common/training/Intro_to_Unix/ $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: $ file /home/group/common/training/Intro_to_Unix/sleepy Bourne-Again shell script text executable Answer : Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The file command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: man file //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink60\").click(function(e){ e.preventDefault(); $(\"#showable60\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.9) What kind of file is /home/group/common/training/Intro_to_Unix/hi ? Hint Take the word file quite literally. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink62\").click(function(e){ e.preventDefault(); $(\"#showable62\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the file command again. If you are in the same directory as hi then: $ file hi ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.9, not stripped Answer : ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called hi contains a binary executable program (raw instructions that the computer can execute directly). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink63\").click(function(e){ e.preventDefault(); $(\"#showable63\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.10) What are the file permissions of /home/group/common/training/Intro_to_Unix/sleepy ? What do they mean? Hint Remember the ls command, and don't forget the -l flag //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink65\").click(function(e){ e.preventDefault(); $(\"#showable65\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can find the permissions of sleepy using the ls command with the -l flag. If you are in the same directory as sleepy then: $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16:36 sleepy Answer : We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and writeable only to training01. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the ln command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink66\").click(function(e){ e.preventDefault(); $(\"#showable66\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3.11) Change your working directory back to your home directory ready for the next topic. Hint cd //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink68\").click(function(e){ e.preventDefault(); $(\"#showable68\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You should know how to do this with the cd command: cd //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink69\").click(function(e){ e.preventDefault(); $(\"#showable69\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Topic 3: Exploring the file system"},{"location":"tutorials/unix/#topic-4-working-with-files-and-directories","text":"In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections. 4.1) In your home directory make a sub-directory called test. Hint You are trying to make a directory , which of the above commands looks like a shortened version of this? More mkdir //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink72\").click(function(e){ e.preventDefault(); $(\"#showable72\").toggleClass(\"showable-hidden\"); if ($(\"#showable72\").hasClass(\"showable-hidden\")) { $(\"#showablelink72\").text(\"More\"); } else { $(\"#showablelink72\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink71\").click(function(e){ e.preventDefault(); $(\"#showable71\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Make sure you are in your home directory first. If not cd to your home directory. Use the mkdir command to make new directories: $ mkdir test Use the ls command to check that the new directory was created. $ ls exp01 file01 muscle.fq test //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink73\").click(function(e){ e.preventDefault(); $(\"#showable73\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.2) Copy all the files from /home/group/common/training/Intro_to_Unix/ into the newly created test directory. Hint You are trying to copy , which of the above commands looks like a shortened version of this? More $ man cp ... SYNOPSIS cp [OPTION]... [-T] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. which means cp expects zero or more flags, a SOURCE file followed by a DEST file or directory //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink76\").click(function(e){ e.preventDefault(); $(\"#showable76\").toggleClass(\"showable-hidden\"); if ($(\"#showable76\").hasClass(\"showable-hidden\")) { $(\"#showablelink76\").text(\"More\"); } else { $(\"#showablelink76\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink75\").click(function(e){ e.preventDefault(); $(\"#showable75\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the cp command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to cp . You only need to perform one of these ways, but we show multiple ones for your reference. Answer 1 : From your home directory: $ cp /home/group/common/training/Intro_to_Unix/* test Answer 2 : Change to the test directory and then copy (assuming you started in your home directory): $ cd test $ cp /home/group/common/training/Intro_to_Unix/* . In the example above the ' . ' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. Answer 3 : Change to the /home/group/common/training/Intro_to_Unix/ directory and then copy: cd /home/group/common/training/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink77\").click(function(e){ e.preventDefault(); $(\"#showable77\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : This exercise assumes that the copy command from the previous exercise was successful. 4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to. Hint Remember ls can show you the file size (with one of its flags) More ls -l //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink80\").click(function(e){ e.preventDefault(); $(\"#showable80\").toggleClass(\"showable-hidden\"); if ($(\"#showable80\").hasClass(\"showable-hidden\")) { $(\"#showablelink80\").text(\"More\"); } else { $(\"#showablelink80\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink79\").click(function(e){ e.preventDefault(); $(\"#showable79\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use ls -l to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: $ ls -l /home/group/common/training/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. Answer : They should each be 1033773 bytes Alternate : Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the -h option for ls: $ ls -lh /home/group/common/training/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /home/group/common/training/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as 1010K . Larger files are reported in megabytes, gigabytes etcetera. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink81\").click(function(e){ e.preventDefault(); $(\"#showable81\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test 4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to. Hint What is the opposite of same ? More diff erence //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink84\").click(function(e){ e.preventDefault(); $(\"#showable84\").toggleClass(\"showable-hidden\"); if ($(\"#showable84\").hasClass(\"showable-hidden\")) { $(\"#showablelink84\").text(\"More\"); } else { $(\"#showablelink84\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink83\").click(function(e){ e.preventDefault(); $(\"#showable83\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the diff command to compare the contents of two files. $ diff /home/group/common/training/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the diff command will NOT produce any output) Answer : Yes, they are the same since no output was given. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink85\").click(function(e){ e.preventDefault(); $(\"#showable85\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.5) How many lines, words and characters are in expectations.txt? Hint Initialisms are key More w ord c ount //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink88\").click(function(e){ e.preventDefault(); $(\"#showable88\").toggleClass(\"showable-hidden\"); if ($(\"#showable88\").hasClass(\"showable-hidden\")) { $(\"#showablelink88\").text(\"More\"); } else { $(\"#showablelink88\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink87\").click(function(e){ e.preventDefault(); $(\"#showable87\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the wc (for \"word count\") to count the number of characters, lines and words in a file: $ wc expectations.txt 20415 187465 1033773 expectations.txt Answer : There are 20415 lines, 187465 words and 1033773 characters in expectations.txt. To get just the line, word or character count: $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink89\").click(function(e){ e.preventDefault(); $(\"#showable89\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano . Hint nano FILENAME Once nano is open it displays some command hints along the bottom of the screen. More ^O means hold the Control (or CTRL) key while pressing the o . Dispite what it displays, you need to type the lower-case letter that follows the ^ character. WriteOut is another name for Save. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink92\").click(function(e){ e.preventDefault(); $(\"#showable92\").toggleClass(\"showable-hidden\"); if ($(\"#showable92\").hasClass(\"showable-hidden\")) { $(\"#showablelink92\").text(\"More\"); } else { $(\"#showablelink92\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink91\").click(function(e){ e.preventDefault(); $(\"#showable91\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Take some time to play around with the nano text editor. Nano is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as vim and emacs , however they take a substantial amount of time to learn. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink93\").click(function(e){ e.preventDefault(); $(\"#showable93\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.7) Did the changes you made to ~/test/expectations.txt have any effect on /home/group/common/training/Intro_to_Unix/expectations.txt ? How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 More Use diff //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink96\").click(function(e){ e.preventDefault(); $(\"#showable96\").toggleClass(\"showable-hidden\"); if ($(\"#showable96\").hasClass(\"showable-hidden\")) { $(\"#showablelink96\").text(\"More\"); } else { $(\"#showablelink96\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink95\").click(function(e){ e.preventDefault(); $(\"#showable95\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use diff to check that the two files are different after you have made the change to the copy of expectations.txt in your ~/test directory. diff ~/test/expectations.txt \\ /home/group/common/training/Intro_to_Unix/expectations.txt You could also use ls to check that the files have different sizes. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink97\").click(function(e){ e.preventDefault(); $(\"#showable97\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.8) In your test subdirectory, rename expectations.txt to foo.txt . Hint Another way to think of it is moving it from expectations.txt to foo.txt More mv Use man mv if you need to work out how to use it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink100\").click(function(e){ e.preventDefault(); $(\"#showable100\").toggleClass(\"showable-hidden\"); if ($(\"#showable100\").hasClass(\"showable-hidden\")) { $(\"#showablelink100\").text(\"More\"); } else { $(\"#showablelink100\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink99\").click(function(e){ e.preventDefault(); $(\"#showable99\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the mv command to rename the file: $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink101\").click(function(e){ e.preventDefault(); $(\"#showable101\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.9) Rename foo.txt back to expectations.txt. Answer Use the mv command to rename the file: $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use ls to check that the file is in fact renamed. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink103\").click(function(e){ e.preventDefault(); $(\"#showable103\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.10) Remove the file expectations.txt from your test directory. Hint We are trying to remove a file, check the commands at the top of this topic. More rm //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink106\").click(function(e){ e.preventDefault(); $(\"#showable106\").toggleClass(\"showable-hidden\"); if ($(\"#showable106\").hasClass(\"showable-hidden\")) { $(\"#showablelink106\").text(\"More\"); } else { $(\"#showablelink106\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink105\").click(function(e){ e.preventDefault(); $(\"#showable105\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the rm command to remove files (carefully): $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink107\").click(function(e){ e.preventDefault(); $(\"#showable107\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.11) Remove the entire test directory and all the files within it. Hint We are trying to remove a directory . More You could use rmdir but there is an easier way using just rm and a flag. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink110\").click(function(e){ e.preventDefault(); $(\"#showable110\").toggleClass(\"showable-hidden\"); if ($(\"#showable110\").hasClass(\"showable-hidden\")) { $(\"#showablelink110\").text(\"More\"); } else { $(\"#showablelink110\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink109\").click(function(e){ e.preventDefault(); $(\"#showable109\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You could use the rm command to remove each file individually, and then use the rmdir command to remove the directory. Note that rmdir will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the -r (for recursive) flag to rm to remove all the files and the directory in one go: Logical Answer : cd ~ rm test/* rmdir test Easier Answer : cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink111\").click(function(e){ e.preventDefault(); $(\"#showable111\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.12) Recreate the test directory in your home directory and copy all the files from /home/group/common/training/Intro_to_Unix/ back into the test directory. Hint See exercises 4.1 and 4.2 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink113\").click(function(e){ e.preventDefault(); $(\"#showable113\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Repeat exercises 4.1 and 4.2. $ cd ~ $ mkdir test $ cp /home/group/common/training/Intro_to_Unix/* test //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink114\").click(function(e){ e.preventDefault(); $(\"#showable114\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c Hint Use man if you can't guess how it might work. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink116\").click(function(e){ e.preventDefault(); $(\"#showable116\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ cd ~/test $ cat hello.c #include <stdio.h> int main(void) { printf (\"Hello World\\n\"); return 0; } hello.c contains the source code of a C program. The compiled executable version of this code is in the file called hi , which you can run like so: $ ./hi Hello World //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink117\").click(function(e){ e.preventDefault(); $(\"#showable117\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.14) Use the head command to view the first 20 lines of the file sample_1.fastq Hint Remember your best friend! More Use man to find out what option you need to add to display a given number of lines . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink120\").click(function(e){ e.preventDefault(); $(\"#showable120\").toggleClass(\"showable-hidden\"); if ($(\"#showable120\").hasClass(\"showable-hidden\")) { $(\"#showablelink120\").text(\"More\"); } else { $(\"#showablelink120\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink119\").click(function(e){ e.preventDefault(); $(\"#showable119\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa``aaa`aaaa_^a```]][Z[DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa`a```^aaaaa`_]aaa`aaa__a_X]`` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa`aaaaaaa^ba_]]aaa^aaaaa_^][aa //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink121\").click(function(e){ e.preventDefault(); $(\"#showable121\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq Hint Its very much like head . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink123\").click(function(e){ e.preventDefault(); $(\"#showable123\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab]abbaa`babaaabbb`bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 `aaaaa``aaa`a``a`^a`a`a_[a_a`a`aa`__ //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink124\").click(function(e){ e.preventDefault(); $(\"#showable124\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.16) Use the grep command to find out all the lines in moby.txt that contain the word \"Ahab\" Hint One might say we are 'looking for the pattern \"Ahab\"' More $ man grep ... SYNOPSIS grep [OPTIONS] PATTERN [FILE...] ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink127\").click(function(e){ e.preventDefault(); $(\"#showable127\").toggleClass(\"showable-hidden\"); if ($(\"#showable127\").hasClass(\"showable-hidden\")) { $(\"#showablelink127\").text(\"More\"); } else { $(\"#showablelink127\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink126\").click(function(e){ e.preventDefault(); $(\"#showable126\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the wc -l command: $ grep Ahab moby.txt | wc -l 491 which shows that there are 491 lines in moby.txt that contain the word Ahab. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink128\").click(function(e){ e.preventDefault(); $(\"#showable128\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera) . Hint One might say we are ignoring case . More $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. (-i is specified by POSIX.) ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink131\").click(function(e){ e.preventDefault(); $(\"#showable131\").toggleClass(\"showable-hidden\"); if ($(\"#showable131\").hasClass(\"showable-hidden\")) { $(\"#showablelink131\").text(\"More\"); } else { $(\"#showablelink131\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink130\").click(function(e){ e.preventDefault(); $(\"#showable130\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the -i flag to grep to make it perform case insensitive search: $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [Project Gutenberg Editor's Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to wc -l to count the number of lines: $ grep -i the expectations.txt | wc -l 8165 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink132\").click(function(e){ e.preventDefault(); $(\"#showable132\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 4.18) Use the gzip command to compress the file sample_1.fastq . Use gunzip to decompress it back to the original contents. Hint Use the above commands along with man and ls to see what happens to the file. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink134\").click(function(e){ e.preventDefault(); $(\"#showable134\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Check the file size of sample_1.fastq before compressing it: # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20:03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20:03 sample_1.fastq You will see that when it was compressed it is 26997595 bytes in size, making it about 0.3 times the size of the original file. Note : in the above section the lines starting with # are comments so don't need to be copied but if you do then they wont do anything. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink135\").click(function(e){ e.preventDefault(); $(\"#showable135\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Topic 4: Working with files and directories"},{"location":"tutorials/unix/#topic-5-pipes-output-redirection-and-shell-scripts","text":"In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \"tip of the iceberg\". Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut 5.1) How many reads are contained in the file sample_1.fastq ? Hint Examine some of the file to work out how many lines each read takes up. More Count the number of lines //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink138\").click(function(e){ e.preventDefault(); $(\"#showable138\").toggleClass(\"showable-hidden\"); if ($(\"#showable138\").hasClass(\"showable-hidden\")) { $(\"#showablelink138\").text(\"More\"); } else { $(\"#showablelink138\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink137\").click(function(e){ e.preventDefault(); $(\"#showable137\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer We can answer this question by counting the number of lines in the file and dividing by 4: $ wc -l sample_1.fastq 3000000 Answer : There are 3000000 lines in the file representing 750000 reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called bc : $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. bc is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink139\").click(function(e){ e.preventDefault(); $(\"#showable139\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.2) How many reads in sample_1.fastq contain the sequence GATTACA ? Hint Check out exercise 4.16 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink141\").click(function(e){ e.preventDefault(); $(\"#showable141\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use grep to find all the lines that contain GATTACA and \"pipe\" the output to wc -l to count them: $ grep GATTACA sample_1.fastq | wc -l 1119 Answer : 1119 If you are unsure about the possibility of upper and lower case characters then consider using the -i (ignore case option for grep). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink142\").click(function(e){ e.preventDefault(); $(\"#showable142\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.3) On what line numbers do the sequences containing GATTACA occur? Hint We are looking for the line numbers . More Check out the manpage for grep and/or nl //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink145\").click(function(e){ e.preventDefault(); $(\"#showable145\").toggleClass(\"showable-hidden\"); if ($(\"#showable145\").hasClass(\"showable-hidden\")) { $(\"#showablelink145\").text(\"More\"); } else { $(\"#showablelink145\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink144\").click(function(e){ e.preventDefault(); $(\"#showable144\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer You can use the -n flag to grep to make it prefix each line with a line number: Answer 1 : $ grep -n GATTACA sample_1.fastq 5078:AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170:AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238:ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... Answer 2 : Or you can use the nl command to number each line of sample_1.fastq and then search for GATTACA in the numbered lines: $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... Just the line numbers : If you just want to see the line numbers then you can \"pipe\" the output of the above command into cut -f 1 : $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... cut will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the -f 1 option) $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink146\").click(function(e){ e.preventDefault(); $(\"#showable146\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning. Hint Check answer to 5.3. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink148\").click(function(e){ e.preventDefault(); $(\"#showable148\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with control-c (hold the control key down and simultaneously press the \" c \" character). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink149\").click(function(e){ e.preventDefault(); $(\"#showable149\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl . Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and ' / ' to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, > FILENAME is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink151\").click(function(e){ e.preventDefault(); $(\"#showable151\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \" > \" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the head command like so: $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa`]baaaaa_aab]D^^`b`aYDW]abaa`^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa`]`ba`]`aaaaYD\\\\_a``XT ... The less command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the ' / ' followed by the search term. You can quit by pressing \" q \". Note that the less command is used by default to display man pages. $ less sample_1.fastq.nl //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink152\").click(function(e){ e.preventDefault(); $(\"#showable152\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq.nl into TSV format: $ cat sample_1.fastq | paste - - - - > sample_1.tsv //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink153\").click(function(e){ e.preventDefault(); $(\"#showable153\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The '-' (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all wil support it. The paste command is useful for merging multiple files together line-by-line, such that the Nth line from each file is joined together into one line in the output, separated by default with a tab character. In the above example we give paste 4 copies of the contents of sample_1.fastq , which causes it to join consecutive groups of 4 lines from the file into one line of output. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink154\").click(function(e){ e.preventDefault(); $(\"#showable154\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.7) Do you expect the output of the following command to produce the same output as above? and why? $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink155\").click(function(e){ e.preventDefault(); $(\"#showable155\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Use less to examine it. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink156\").click(function(e){ e.preventDefault(); $(\"#showable156\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Answer : No, in the second instance we get 4 copies of each line. Why : In the first command paste will use the input file (standard input) 4 times since the cat command will only give one copy of the file to paste , where as, in the second command paste will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink157\").click(function(e){ e.preventDefault(); $(\"#showable157\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file. Hint Remember the wc command. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink159\").click(function(e){ e.preventDefault(); $(\"#showable159\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer We can count the number of lines in sample_1.tsv using wc : $ wc -l sample_1.tsv The output should be 750000 as expected (1/4 of the number of lines in sample_1.fastq). To view the first 20 lines of sample_1.tsv use the head command: $ head -20 sample_1.tsv //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink160\").click(function(e){ e.preventDefault(); $(\"#showable160\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.9) Use the cut command to print out the second column of sample_1.tsv . Redirect the output to a file called sample_1.dna.txt . Hint See exercise 5.3 (for cut) and 5.5 (redirection) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink162\").click(function(e){ e.preventDefault(); $(\"#showable162\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using head or less . //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink163\").click(function(e){ e.preventDefault(); $(\"#showable163\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt . Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As. Hint Use man (sort) and see exercise 5.5 (redirection) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink165\").click(function(e){ e.preventDefault(); $(\"#showable165\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running head on the output file reveals that there are duplicate DNA sequences in the input FASTQ file. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink166\").click(function(e){ e.preventDefault(); $(\"#showable166\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt , redirect the result to sample_1.dna.uniq.txt . Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt . Hint I am pretty sure you have already used man (or just guessed how to use uniq ). You're also a gun at redirection now. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink168\").click(function(e){ e.preventDefault(); $(\"#showable168\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of sample_1.dna.uniq.txt to check that the duplicate DNA sequences have been removed. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink169\").click(function(e){ e.preventDefault(); $(\"#showable169\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt ? Hint Checkout the uniq manpage //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink171\").click(function(e){ e.preventDefault(); $(\"#showable171\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Hint Look at the man page for uniq. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink172\").click(function(e){ e.preventDefault(); $(\"#showable172\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Use the -d flag to uniq to print out only the duplicated lines from the file: $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink173\").click(function(e){ e.preventDefault(); $(\"#showable173\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq. Hint That is, piping most of the commands you used above instead of redirecting to file More I.e. 6 commands ( cat , paste , cut , sort , uniq , wc ) //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink176\").click(function(e){ e.preventDefault(); $(\"#showable176\").toggleClass(\"showable-hidden\"); if ($(\"#showable176\").hasClass(\"showable-hidden\")) { $(\"#showablelink176\").text(\"More\"); } else { $(\"#showablelink176\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink175\").click(function(e){ e.preventDefault(); $(\"#showable175\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: Answer : $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have 56079 lines. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink177\").click(function(e){ e.preventDefault(); $(\"#showable177\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq. Hint Check out the sleepy file (with cat or nano ); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out chmod command). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink179\").click(function(e){ e.preventDefault(); $(\"#showable179\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Put the answer to 5.13 into a file called sample_1_dups.sh (or whatever you want). Use nano to create the file. Answer : the contents of the file will look like this: #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: $ chmod +x sample_1_dups.sh You can run the script like so: $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink180\").click(function(e){ e.preventDefault(); $(\"#showable180\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter. Hint Shell scripts can refer to command line arguments by their position using special variables called $0 , $1 , $2 and so on. More $0 refers to the name of the script as it was called on the command line. $1 refers to the first command line argument, and so on. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink183\").click(function(e){ e.preventDefault(); $(\"#showable183\").toggleClass(\"showable-hidden\"); if ($(\"#showable183\").hasClass(\"showable-hidden\")) { $(\"#showablelink183\").text(\"More\"); } else { $(\"#showablelink183\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink182\").click(function(e){ e.preventDefault(); $(\"#showable182\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Copy the shell script from 5.14 into a new file: $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: $ ./fastq_dups.sh sample_1.fastq In the above example the script takes sample_1.fastq as input and prints the number of duplicated sequences as output. A better Answer : Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: #!/bin/bash if [ $# -eq 1 ]; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The ' if ...; then ' line means: do the following line(s) ONLY if the ... (called condition) bit is true. The ' else ' line means: otherwise do the following line(s) instead. Note: it is optional. The ' fi ' line means: this marks the end of the current if or else section. The ' [ $# -eq 1 ] ' part is the condition: $# : is a special shell variable that indicates how many command line arguments were given. -eq : checks if the numbers on either side if it are equal. 1 : is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink184\").click(function(e){ e.preventDefault(); $(\"#showable184\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file. Answer We can add a loop to our script to accept multiple input FASTQ files: #!/bin/bash for file in $@; do dups=$(cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l) echo \"$file $dups\" done There's a lot going on in this script. The $@ is a sequence of all command line arguments. The ' for ...; do ' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called file . The $(...) allow us to capture the output of another command (in-place of the ... ). In this case we capture the output of the pipeline and save it to the variable called dups . If you had multiple FASTQ files available you could run the script like so: ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink186\").click(function(e){ e.preventDefault(); $(\"#showable186\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Topic 5: Pipes, output redirection and shell scripts"},{"location":"tutorials/unix/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be confortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this Workshop for later reference. Thank you for your attendance, please don't forget to complete the Melbourne Bioinformatics training survey and give it back to the Workshop facilitators.","title":"Finished"},{"location":"tutorials/using_git/","text":"PR reviewers and advice: Juan Nunez-Iglesias Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/using_git/Using_Git/","text":"Using Git and Github for revision control What is Git? Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly. Getting help There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough. A simple workflow Step 1, create a github account. Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics . Step 2, sign into github and create a repository. Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning) Step 3, clone your repository to your local computer. Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). $ git clone https://github.com/bjpop/test.git Cloning into 'test'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README.md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs Step 4, commit a file to the repository. Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: $ python hello.py hello world Check the status of your repository: $ git status # On branch master # Untracked files: # (use \"git add <file>...\" to include in what will be committed) # # hello.py nothing added to commit but untracked files present (use \"git add\" to track) Notice that git tells you that the new file hello.py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \"stage\" your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello.py have been staged and are ready to be committed. Notice that hello.py is no longer untracked. Commit your changes with a commit message: $ git commit -m \"A little greeting program\" [master b1cce11] A little greeting program 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 hello.py Re-check the status of your repository: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin/master ). Step 5, push your changes to github. Push the commit in your local repository to github (thus synchronising them). $ git push origin Username for 'https://github.com': <type your github username> Password for 'https://<your github username>@github.com': Counting objects: 4, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 305 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git 71a771a..b1cce11 master -> master Now if you look at your repository on github you should see the file hello.py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name: Step 6, create a branch in your local repository. You can ask git to tell you about the names of the current branches: $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello.py file: $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): $ git status # On branch documentation # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hello.py # no changes added to commit (use \"git add\" and/or \"git commit -a\") Stage the new changes and commit them, and check the status again: $ git add hello.py $ git commit -m \"Added a comment\" [documentation 9bbe430] Added a comment 1 files changed, 1 insertions(+), 0 deletions(-) $ git status # On branch documentation nothing to commit (working directory clean) Now we can push the new \u201cdocumentation\u201d branch to github: $ git push origin documentation Username for 'https://github.com': <your github username> Password for 'https://<your github username>@github.com': Counting objects: 5, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 314 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git * [new branch] documentation -> documentation On github you should be able to see the new branch: Step 7, merge the changes back into the master branch. To go back to the master branch you must check it out: $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: $ cat hello.py print(\"hello world\") Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: $ cat hello.py print(\"hello world\") #this is a comment Check the status of the master branch: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Push the changes in the master branch back to github: $ git push origin master Username for 'https://github.com': bjpop Password for 'https://bjpop@github.com': Total 0 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git b1cce11..9bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17:29:02 2013 -0700","title":"Using Git and Github for revision control"},{"location":"tutorials/using_git/Using_Git/#using-git-and-github-for-revision-control","text":"","title":"Using Git and Github for revision control"},{"location":"tutorials/using_git/Using_Git/#what-is-git","text":"Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly.","title":"What is Git?"},{"location":"tutorials/using_git/Using_Git/#getting-help","text":"There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough.","title":"Getting help"},{"location":"tutorials/using_git/Using_Git/#a-simple-workflow","text":"","title":"A simple workflow"},{"location":"tutorials/using_git/Using_Git/#step-1-create-a-github-account","text":"Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics .","title":"Step 1, create a github account."},{"location":"tutorials/using_git/Using_Git/#step-2-sign-into-github-and-create-a-repository","text":"Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning)","title":"Step 2, sign into github and create a repository."},{"location":"tutorials/using_git/Using_Git/#step-3-clone-your-repository-to-your-local-computer","text":"Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). $ git clone https://github.com/bjpop/test.git Cloning into 'test'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0) Unpacking objects: 100% (3/3), done. This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README.md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs","title":"Step 3, clone your repository to your local computer."},{"location":"tutorials/using_git/Using_Git/#step-4-commit-a-file-to-the-repository","text":"Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: $ python hello.py hello world Check the status of your repository: $ git status # On branch master # Untracked files: # (use \"git add <file>...\" to include in what will be committed) # # hello.py nothing added to commit but untracked files present (use \"git add\" to track) Notice that git tells you that the new file hello.py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \"stage\" your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello.py have been staged and are ready to be committed. Notice that hello.py is no longer untracked. Commit your changes with a commit message: $ git commit -m \"A little greeting program\" [master b1cce11] A little greeting program 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 hello.py Re-check the status of your repository: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin/master ).","title":"Step 4, commit a file to the repository."},{"location":"tutorials/using_git/Using_Git/#step-5-push-your-changes-to-github","text":"Push the commit in your local repository to github (thus synchronising them). $ git push origin Username for 'https://github.com': <type your github username> Password for 'https://<your github username>@github.com': Counting objects: 4, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 305 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git 71a771a..b1cce11 master -> master Now if you look at your repository on github you should see the file hello.py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name:","title":"Step 5, push your changes to github."},{"location":"tutorials/using_git/Using_Git/#step-6-create-a-branch-in-your-local-repository","text":"You can ask git to tell you about the names of the current branches: $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello.py file: $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): $ git status # On branch documentation # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hello.py # no changes added to commit (use \"git add\" and/or \"git commit -a\") Stage the new changes and commit them, and check the status again: $ git add hello.py $ git commit -m \"Added a comment\" [documentation 9bbe430] Added a comment 1 files changed, 1 insertions(+), 0 deletions(-) $ git status # On branch documentation nothing to commit (working directory clean) Now we can push the new \u201cdocumentation\u201d branch to github: $ git push origin documentation Username for 'https://github.com': <your github username> Password for 'https://<your github username>@github.com': Counting objects: 5, done. Delta compression using up to 16 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 314 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git * [new branch] documentation -> documentation On github you should be able to see the new branch:","title":"Step 6, create a branch in your local repository."},{"location":"tutorials/using_git/Using_Git/#step-7-merge-the-changes-back-into-the-master","text":"branch. To go back to the master branch you must check it out: $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: $ cat hello.py print(\"hello world\") Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: $ cat hello.py print(\"hello world\") #this is a comment Check the status of the master branch: $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit (working directory clean) Push the changes in the master branch back to github: $ git push origin master Username for 'https://github.com': bjpop Password for 'https://bjpop@github.com': Total 0 (delta 0), reused 0 (delta 0) To https://github.com/bjpop/test.git b1cce11..9bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12:08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17:29:02 2013 -0700","title":"Step 7, merge the changes back into the master"},{"location":"tutorials/var_detect_advanced/","text":"PR reviewers and advice: Clare Sloggett Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/","text":"Variant Detection - Advanced Workshop Tutorial Overview In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information Background Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ Preparation Make sure you have an instance of Galaxy ready to go. If you don't have your own - go to our Galaxy-Tut or Galaxy-Melbourne server. Log in so that your work will be saved. If you don't already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won't need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf . Section 1: Quality Control The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30). Section 2: Alignment and depth of coverage In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \"NA12878\" Set the platform to ILLUMINA Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download. If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it. Section 3. Local realignment Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant. Section 4. Calling variants with FreeBayes FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Section 5. Calling variants with GATK Unified Genotyper For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378. Section 6. Evaluate variants How can we evaluate our variants? We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data... (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \"^#\". ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\". As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \"FreeBayes\". As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\". Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to remove many false positives, but also a few true positives. Section 7. Annotation The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated ... and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF... header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \"missense_variant\". Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?","title":"Tutorial"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#variant-detection-advanced-workshop","text":"","title":"Variant Detection - Advanced Workshop"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#tutorial-overview","text":"In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information","title":"Tutorial Overview"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#background","text":"Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#preparation","text":"Make sure you have an instance of Galaxy ready to go. If you don't have your own - go to our Galaxy-Tut or Galaxy-Melbourne server. Log in so that your work will be saved. If you don't already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won't need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf .","title":"Preparation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-1-quality-control","text":"The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).","title":"Section 1: Quality Control"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-2-alignment-and-depth-of-coverage","text":"In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \"NA12878\" Set the platform to ILLUMINA Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download. If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.","title":"Section 2: Alignment and depth of coverage"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-3-local-realignment","text":"Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.","title":"Section 3. Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-4-calling-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.","title":"Section 4. Calling variants with FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-5-calling-variants-with-gatk-unified-genotyper","text":"For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.","title":"Section 5. Calling variants with GATK Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-6-evaluate-variants","text":"How can we evaluate our variants? We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data... (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \"^#\". ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\". As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \"FreeBayes\". As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\". Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to remove many false positives, but also a few true positives.","title":"Section 6. Evaluate variants"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-7-annotation","text":"The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated ... and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF... header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \"missense_variant\". Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?","title":"Section 7. Annotation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/","text":"Introduction to Variant detection Background A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV) Variant Calling vs genotyping Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants. Process of variant calling Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position. Homozygous or Heterozygous mutations: What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events. Variant Calling Software: There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods Variant Calling using Samtools (Mpileup + bcftools) Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information Variant Calling using GATK-Unified Genotyper GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: P(Genotype | Data) = (P(Data | Genotype) * P(Genotype)) / P(Data) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information Variant Calling using FreeBayes FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information Evaluation of detected variants using Variant Eval The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file. Quality Matrix: TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance. Local realignment In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml The Galaxy workflow platform Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right. Data Format used in the tutorial Sequence Alignment Map format SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy Binary Sequence Alignment Map format BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV VCF file format What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. VCF format in Galaxy: Bcf file format: BCF format: BCF, or the binary variant call format, is the binary version of VCF. It keeps the same information in VCF, while much more efficient to process especially for many samples. The relationship between BCF and VCF is similar to that between BAM and SAM.","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#introduction-to-variant-detection","text":"","title":"Introduction to Variant detection"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#background","text":"A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV)","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-vs-genotyping","text":"Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants.","title":"Variant Calling vs genotyping"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#process-of-variant-calling","text":"Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.","title":"Process of variant calling"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#homozygous-or-heterozygous-mutations","text":"What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.","title":"Homozygous or Heterozygous mutations:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-software","text":"There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods","title":"Variant Calling Software:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-samtools-mpileup-bcftools","text":"Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information","title":"Variant Calling using Samtools (Mpileup + bcftools)"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-gatk-unified-genotyper","text":"GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: P(Genotype | Data) = (P(Data | Genotype) * P(Genotype)) / P(Data) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information","title":"Variant Calling using GATK-Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-freebayes","text":"FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information","title":"Variant Calling using FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#evaluation-of-detected-variants-using-variant-eval","text":"The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file.","title":"Evaluation of detected variants using Variant Eval"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#quality-matrix","text":"TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.","title":"Quality Matrix:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#local-realignment","text":"In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml","title":"Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#the-galaxy-workflow-platform","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.","title":"The Galaxy workflow platform"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#data-format-used-in-the-tutorial","text":"","title":"Data Format used in the tutorial"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#sequence-alignment-map-format","text":"SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy","title":"Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#binary-sequence-alignment-map-format","text":"BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV","title":"Binary Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-file-format","text":"What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT.","title":"VCF file format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-format-in-galaxy","text":"","title":"VCF format in Galaxy:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#bcf-file-format","text":"","title":"Bcf file format:"},{"location":"tutorials/variant_calling_galaxy_1/","text":"PR reviewers and advice: Clare Sloggett, Khalid Mahmood, Jessica Chung Current slides: https://docs.google.com/presentation/d/18rjOlb3Q_i_IY65_4cA2UWp76uzAyrrcS6YJS2AzquE (these are Clare's from May 2017 workshop round - fairly cut-down) Other slides: Original main folder: https://drive.google.com/drive/u/0/folders/0B-gCKa6V4E0lelJGVDFGaHBLSlk","title":"Home"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/","text":"Introduction to Variant Calling using Galaxy Tutorial Overview In this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22. Note: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants. Learning Objectives At the end of this tutorial you should: Be familiar with the FASTQ format and base quality scores Be able to align reads to generate a BAM file and subsequently generate a pileup file Be able to run the FreeBayes variant caller to find SNVs and indels Be able to visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye Background Some background reading material - background Where is the data in this tutorial from? The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project. 1. Preparation Make sure you have an instance of Galaxy ready to go. If you are not using your own Galaxy instance, you can use our Galaxy Tutorial server or Galaxy Melbourne server . Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis. 2. Quality Control The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome! 1. Take a look at the FASTQ file Click on the eye icon to the top right of the fastq file to view the a snippet of the file. Note that each read is represented by 4 lines: read identifier short read sequence separator short read sequence quality scores e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ . 2. Assessing read quality from the FASTQ files From the Galaxy tools panel, select NGS: QC and manipulation > FastQC: Read Quality reports . The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30). Note that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step. 3. Alignment to the reference - (FASTQ to BAM) The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome. 1. Align the reads with BWA Map/align the reads with the BWA tool to Human reference genome 19 (hg19) UCSC hg19 . From the Galaxy tools panel, select NGS: Mapping > Map with BWA-MEM [3-5mins] From the options: Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file. From the Galaxy tools panel, select NGS: SAM Tools > Sort BAM dataset From the options: BAM File: set to the output from the alignment BAM file Sort by: Chromosomal coordinates Keep other options as default and click execute 2. Examine the alignment To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, select NGS: SAM Tools > BAM-to-SAM From the options: BAM File to Convert: set to the output to the sorted BAM file Keep other options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc. 3. Assess the alignment data We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, select NGS: SAM Tools > IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, select NGS: Sam Tools > Flagstat From the options: The BAM: select the sorted BAM file Keep other options as default and click execute Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant. 4. Visualise the BAM file. To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \"Display with IGV web current \". This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22:36,006,744-36,007,406 Can you see a few variants? Don't close IGV yet as we'll be using it later. 5. Generate a pileup file A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence. Generate a pileup file: From the Galaxy tools panel, select NGS: SAMtools > Generate Pileup From the options: Call consensus according to MAQ model = Yes This generates a called 'consensus base' for each chromosomal position. Keep other options as default and click execute For each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you're editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores Filter the pileup file: If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, select: NGS: SAM Tools > Filter Pileup From the options: which contains = Pileup with ten columns (with consensus) Do not report positions with coverage lower than = 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row. 6. Call variants with FreeBayes FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial. Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. Check the generated list of variants . Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification . Visualise the variants and compare files Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22:36,006,744-36,007,406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1==\"chr22\" and c2 > 36006744 and c2 < 36007406 . Optional: filter variants : See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort: Filter tool we used above. 7. Further steps We've seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"Introduction to Variant Calling"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#introduction-to-variant-calling-using-galaxy","text":"","title":"Introduction to Variant Calling using Galaxy"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#tutorial-overview","text":"In this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22. Note: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants.","title":"Tutorial Overview"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#learning-objectives","text":"At the end of this tutorial you should: Be familiar with the FASTQ format and base quality scores Be able to align reads to generate a BAM file and subsequently generate a pileup file Be able to run the FreeBayes variant caller to find SNVs and indels Be able to visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye","title":"Learning Objectives"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#background","text":"Some background reading material - background","title":"Background"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#where-is-the-data-in-this-tutorial-from","text":"The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project.","title":"Where is the data in this tutorial from?"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-preparation","text":"Make sure you have an instance of Galaxy ready to go. If you are not using your own Galaxy instance, you can use our Galaxy Tutorial server or Galaxy Melbourne server . Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis.","title":"1. Preparation"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-quality-control","text":"The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!","title":"2. Quality Control"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-take-a-look-at-the-fastq-file","text":"Click on the eye icon to the top right of the fastq file to view the a snippet of the file. Note that each read is represented by 4 lines: read identifier short read sequence separator short read sequence quality scores e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ .","title":"1. Take a look at the FASTQ file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-assessing-read-quality-from-the-fastq-files","text":"From the Galaxy tools panel, select NGS: QC and manipulation > FastQC: Read Quality reports . The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30). Note that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step.","title":"2. Assessing read quality from the FASTQ files"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-alignment-to-the-reference-fastq-to-bam","text":"The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.","title":"3. Alignment to the reference - (FASTQ to BAM)"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-align-the-reads-with-bwa","text":"Map/align the reads with the BWA tool to Human reference genome 19 (hg19) UCSC hg19 . From the Galaxy tools panel, select NGS: Mapping > Map with BWA-MEM [3-5mins] From the options: Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file. From the Galaxy tools panel, select NGS: SAM Tools > Sort BAM dataset From the options: BAM File: set to the output from the alignment BAM file Sort by: Chromosomal coordinates Keep other options as default and click execute","title":"1. Align the reads with BWA"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-examine-the-alignment","text":"To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, select NGS: SAM Tools > BAM-to-SAM From the options: BAM File to Convert: set to the output to the sorted BAM file Keep other options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.","title":"2. Examine the alignment"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-assess-the-alignment-data","text":"We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, select NGS: SAM Tools > IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, select NGS: Sam Tools > Flagstat From the options: The BAM: select the sorted BAM file Keep other options as default and click execute Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.","title":"3. Assess the alignment data"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-visualise-the-bam-file","text":"To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \"Display with IGV web current \". This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22:36,006,744-36,007,406 Can you see a few variants? Don't close IGV yet as we'll be using it later.","title":"4. Visualise the BAM file."},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#5-generate-a-pileup-file","text":"A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence. Generate a pileup file: From the Galaxy tools panel, select NGS: SAMtools > Generate Pileup From the options: Call consensus according to MAQ model = Yes This generates a called 'consensus base' for each chromosomal position. Keep other options as default and click execute For each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you're editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores Filter the pileup file: If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, select: NGS: SAM Tools > Filter Pileup From the options: which contains = Pileup with ten columns (with consensus) Do not report positions with coverage lower than = 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.","title":"5. Generate a pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#6-call-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial. Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. Check the generated list of variants . Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification . Visualise the variants and compare files Open the VCF file in IGV using the dataset's display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22:36,006,744-36,007,406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1==\"chr22\" and c2 > 36006744 and c2 < 36007406 . Optional: filter variants : See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort: Filter tool we used above.","title":"6. Call variants with FreeBayes"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#7-further-steps","text":"We've seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"7. Further steps"}]}